\section{Symptoms}

We want to study symptoms in the context of fault localization. We define symptoms as the output given in failing test cases. In Java, most of these symptoms will be exceptions and their accompanying error message. Since most program repair is based off of failing tests, we want to see if those symptoms exhibited in those tests can correlate to type of repair.

For this paper, we specifically wanted to look at whether symptoms correlated with whether a bug could be fixed in a single location or multiple locations. If we can classify which bugs are single edit or multi edit, we can choose fault localization or patch generation techniques that are more suited.

We looked at all symptoms for all bugs in Defects4J and Bears, and categorized them based on whether they were one of the identified multi-chunk bugs or not. Then we fit the data to a linear regression model to see if there were statistically significant differences between the type of symptoms for multi edit and single edit bugs.

In order to find statistical significance, we needed to categorize the symptoms in big enough categories. We experimented with three groupings:

\begin{enumerate}
	\item Group all exceptions together except for assertion exceptions and exceptions for which a message indicates some sort of assertion (in our case, we simply looked for the keyword "expected"). We grouped the assertions into a few different types:
	\begin{itemize}
		\item \lstinline{assert_null} is when the assertion is either expecting null, or got null when it wasn't expecting it.
		\item \lstinline{assert_int} is when the failing assertion was expecting a particular int value.
		\item \lstinline{assert_float} is the same as above, but for floats.
		\item \lstinline{assert_obj_arr_date} is when the assertion is expecting an object address, array of objects, or date object/date string. These are grouped together as commonly found but more complex assertions.
		\item \lstinline{error_expected} is when the failing test expected an exception, error, or warning.
		\item \lstinline{timeout} is when a Junit test times out, but also includes errors like stack overflows or out of memory exceptions.
		\item \lstinline{other_assert} for any other assertion that I couldn't easily categorize or parse. The large bulk of bugs in this category are bugs that had no error message at all; it simply failed with an \lstinline{AssertionError} or \lstinline{StackOverflow}.
		\item \lstinline{other}: all non-assertion exceptions.
	\end{itemize}
	\item Grouping symptoms together in an ad hoc way \todo{sell it better.}
	\begin{itemize}
		\item \lstinline{assert_prim}: assertions that compare to a Java primitive, such as int, float, or boolean.
		\item \lstinline{assert_null}: either expected or actual is null
		\item \lstinline{other_assert}: Asserting to anything that's not clearly a primitive or null.
		\item \lstinline{access}: all the bugs pertaining to wrongfully accessing or invoking certain fields or methods, or problems with classpath.
		\item \lstinline{null_pointer}: null pointer exceptions.
		\item \lstinline{timeout}: when a Junit test times out, but also includes errors like stack overflows or out of memory exceptions.
		\item \lstinline{parsing}: Anything related to parsing, serialization, or type conversion.
		\item \lstinline{other}: everything else
	\end{itemize}
	\item This last grouping is an even coarser version of the previous grouping.
	\begin{itemize}
		\item \lstinline{assert_equal}: Any assertion in which the test expected one value but got another
		\item \lstinline{other_assert}: any other assertion
		\item \lstinline{access}: all the bugs pertaining to wrongfully accessing or invoking certain fields or methods, or problems with classpath.
		\item \lstinline{null_pointer}: null pointer exceptions.
		\item \lstinline{parsing}: Anything related to parsing, serialization, or type conversion.
		\item \lstinline{other}: everything else
	\end{itemize}
\end{enumerate}

\todo{raw data:}

assert only
\begin{lstlisting}[basicstyle=\tiny]
Call:
glm(formula = multi ~ assert_obj_arr_date + assert_int + assert_float + 
error_expected + timeout + assert_null + other_assert + other, 
family = "binomial", data = symptoms)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-1.1630  -0.8391  -0.6331  -0.5009   1.9894  

Coefficients:
Estimate Std. Error z value Pr(>|z|)    
(Intercept)             -1.46956    0.31792  -4.622 3.79e-06 ***
assert_obj_arr_dateTRUE  0.63225    0.47659   1.327  0.18464    
assert_intTRUE          -0.18226    0.38280  -0.476  0.63399    
assert_floatTRUE         1.42011    0.44206   3.212  0.00132 ** 
error_expectedTRUE      -0.36046    0.48704  -0.740  0.45924    
timeoutTRUE              0.50710    0.59237   0.856  0.39197    
assert_nullTRUE          0.82904    0.37982   2.183  0.02905 *  
other_assertTRUE        -0.03597    0.30388  -0.118  0.90577    
otherTRUE                0.60662    0.31835   1.906  0.05671 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 725.44  on 645  degrees of freedom
Residual deviance: 697.26  on 637  degrees of freedom
AIC: 715.26

Number of Fisher Scoring iterations: 4
\end{lstlisting}

grouping 1
\begin{lstlisting}[basicstyle=\tiny]
Call:
glm(formula = multi ~ access + assert_prim + null_pointer + timeout + 
assert_null + parsing + other_assert + other, family = "binomial", 
data = symptoms)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-1.3448  -0.7293  -0.6413  -0.6152   1.9233  

Coefficients:
Estimate Std. Error z value Pr(>|z|)    
(Intercept)      -1.15825    0.30011  -3.859 0.000114 ***
accessTRUE        0.49743    0.35565   1.399 0.161918    
assert_primTRUE   0.28861    0.31792   0.908 0.363983    
null_pointerTRUE -0.20119    0.45263  -0.444 0.656682    
timeoutTRUE       0.23758    0.58290   0.408 0.683582    
assert_nullTRUE   0.57468    0.37483   1.533 0.125236    
parsingTRUE       0.96885    0.41871   2.314 0.020675 *  
other_assertTRUE -0.31892    0.28866  -1.105 0.269231    
otherTRUE        -0.09137    0.40435  -0.226 0.821223    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 725.44  on 645  degrees of freedom
Residual deviance: 703.98  on 637  degrees of freedom
AIC: 721.98

Number of Fisher Scoring iterations: 4
\end{lstlisting}

grouping 2
\begin{lstlisting}[basicstyle=\tiny]
Call:
glm(formula = multi ~ assert_equal + access + null_pointer + 
parsing + other_assert + other, family = "binomial", data = symptoms)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-1.2102  -0.7915  -0.6497  -0.6220   1.8644  

Coefficients:
Estimate Std. Error z value Pr(>|z|)    
(Intercept)      -1.24535    0.29793  -4.180 2.92e-05 ***
assert_equalTRUE  0.24537    0.28801   0.852  0.39424    
accessTRUE        0.55033    0.35105   1.568  0.11696    
null_pointerTRUE -0.09632    0.45470  -0.212  0.83224    
parsingTRUE       1.07674    0.41778   2.577  0.00996 ** 
other_assertTRUE -0.20294    0.27965  -0.726  0.46804    
otherTRUE         0.07740    0.37084   0.209  0.83466    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 725.44  on 645  degrees of freedom
Residual deviance: 709.62  on 639  degrees of freedom
AIC: 723.62

Number of Fisher Scoring iterations: 4
\end{lstlisting}

In assert only, \lstinline{assert_float} correlates with multiedit bugs. In addition, \lstinline{assert_null} (expecting null or getting null) correlates somewhat with multiedit bugs.

In the two groupings, there are slight to medium correlations between parsing errors (errors in which there is some sort of parsing or conversion of text or objects) and multiedit bugs.

Interestingly, parsing errors also correlate with decreased APR success in multiedit bugs.s

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 0 & 133 & 133 \\
            	Not Auto-repaired & 12 & 158 & 170 \\
            	\midrule
            	Total & 12 & 291 & 303\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of Defects4J multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test found a statistically significant relationship
	($p = 0.001$).}
	\label{tab:parsing-repair-frequencies-d4j}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 2 & 14 & 16 \\
            	Not Auto-repaired & 10 & 124 & 134 \\
            	\midrule
            	Total & 12 & 138 & 150\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of Bears multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test did not find a statistically significant relationship
	($p = 0.619$).}
	\label{tab:parsing-repair-frequencies-bears}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 2 & 147 & 149 \\
            	Not Auto-repaired & 22 & 282 & 304 \\
            	\midrule
            	Total & 24 & 429 & 453\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of both Defects4 and Bears multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test found a statistically significant relationship
	($p = 0.007$).}
	\label{tab:parsing-repair-frequencies-bears}
\end{table}
