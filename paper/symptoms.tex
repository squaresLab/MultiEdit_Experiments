\subsection{Symptoms}

\rqorinsight{RQ3: Do certain symptoms correlate with multi-edit patches?} 

Current fault localization approaches, particularly SBFL, are not ideal for localizing most multi-edit 
bugs. Thus, we want to look at other information we can leverage in fault localization. Since most 
APR techniques use failing tests to identify a fault, correlating the symptoms exhibited by failing 
tests with certain classes of repairs may provide a starting point for developing new methods for 
fault localization.

We define symptoms as the output given in failing test cases. In Java, most of 
these symptoms are exceptions (including JUnit exceptions) and the accompanying error message.

\paragraph{Methodology}
We recorded the incidence of each symptom among multi-edit and single-edit bugs, and then fit the 
data to a linear regression in order to find which symptoms would be able to predict whether a bug 
is multi-edit or single-edit.

We included all 646 bugs found in Bears and Defects4J, and between those 646 bugs, there were 
71 unique exceptions thrown in failing test cases. Of those exceptions, 74\% were either 
\texttt{junit.framework.AssertionFailedError}, \texttt{java.lang.AssertionError}, 
\texttt{java.lang.NullPointerException}, or \texttt{junit.framework.ComparisonFailure}.

Since the sample sizes are so small, we needed to categorize the symptoms in larger groups in order 
to find statistical significance. We experimented with a few groupings, not all of which generated 
results~\footnote{The scripts we used to generate the classifications can be found at \url{Github link 
redacted}.}.  



\paragraph{Results}
In assert only, \lstinline{assert_float} correlates with multiedit bugs. In addition, \lstinline{assert_null} (expecting null or getting null) correlates somewhat with multiedit bugs.

In the two groupings, there are slight to medium correlations between parsing errors (errors in which there is some sort of parsing or conversion of text or objects) and multiedit bugs.

\rqorinsight{RQ4: Do certain symptoms correlate with repair success by existing repair tools?} 
\todo{where to fit this, it's not exactly localization.}

Looking at symptoms that are more or less likely to be repaired by existing repair tools may give us 
insight to the strengths and weaknesses of existing repair tools, and provides insight to opportunities 
to improve program repair tools.

Classifying multi-edit patches using the same symptom categorization previously outlined, we 
performed a Fisher's Exact Test for each symptom category to see if a bug exhibiting that class of 
symptom would be more or less likely to be repaired by automated repair tools than a bug that is not 
exhibiting that symptom. We used the RepairThemAll experiment in order to determine whether a 
bug could be repaired by automated repair tools~\cite{durieux-repair-them-all}.

\paragraph{Results}
We found statistically significant results for symptoms categorized as parsing errors, particularly in 
Defects4J. Table \ref{tab:parsing-repair-frequencies-d4j} shows a strong correlation between bugs 
that exhibit a parsing error and bugs that are unable to be repaired by APR tools. This correlation 
still holds when looking at Defects4J and Bears bugs together, as shown in Table 
\ref{tab:parsing-repair-frequencies-both}. However, in Table 
\ref{tab:parsing-repair-frequencies-bears}, the parsing symptom does not correlate with 
repairability. \todo{Is repairability a word I can use here?}

\todo{We found a difference between bears and d4j in coverage experiments as well as symptoms 
vs. repairability. Is the difference between datasets something we want to dwell on? Should I look at 
the difference between bears and d4j for symptoms vs multi-edit?}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 0 & 133 & 133 \\
            	Not Auto-repaired & 12 & 158 & 170 \\
            	\midrule
            	Total & 12 & 291 & 303\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of Defects4J multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test found a statistically significant relationship
	($p = 0.001$).}
	\label{tab:parsing-repair-frequencies-d4j}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 2 & 14 & 16 \\
            	Not Auto-repaired & 10 & 124 & 134 \\
            	\midrule
            	Total & 12 & 138 & 150\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of Bears multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test did not find a statistically significant relationship
	($p = 0.619$).}
	\label{tab:parsing-repair-frequencies-bears}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | r}
            	\toprule
            	& Symptomatic & Asymptomatic & Total \\
            	\midrule
            	Auto-repaired & 2 & 147 & 149 \\
            	Not Auto-repaired & 22 & 282 & 304 \\
            	\midrule
            	Total & 24 & 429 & 453\\
            	\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of both Defects4 and Bears multi-line patches with respect to the presence of
	the parsing symptom and whether an APR tool successfully
	repaired the bug in~\cite{durieux-repair-them-all}.
	A Fisher's Exact Test found a statistically significant relationship
	($p = 0.007$).}
	\label{tab:parsing-repair-frequencies-both}
\end{table}
