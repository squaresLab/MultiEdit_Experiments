\section{Fault localization} \label{secFL}

%% what are the claims, what are we studying, why are we studying it


Spectrum-based fault localization (SBFL) is the most commonly studied dynamic
fault localization technique, and studies have shown that it is more effective
than other techniques such as Mutation-based fault
localization~\cite{mut-analysis} or dynamic program
slicing~\cite{zou2019empirical}. It is a key first step to characterizing the
\emph{fault space} in automatic program repair, narrowing it to a portion of the
program most likely to correspond to the fault.

Fundamentally, the core assumption underlying SBFL is that \emph{failing tests
  execute buggy portions of the code relatively more often than passing tests.}
Thus, if all failing tests execute a particular line of code, then that line of
code is highly suspicious.  SBFL techniques compute a suspiciousness by
measuring how often a line is executed by failing tests as compared to passing
tests. This suspiciousness score can be calculated a few different ways, but is
typically a linear combination of the passing and failing test coverage. One of
the oldest and commonly studied SBFL technique is Tarantula~\cite{tarantula}. In
Tarantula, the suspiciousness score for a line $s$ is calculated by:

$$\mathit{susp(s)} 
=\frac{\mathit{\%F(s)}}{\mathit{\%F(s)} + \mathit{\%P(s)}}$$

where  $\mathit{\%F(s)}$ and $\mathit{\%P(s)}$ are, respectively, the percentage of failing 
tests and passing tests that execute $s$. There are newer, more effective 
SBFL techniques that calculate this score differently, such as Ochiai~\cite{ochiai} and 
DStar~\cite{wong2013dstar}. Both of these were included in an empirical study 
comparing fault localization techniques and were found to localize a similar set of 
faults~\cite{zou2019empirical}.

Assigning a suspiciousness score to each line of code is well-suited to single
location repair. Indeed, the evaluation of most SBFL techniques asks exactly the
question of interest when considering a technique's suitability for
single-location repair: how often does a given technique assign high scores to 
individual buggy lines of code?

Such evaluations, by and large, do not consider the implications of
suspiciousness scoring in a multi-location repair context.  Instead, evaluations
typically consider a technique ``successful'' if it identifies \emph{any} of a
set of changed lines as highly-ranked or likely-suspicious.  While appropriate
for the question being asked in such evaluations, this does not address
suitability for multi-location program repair.  Identifying one of several buggy
locations is generally inadequate in a context where multiple locations must be
modified. In order to investigate how well the SBFL assumption
applies to tests that identify bugs that require multi-location patches, we ask 
the following research question:


\rqorinsight{2}{How well do multiple tests cover the multiple locations
  implicated in bugs that require multi-location patches?}

We focus especially on bugs that are associated with multiple failing tests: a bug
with only a single failing test trivially and equally implicates all the lines
that test executes.  If multiple tests cover the modified locations well, then
SBFL's core assumption holds and multi-location repair can expect to effectively
make use of the off-the-shelf ranking these techniques currently provide
(indeed, this has been tried~\cite{angelix}). If not---that is, if multiple
tests exercise \emph{different} portions of the buggy code---SBFL off-the-shelf
will by definition be less effective in guiding APR to correctly modifying
multiple buggy locations at once.
Fundamentally, we ask whether multiple failing tests cover exactly the same
patch locations, exactly disjoint patch locations, or some combination.

\paragraph{Methodology}

Between both datasets, there are 92 total bugs that both require multi-location
patches and contain multiple failing tests. However, we were not able to obtain coverage 
data for one of the bugs in Bears, leaving 91 total bugs: 59 in Defects4J, and 32 in
Bears. 
For each of these bugs, we used JaCoCo\footnote{https://www.eclemma.org/jacoco/}
to determine which of the patched locations were executed (at least once) by
each failing test. Using this coverage data, we categorized each bug as follows:
\begin{itemize}
\item \emph{disjoint} bugs are those for which no patch location is covered by all
failing tests.  Intuitively, these are the bugs for which the core SBFL
assumption is violated.
\item \emph{overlap} bugs are those for which some faulty locations are covered
by all failing tests, but some are only covered by a subset of the failing
tests. These bugs also violate the core assumption of SBFL, albeit to a lesser
extent.
\item \emph{identical} bugs are those for which all tests cover the exact same
  set of patch locations.
\end{itemize}


\begin{figure}
	\includegraphics[width=\linewidth]{img/coverage-all.png}
	\caption{Distribution of coverage patterns for bugs with multiple failing
      tests that are repaired with multi-location patches in Bears and Defects4J.}
	\label{fig:coverage-all}
\end{figure}


\begin{figure}
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{img/coverage-d4j.png}
		\caption{Distribution of coverage patterns for Defects4J.}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{img/coverage-bears.png}
		\caption{Distribution of coverage patterns for Bears.}
	\end{subfigure}
	\caption{Distribution of coverage patterns divided by dataset,
          indicating significant differences between the multi-location bugs in
          Bears and Defects4J.}
	\label{fig:coverage-datasets}
\end{figure}

\paragraph{Results}
Figure~\ref{fig:coverage-all} shows results for the overall distribution,
combining the bugs in both datasets. 27\%
of the bugs were fully disjoint.  Thus, for a significant portion of multi-location bugs,
the failing tests do not all execute any of the buggy code in common.  
In addition, another 31\% were classified as \emph{overlap}: only some of the
buggy locations were executed by all failing tests. In all, over half of 
the bugs that were both multi-location and multi-test contained edit locations that were 
not executed by all failing test cases.

Note, however, that the behavior varies considerably by dataset;
Figure~\ref{fig:coverage-datasets} shows results.  Defects4J and 
Bears have different distributions with respect to the 
\emph{disjoint} and \emph{same} categories. In Defects4J, 36\% of bugs are \emph{disjoint} and 
22\% are \emph{overlap}; in Bears, the proportions are 12\% and 47\%, respectively.
We hypothesize that this may be due to differences in how the two  
datasets were selected and constructed: Defects4J is a dataset of hand-isolated, and curated 
bugs, while the bugs in Bears are scraped directly from the
commits. The Defects4J dataset specifically enforces a requirement that the patches in the 
dataset be isolated, i.e., not containing any refactorings or new features, to improve the 
usability of the dataset. The authors specifically chose patches that met this requirement, 
and in some cases, isolated the bug themselves, manually~\cite{defects4j}. By contrast, the 
only requirements for inclusion of bugs in Bears is that it must be reproducible, and that
the patch must be written by a human. In addition, Bears was designed to be evolvable 
and relatively easily expanded as a dataset, which is at odds with manual inspection and isolation of 
bugs~\cite{bears}.
Given that these two datasets were designed with different values and demonstrate very 
different behavior, these findings highlight the importance of using diverse datasets in 
evaluating program repair techniques.

Overall, SBFL assumes that faulty locations are executed more often by identifying 
or failing test cases and is not designed to find many of these multi-location
faults. Our results suggest that off-the-shelf SBFL techniques are not
well-suited to guiding APR techniques that conform to the the dominant paradigm to reparining
these types of multi-location, multi-test bugs.


