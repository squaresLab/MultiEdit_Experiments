\section{Fault localization}

%% what are the claims, what are we studying, why are we studying it


Spectrum-based fault localization (SBFL) is the most commonly studied, as well as the most 
effective~\cite{zou2019empirical}, fault localization technique. It is a key
first step to characterizing the \emph{fault space} in automatic program repair,
narrowing the search space to a portion of the program more likely (based on
test case behavior) to correspond to the fault.  

\paragraph{SBFL Basics.} \todo{Serena, please add an explanation of how SBFL
  works; it's key to understanding the underlying assumption.  This should
  include references as appropriate.}

Given these basics, fundamentally, a core
assumption underlying SBFL is that \emph{failing tests execute buggy portions of 
the code relatively more often than passing tests.} Thus, if all failing tests execute a 
particular line of code, then that line of code is scored as more suspicious.
This assumption is well-suited to single edit repair. Indeed, the evaluation
of most SBFL techniques asks exactly the question of interest when considering a
technique's suitability for single-edit repair: how often does a given technique
correctly, highly rank individual buggy lines of code? 

Such evaluations, by and large, do not consider the implications of
suspeciousness scoring in a multi-edit repair context.  Instead, evaluations
typically consider a technique ``successful'' if it identifies \emph{any} of a
set of changed lines as highly-ranked or likely-suspicious.  While appropriate
for the question being asked in such evaluations, this does not address
suitability for multi-line program repair.  Identifying one of several buggy
locations is generally inadequate in a context where multiple locations must be
modified.  

Thus, in this research question, we investigate how well the SBFL assumption
applies to tests that identify multi-line bugs. We focus especially on
multi-line bugs that are associated with multiple tests; \todo{short explanation
  of the intuition why. This is a placeholder, I'm a bit tired and so this
  succinct explanation is escaping me.} If multiple tests all cover the multiple
modified locations well, then SBFL's core assumption holds and multi-edit repair
can expect to effectively make use of the off-the-shelf ranking these techniques
currently provide (indeed, this has been tried~\cite{angelix}). If not---that
is, if multiple tests exercise \emph{different} portions of the buggy
code---SBFL off-the-shelf will by definition be less effective in guiding APR to
correctly modifying multiple buggy locations at once.

Therefore, for multi-edit bugs with multiple identifying failing tests, do the 
different failing tests cover exactly the same patch locations, exactly 
disjoint path locations, or some combination?


\rqorinsight[2]{How well do multiple tests cover the multiple locations
  implicated in multi-edit bugs?}

\paragraph{Methodology}

Between both datasets, there are 60 total bugs that are both multi-edit and 
multi-test -- 45 in Defects4J and 15 in Bears. For each of these bugs, we used 
Jacoco to determine which of the chunks in the human patch were executed by each failing test. Using 
this coverage data, we categorized the bug as one of 
three coverage patterns: \textit{disjoint} for bugs in which no chunk is covered by all failing tests, 
\textit{same} for bugs in which all 
tests cover the exact same chunks of the patch, and \textit{overlap} for all 
other bugs, where some chunks are covered by all failing tests, but some chunks are only covered by a 
subset of failing tests.

\paragraph{Results}
The overall distribution is seen in Figure \ref{fig:coverage-all}. Over a third 
of the bugs were classified as disjoint, indicating that for a significant 
portion of bugs, the failing tests do not all execute any portion of the faulty 
code. In addition, another 22\% were classified as overlap. Thus, over half of 
the bugs that were both multi-edit and multi-test contained chunks that were 
not executed by all failing test cases.
2

\begin{figure}
	\includegraphics[width=\linewidth]{img/coverage-all.png}
	\caption{Distribution of coverage patterns for all multi-test and 
	multi-edit bugs in Bears and Defects4J. In all, 38\% of bugs were classified as disjoint, 40\% were 
	classified as same, and 22\% were classified as overlap.}
	\label{fig:coverage-all}
\end{figure}

SBFL assumes that faulty locations are executed more often by identifying 
or failing test cases and is not designed to find faults like these. Since SBFL 
is the most common fault localization technique in APR, most APR techniques 
are not well suited to fix a majority of these multi-edit and multi-test 
bugs.

If we divide distribution by dataset, we can see even more interesting 
behavior. Figure \ref{fig:coverage-datasets} shows the Defects4J dataset and 
the Bears dataset have very different distributions with respect to the 
disjoint and same categories -- in Defects4J, 47\% of bugs are disjoint and 
31\% are same, whereas in Bears, the proportions are 13\% disjoint and 67\% 
same. We hypothesize that this may be due to differences in how the two 
datasets were minimized, since Defects4J is a dataset of minimized and curated 
bugs while the bugs in Bears are taken scraped directly from the commits.


\begin{figure}
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{img/coverage-d4j.png}
		\caption{Distribution of coverage patterns for Defects4J. 47\% disjoint, 31\% same, and 22\% 
		overlap.}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[width=\linewidth]{img/coverage-bears.png}
		\caption{Distribution of coverage patterns for Defects4J. 13\% disjoint, 67\% same, and 20\% 
		overlap.}
	\end{subfigure}
	\caption{Distribution of coverage patterns divided by dataset.}
	\label{fig:coverage-datasets}
\end{figure}


%\input{symptoms.tex}

