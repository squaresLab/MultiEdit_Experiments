\documentclass[sigconf, timestamp-false, anonymous=true]{acmart}

\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{fancybox}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}

\acmConference[ESEC/FSE '20]{ESEC/FSE '20: ACM Joint European Software Engineering Conference and Symposium 
on the Foundations of Software Engineering}{November 8-13, 2020}{Sacramento, CA, USA}
\acmYear{2020}

\newcommand\todo[1]{\textcolor{red}{#1}}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{A Study of Multi-Edit Bug Patches}

%add authors & shortauthors if we are lucky :)

\begin{abstract}
  \todo{note that this is probably full of falsehoods, I'm just thinking by
    typing.} Automatic program repair is a promising approach for reducing the
    cost of quality assurance practices and faulty software. To date, most
    techniques proposed for test-driven automatic repair have succeeded
    primarily on bugs that benefit from short, single-edit patches. Techniques
    that succeed on multi-edit bugs often do so by patching them in an
    alternative, single-edit way, or by targeting particular multi-edit bug
    patterns. Empirical studies of real-world similarly tend to focus on the
    patterns exhibited by single-edit bugs, and have not examined repairability
    of multi-edit bugs in detail. We present a comprehensive empirical analysis
    of multi-edit bugs in open source Java programs, focusing on static and
    dynamic properties that define the repair search space for a given bug (and
    thus, in turn, the challenges that apply to automatically addressing them).
    This analysis focuses on the key challenges of the dynamic program repair
    problem: the \emph{mutations and fix code} used to repair multi-edit bugs;
    the \emph{fault locations} and their relationships; and the \emph{objective
      function}, and in particular how and to what degree test cases can be used
    (or not) to identify partial repairs. We identify key takeaways and
    challenges, with implications for future work in expressive, multi-chunk bug
    repair.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011102</concept_id>
<concept_desc>Software and its engineering~Software defect analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011784</concept_id>
<concept_desc>Software and its engineering~Search-based software engineering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software defect analysis}
\ccsdesc[500]{Software and its engineering~Search-based software engineering}

\keywords{software bugs, program repair}

\maketitle


\newcommand{\rqorinsight}[1]{
  \setlength{\fboxsep}{0.8em}
  \vspace{0.5em}
  \begin{center}
  \Ovalbox{\begin{minipage}{0.9\linewidth}
    \textbf{Research Question:} #1
    \end{minipage}}
  \end{center}
  \vspace{0.5em}}

\section{Introduction}

Buggy software has a significant cost in terms of both social impact~\cite{tricentis} 
and developer effort~\cite{cambridge-study}. This
motivates significant research in techniques to automatically find and fix
them~\cite{whatever}, with the ultimate goal of reducing Quality Assurance costs
while improving software quality. 

A significant class of program repair techniques in both
research~\cite{genprog,angelix,Le17, Xuan17} and practice~\cite{sapfix} use test cases to guide
patch construction, typically by following what's known as a
generate-and-validate paradigm. At a high level, these techniques use test cases
to localize a defect --- identified by at least one failing test --- to a set of
likely-suspicious program locations. Then, they use a variety of techniques to
construct (or \emph{generate}) patch candidates for the bug in question,
checking each to see (or \emph{validate}) if any of them cause the program to
pass all of the provided test cases.  
%
Patches are constructed in a variety of ways, ranging from heuristic, syntactic
program manipulation~\cite{syntax,examples}, to specially adapted program
synthesis techniques~\cite{examples}. These techniques have successfully
repaired real, meaningful defects in large, complex programs~\cite{examples}.

Practically speaking, these techniques are typically limited in the types and
variety of defects they can repair~\cite{for,example}. Some of this limitation
is by design. Some techniques specifically target certain classes of
bugs~\cite{nopol,sapfix}, and it has been argued that doing so is necessary for
repair to reach true practical applicability~\cite{maybe}. Fundamentally, the
vast majority of program repair techniques can only produce single-edit repairs,
often by design~\cite{rsrepair, ae, hdrepair}. \todo{more recent citations?}
Even those that can in principle produce multi-edit bugs typically
don't~\cite{poor,genprog}.

This leaves a large proportion of real-world bugs unrepairable by modern
research techniques in program repair.  Over half of the bugs in popular bug
benchmark Defects4J are patched by humans using multi-part
patches~\cite{d4j-dissection}. Approximately 70\% of buggy source files in a
large study of bugs in Apache projects required two or more edit
actions~\cite{zhong2015}.

A key factor or tension in how these techniques are constructed is in their
management of the trivially-infinite search space of possible patches for a
given bug, while constructing patches in which a user can have confidence of
their quality or correctness.  This space has been parameterized~\cite{refs}
along several axes: (1) the \emph{fault space}, or the potential locations that
may be changed, (2) the \emph{mutation space}, or the possible modifications
that may be tried at a location, and (3) \emph{fix code space}, or the code that may
be instantiated as part of a mutation (e.g., if the mutation seeks to
\emph{insert} code, the fix code is the code that is inserted).  Many dynamic
repair techniques can be compared or described in terms of their choices along
each of these axes; the ways each technique constructs and then traverses the
space has first-order implications for scalability, and for the type and quality
of patches produced.  This has been studied many ways~\cite{examples}.

Multi-edit repair poses distinct challenges for each of these axes for repair.  
Spectrum-based fault localization, the most
prevalent class of fault localization techniques used in program repair, does
not specifically identify sets of locations that might be related or repaired
together; indeed, the evaluation of most fault localization
techniques typicaly assumes that a bug is localized if any one buggy line is
identified~\cite{fl-survey-wong}.  Researchers have observed that some bugs are
repaired in multiple locations using very similar
code~\cite{saha2019harnessing,jiang2019cmsuggester}, informing novel techniques
that constrain the \emph{fix space} of possible multi-edit repairs accordingly.
One key question for applicability of these types of techniques is how prevalent
such bugs are, and how often multi-edit bugs instead require multiple
coordinating (but ultimately different) edits or pieces of fix code.  Over 50\% of the fixes in four 
Apache projects involve two or more entities -- i.e., a Java class, method, or field -- and 66\%-76\% of 
those multi-entity fixes involved syntactic dependencies.~\cite{wang2018}. 
Test cases are used to evaluate candidate repairs in virtually all dynamic
generate-and-validate repair techniques, but, anecdotally, may not be effective
at identifying partial repairs in a multi-edit
context~\cite{fitness10,maybeeric}.  
Although multi-edit repair has been discussed in the context of other analyses
that study bug fix characteristics in general~\cite{examples} as well as for
repair applicability specifically~\cite{moar,examples}, to the best of our
knowledge there has been no significant previous study of the characteristics of
multi-edit repairs in terms of their implications for repairability or, more
broadly, program repair.  

In this paper, we present a systematic study of real-world multi-edit bugs,
looking specifically at their characteristics with respect to the problem of the
automatic repair search space.  
We study the bugs curated in two
real-world datasets that support program repair research: Defects4J~\cite{defects4j}
and BEARS~\cite{bears}, in total, 576 bugs in 40 projects. 
More than half of these bugs were repaired by a
human developer using a multi-chunk repair (as we define in
Section~\ref{sec:background}).  We look at characteristics along each of the
relevant axes of the program repair search problem: fault locations, mutation
operators, fix code, and evaluation (or fitness or objective) using test cases.  
\todo{we also examine symptoms, need to fit that one in.}  Our contributions are
as follows:

\begin{itemize}
\item \todo{fixme} A study on X and Y.
\item \todo{fixme} characterizing of fault localization.  We find that FOO.
\item \todo{fixme} characterizing fix types and code.  We find that BAR.
\item \todo{fixme} study of test cases as a means to evaluation partial and full
  multi-site repairs.  We find that BAZ.
\item A new fitness function that measures patch quality using the number of 
	passing and failing assertions within test cases.
\item \todo{fixme} whatever else.
\item \todo{A set of recommendations?}
\end{itemize}

\todo{This paragraph is a lie and some people hate them so maybe we'll cut it,
  it just feels comforting to me to write them as I'm working on a paper draft...}
The rest of this paper is organized as follows.  We provide background on
generate-and-validate program repair, with a focus on the key parameters of the
program repair search problem, in Section~\ref{sec:background}.  We describe the results of our analysis in
Section~\ref{sec:results}, with discussion of implications as well as
limitations in Section~\ref{sec:discussion}.  Related work is discussed in
Section~\ref{sec:related}; Section~\ref{conclusions} concludes.

\section{Key Concepts and Research Questions}
\label{sec:background}

\paragraph{Search spaces in automatic program repair.} Automatic program repair (APR) techniques aim to find patches for bugs in
programs.  We restrict attention to dynamic or test-case guided program repair,
which describes the majority of research advancements over the past ten years.
Such techniques take as input a program and a set of test cases that serve as
the oracle for program correctness.  At least one of those tests should be
failing, corresponding to the bug to be repaired.  

At a high level, any APR technique aims to solve a search or optimization
problem, seeking a set of edits (or patch) to the program that will lead it to
pass all of the provided tests.  Although The repair search is classically defined along
the following axes:
\begin{itemize}
\item \emph{Fault space.} The first problem in fixing a bug concerns
  \emph{where} in the code modifications should be applied. All dynamic repair
  techniques begin by using the provided test cases as input to a fault
  localization technique. Such techniques identify (and typically score)
  suspicious code based on which test cases execute which pieces of code.
  Although the particulars of the fault localization employed can vary, most APR
  use some variant of spectrum-based fault localization (SBFL)~\cite{refs} in
  this first step. The resulting computation defines the fault space, or the
  candidate locations or code entities considered for repair. This first step is
  key to reducing the search space.

\item \emph{Mutation space.} The mutation space is the set of applicable 
modifications at a program location. Examples include GenProg's append, 
replace, and delete operators over statements~\cite{genprog-operators} and 
Nopol's condition replacement over if-statements and missing precondition 
addition over non-branch and non-loop statements~\cite{Xuan17}.
A larger mutation space generates 
a richer search space with potentially more repairs, but
the resulting combinatorial growth of the search space can also hinder 
the search process~\cite{long-search-spaces}.

\item \emph{Fix code.}

\item \emph{Objective.} 

\end{itemize}



\paragraph{Edit granularity in bug-fixing patches.}  Developer patches for
program bugs naturally vary in length and complexity.  Estimates range from
\todo{FIXME} to \todo{FIXME}\% of bugs are repaired using fewer than
\todo{FIXME} lines of code.  \todo{rephrpase/repeat stuff in intro about
  studies/prevalence of multi-edit bugs, but expand the rephrase of the results
  in that prior work to contrast with incidence of single-edit bugs.}  

There are indeed several plausible definitions of multi- vs. single-edit bugs,
with implications for how they are studied.  We consider edits at the granularity 
of lines and of chunks. We define a line edit as an added, removed, or modified 
line. We define a chunk as a continuous sequence of line edits, with the exception 
that if one chunk adds a \texttt{\{} whose corresponding \texttt{\}} is in a different chunk, 
then we merge the two chunks together. 
We ignore changes to comments, whitespace, or import statements.

Alternative definitions may have bearing on our results.  We argue that this
definition is natural for the purposes of our study, which focuses in particular
on understanding larger bugs---especially multi-chunk bugs---for the purposes of 
adapting, extending, or applying new program repair techniques to address them. 
\todo{A couple more sentences re: why this is reasonable. Might be solved by 
pointing to low rates of repair for multi-chunk bugs.}

\paragraph{Research questions.}  The parameterization of the program repair
search problems into multiple subspaces informs our empirical study of those
spaces for the purposes of understanding multi-edit repair.  We address the
following research questions:

\begin{enumerate}
\item \emph{RQ1: What is the incidence of multi-edit repairs in real-world bug data?}
\item RQ2: ...
\item \todo{WHAT ARE OUR RESEARCH QUESTIONS?}
\end{enumerate}

\section{Dataset Characteristics}
\label{sec:data-rq1}

\begin{table}
\begin{center}
\begin{tabular}{l | rrr}
\toprule
\multicolumn{4}{c}{\textbf{Defects4J}} \\
\midrule
Project & Bugs & Src (kloc) & Test (kloc) \\
\midrule
JFreeChart  & 26 & 193.3 & 74.6 \\
Closure compiler & 133 & 150.6 & 112.6 \\
Apache commons-lang & 65 & 57.8 & 47.4 \\
Apache commons-math & 106 & 45.0 & 41.5 \\
Mockito & 38 & 23.0 & 28.5 \\
Joda-Time & 27 & 82.9 & 70.4 \\
\midrule
\multicolumn{4}{c}{\textbf{Bears (single-module)}} \\
\midrule
FasterXML-jackson-databind & 26 & 95.7 & 53.6 \\
INRIA-Spoon & 62 & 66.2 & 30.8 \\
spring-data-commons & 15 & 45.8 & 28.8 \\
traccar-traccar & 42 & 47.9 & 8.6 \\
30 other projects & 62 & - & -\\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:data} Characteristics of the Defects4J (top) and Bears (bottom) datasets.}
\end{table}


\rqorinsight{RQ1: How many multi-edit patches are in Defects4J and Bears?}
We start our analysis of multi-edit patches by asking how many such patches 
exist in our benchmarks. We analyze edits at line and chunk granularities,
which we defined in Section~\ref{sec:background}.

\paragraph{Datasets}  Our study requires a dataset of indicative, real-world,
multi-edit defects.  We study both the defects in Defects4J
Defects4J~\cite{defects4j} and Bears~\cite{bears}.  Table~\ref{tab:data}
summarizes these datasets, both of which
consist of historical
bugs found in real world software projects. Defects4J contains 395 bugs from 
six Java software projects, and is currently very popular dataset for evaluating 
program repair tools that target Java~\cite{durieux-repair-them-all}.
However, there is a risk associated with any such dataset that tools may overfit
to the defects in question, and there is evidence that this situation applies to
program repair and Defects4j~\cite{durieux-repair-them-all}. 
We thus also study bugs from Bears~\cite{bears}, 
a set of Java bugs derived from failed Travis-CI builds of GitHub
projects. 
Bears offers 251 bugs from 72 software projects, providing a greater diversity of 
projects compared to Defects4J. 
Several projects in Bears, however, are structured as multi-module projects, 
which are not currently compatible with our automation tools.
We thus limit our analysis of Bears to 181 bugs from 30 single-module projects.
We intend to address the remaining multi-module software projects in future work.

\todo{Consider adding a column to the dataset table above with ``subset
  considered'' for the defects; is this worth doing?  How many bugs do we
  exclude? Do we include Closure?}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | rr | rr}
		\toprule
		\multicolumn{7}{c}{\textbf{Line edit granularity}} \\
		\midrule
		Dataset & \multicolumn{2}{c}{Single line} & \multicolumn{2}{c}{Multi-line} & \multicolumn{2}{c}{Total} \\
		\midrule
		Defects4J & 92 & 23\% & 303 & 77\% & 395 & 100\%\\
		Bears & 31 & 17\% & 150 & 83\% & 181 & 100\%\\ 
		Combined & 123 & 21\% & 453 & 79\% & 576 & 100\% \\ 
		\midrule
		\multicolumn{7}{c}{\textbf{Chunk edit granularity}} \\
		\midrule
		Dataset & \multicolumn{2}{c}{Single chunk} & \multicolumn{2}{c}{Multi-chunk} & \multicolumn{2}{c}{Total} \\
		\midrule
		Defects4J & 211 & 53\% & 184 & 47\% & 395 & 100\%\\
		Bears & 72 & 40\% & 109 & 60\% & 181 & 100\%\\
		Combined & 283 & 49\% & 293 & 51\% & 576 & 100\%\\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequencies and percentages of bugs with multi-edit patches
	based on the edit granularities defined in Section~\ref{sec:background}.
	Multi-line patches comprise a large majority of patches, and multi-chunk 
	patches comprise a majority or a large minority of patches in both datasets.}
	\label{tab:multiedit-frequencies}
\end{table}

\paragraph{RQ1: Results.} 
Table~\ref{tab:multiedit-frequencies} lists the number and percentages of
multi-line and multi-chunk patches. For both datasets, over 75\% of patches are multi-line, 
and $\geq$25\% are multi-chunk. At both edit granularities, multi-edit bugs 
comprise a majority or a substantial minority of both datasets.

\todo{Maybe add some data to show repair rates of multi-line \& multi-chunk bugs}

\todo{CLG says: ...I honestly don't understand the ``frequencies'' table or the
  2--3 sentences above. And: What is the interpretation?
  Why do I care?}


\input{coverage.tex}



\section{Dependency Analysis}

\rqorinsight{
	RQ?: How prevalent are dependencies between edited code?
}

APR techniques differ in the treatment of potentially dependent code mutations.
Semantics-based repair techniques can represent dependent edits as a 
conjunction of multiple constraints to simultaneously solve.
\todo{Are there any search based techniques that take the dependency of 
multiple mutations into account?}
On the other hand, GenProg's~\cite{genprog}
mutation operations make no consideration of 
any possible dependencies between applied edits.
We thus investigate the control and data dependency relationships between 
edits in human patches.

\subsection{Methodology}

For dependency analysis, we consider edits at the granularity of lines instead 
of chunks. \todo{Justify.} We consider a patch to contain
dependent edits if there exists control or data dependencies 
between added, removed, or changed lines in the pre- or post-patch
source code. For practical performance and scalability
reasons, we perform intraprocedural analysis, 
although we heuristically consider function arguments as reads 
and invocations of getter and setter methods as reads and writes.
We implement our dependency analyses in Soot~\cite{soot}.

\subsection{Results}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | rr | rr}
		\toprule
		Dataset & \multicolumn{2}{c}{Dependent} & \multicolumn{2}{c}{Independent} & \multicolumn{2}{c}{Total}  \\
		\midrule
		Defects4J & 132  & 44\% & 171 & 56\% & 303 & 100\% \\
		Bears & 95 & 63\% & 55 & 37\% & 150 & 100\% \\
		Combined & 227 & 50\% & 226 & 50\% & 453 & 100\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequencies and percentages of multi-line patches with(out) control or data 
	dependencies between edited lines.}
	\label{tab:dependency}
\end{table}

Table~\ref{tab:dependency} shows the 
frequencies and percentages of multi-line patches with control or data dependent 
line edits. We find a higher proportion of dependent patches in Bears compared to 
Defects4J, indicating higher patch complexity among multi-line patches in Bears.


\begin{table}
{\begin{center}
	\begin{tabular}{l | l | r r r r | r}
		\toprule
		Dataset & APR & Dependent & Independent & Total \\
		\midrule
		\multirow{2}{*}{Defects4J} & Success & 39 & 94 & 133 \\
		                                          & Failure   & 93 & 77 & 170 \\
		\midrule
		\multirow{2}{*}{Bears}       & Success &   9 &   7 &   16 \\
		                                          & Failure   & 86 & 48 & 134 \\
		\midrule
		\multirow{2}{*}{Combined}& Success & 48 &101& 149 \\
		                                          & Failure   &179&125& 304 \\
	\end{tabular}
 \end{center}
}
	\caption{Frequency of multi-line patches with respect to the presence of 
	dependent line edits and whether an APR tool successfully 
	repaired the bug in~\cite{durieux-repair-them-all}.}
	\label{tab:dependency-repair-contingency-table}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rrr}
            	\toprule
		& Defects4J & Bears & Combined \\
		\midrule
		$P(\mbox{APR Success } | \mbox{ Dependent})$ & 30\% & 9\% & 21\% \\
		$P(\mbox{APR Success } | \mbox{ Independent})$ & 45\% & 13\% & 45\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Percentages of auto-repaired bugs in~\cite{durieux-repair-them-all} 
	that (do not) contain dependent line edits.}
	\label{tab:dependency-repair-percents}
\end{table}

Tables~\ref{tab:dependency-repair-contingency-table} and~\ref{tab:dependency-repair-percents}
show the frequencies and percentages of multi-line patches with respect to dependencies 
and whether an APR tool successfully repaired bug in~\cite{durieux-repair-them-all}.
We find that the presence of dependencies generally reduces the likelihood of APR tool success.
Using a $\chi^2$ test, we find statistically significant relationships ($p < 0.001$)
between APR success and control, data, and the disjunction of either dependencies 
for Defects4J and Defects4J $\cup$ Bears patches. We fail to find statistically 
significant relationships over only Bears patches, possibly due to the small number (16) of 
successfully auto-repaired Bears bugs with multi-line human patches.


\section{Test case-based validation}

In search based program repair, one key area of research is to find ways to validate
candidate patches by
measuring how "close" a candidate patch is to a full repair (a fitness function),
and this is usually done via measuring unit test performance of the candidate patch. 

\rqorinsight{RQ?: Do test cases fully capture the effects of multiple edits?}

Sometimes not all edits in a human patch is necessary to pass all tests. This could be
due to human patch including refactoring or other changes that does not actually
change code behavior, or it could be due to the unit tests not being able to catch
an incorrect behavior that the remaining edits correct. We would like to explore
to what extent does this phenomenon affect the bugs in our benchmarks.

\rqorinsight{RQ?: How well do test case based validation methods identify partial repairs?}

Given a buggy program and a valid repair, if we apply a part of the valid repair to 
the buggy program (a partial repair), then semanticly the partial repair is closer
 to the full repair compared to the original. 
Ideally, we would want the fitness function to guide the search towards a full 
repair by identifying partial repairs as "closer" to full repair than original.

However, sometimes partial repairs will perform no different in unit tests compared 
to the original program, and other times they could perform worse. We want to find 
out how often each of these situations happen.

Moreover, unit test performance can be measured in different granularity levels. 
We would like to compare the performance of the fitness functions at identifying 
partial repairs at different granularity levels.


\subsection{Method}

To address RQ? and RQ?, we designed the following experiment: for each bug in Defects4j 
(excluding Clojure bugs because they dont use JUnit) 
and Bears (single-module only), we generate all of its partial repairs using method described in 
the Partial Repairs subsubsection, and then run unit tests through all partial repairs
as well as the original buggy program on three different granularity levels (described
in details in the Granularity subsubsection), and compare the results.

\subsubsection{Partial Repairs}

To construct partial repairs for each bug, we apply a struct, non-empty subset 
of chunk-granularity edits from the human patch to the broken code.
In order to minimize the number of syntactically invalid partial repairs, 
we pre-add, prevent the deletion of, or prevent the narrowing of scope of 
import statements, helper methods, and variable declarations.
For example, if a patch contains a chunk that declares a new variable 
\texttt{var} and a second chunk that might use \texttt{var}, then we 
pre-add the declaration of \texttt{var} from the first chunk. 
We argue that the semantic behavior of partial edits is more interesting 
than their syntactic correctness, which prompts us to apply the 
aforementioned preprocessing.

We do not split chunks into two when applying any of our preprocessing steps. 
If a chunk, however, only contains edits that we pre-apply or remove during 
preprocessing, then we discard the now-empty chunk from the set of 
potential edits to apply. We eliminate bugs whose human patches contain only 
one chunk after preprocessing (for example, a 2-chunk patch that adds a new 
helper method in one chunk and invokes the helper method in the other). 

Due to the exponential growth of the number of partial repairs with respect 
to the number of available edit operations, we evaluate bugs whose patches 
contain between two and six chunks after preprocessing.
Moreover, we exclude Defect4J's Closure compiler bugs due to 
their nonstandard test suite setup.
We are left with 97 Defects4J and 64 Bears bugs.

\subsubsection{Granularity}

There are different ways to compare unit test results, and the most common ways 
are class-level granularity and method-level granularity. At Class-level granularity, 
we only look at which test classes passed and which failed; at method-level 
granularity, we look at which test methods passed and which failed.

Here we introduce a third level of granularity: assertion-level granularity. 
For each test method $M$, let $A(M)$ be the set of all assert statements in $M$. 
When $M$ is run, if an assertion failed, the failure is recorded and the method 
is allowed to continue to run (as opposed to normally the test method throws an 
error and terminates). After running the method, for each assert statement 
$a\in A(M)$, let $b(a)$ be 1 if $a$ never failed once during the running of $M$, 
and 0 otherwise. We define the assertion score of $M$ to be 
$AS(M)=\frac{\Sigma_{a\in A(M)}b(a)}{|A(M)|}$. If $M$ failed to run to completion 
due to timeouts or exceptions that are not related to assertions, then we define 
$AS(M)=0$. Thus by definition, $AS(M)=1$ if $M$ passes. If a program passed more 
assertions in $M$, there should be an increase in $AS(M)$.


\subsection{Results}

With 97 Defects4J and 64 Bears bugs, we evaluate
a total of 898 partial repairs for Defects4j and 444 partial repairs for Bears.
Out of these bugs, 32 out of 97 bugs in Defects4j and 
34 out of 64 bugs in Bears has at least one chunk that
is unnecessary to pass all unit tests.

Due to the phenomenon described in the previous section, we decided to present the
data of this experiment in two ways: always view full provided human patch as full repair
(Unminimized), and viewing minimum set of edit chunks in provided human patch necessary
to pass all unit tests as full repair (Minimized).
Some bugs ends up with chunk number 1 after minimization, 
so they are not included in the Minimized results. The minimized results 
include 75 remaining bugs (544 partial repairs) in defects4j and 38 remaining bugs (156 partial repairs) of Bears. 

Since each bug may have different 
number of partial repairs, we analyzed the results in two different ways: 
Unweighted (where each partial repair is weighted equally), and weighted (where each bug 
has equal weight, so if a bug has n partial repairs each of them has weight 
$\frac{1}{n}$). We record the percentage of partial repairs that performed better (positive), same (neutral)
or worse (worse) compared to original code in each assertion level. Non compiling
partial repairs do not count towards any of the three categories.

The full unminimized result is presented in Table 9 and minimized results in Table 10.

\begin{table}
{\begin{center}
\begin{tabular}{| l | l | r | r | r | r | r | r |}
\hline
\multicolumn{2}{|c|}{Dataset} &\multicolumn{2}{|c|}{Defects4j} & \multicolumn{2}{|c|}{Bears} \\
\hline
\multicolumn{2}{|c|}{Weightedness} & U & W & U & W  \\
\hline
Positive & Class & 11.36\% & 20.46 \% & 29.05 \% & 27.60\%  \\
Positive & Method & 35.41\% & 41.25 \% & 37.61 \% & 31.29\%  \\
ositive & Assertion & 40.42\% & 48.25 \% & 38.96 \% & 31.60\%  \\ 
\hline
Neutral & Class & 68.04\% & 65.53 \% & 65.09 \% & 56.84\% \\
Neutral & Method & 36.64\% & 39.33 \% & 55.63 \% & 51.60\%  \\
Neutral & Assertion & 27.62\% & 31.73 \% & 49.32 \% &  47.48\%  \\ 
\hline
Negative & Class & 12.03\% & 7.18 \% & 4.05 \% & 6.70\%  \\
Negative & Method & 19.27\% & 12.52 \% & 4.95 \% & 8.26\%  \\
Negative & Assertion & 21.83\% & 12.90 \% & 9.68 \% &  11.29\%  \\ 
\hline
\end{tabular}
\end{center}}
\caption{Full Unminimized Results at all granularity levels, "U" stands for Unweighted, "W" stands for weighted}
\end{table}


\begin{table}
{\begin{center}
\begin{tabular}{| l | l | r | r | r | r | r | r |}
\hline
\multicolumn{2}{|c|}{Dataset} &\multicolumn{2}{|c|}{Defects4j} & \multicolumn{2}{|c|}{Bears} \\
\hline
\multicolumn{2}{|c|}{Weightedness} & U & W & U & W  \\
\hline
Positive & Class & 5.30\% & 9.78 \% & 12.18 \% & 10.59\%  \\
Positive & Method & 34.81\% & 36.80 \% & 23.72 \% & 19.71\%  \\
Positive & Assertion & 40.28\% & 45.91 \% & 27.56 \% & 20.24\%  \\ 
\hline
Neutral & Class & 77.56\% & 73.56 \% & 73.72 \% & 73.75\% \\
Neutral & Method & 40.11\% & 39.95 \% & 62.18 \% & 64.62\%  \\
Neutral & Assertion & 31.98\% & 30.30 \% & 49.36 \% &  59.19\%  \\ 
\hline
Negative & Class & 9.89\% & 7.64 \% & 10.90 \% & 10.84\%  \\
Negative & Method & 17.67\% & 14.13 \% & 10.90 \% & 10.84\%  \\
Negative & Assertion & 18.55\% & 14.39 \% & 19.23 \% &  14.44\%  \\ 
\hline
\end{tabular}
\end{center}}
\caption{Full minimized Results at all granularity levels, "U" stands for Unweighted, "W" stands for weighted}
\end{table}

    
\subsection{Conclusions}

\subsubsection{RQ?}
About a third of Defects4j bugs and over half of
Bears bugs in this experiment do not need all chunks in the provided human patches to pass
all unit tests. This suggests that significant portion of multi-chunk bugs contains some
chunk that has completely no effect on the test case behavior. This could be due to some
edit chunk is just human developer refactoring code that doesn't change code behavior,
or it could be due to test cases not being able to detect incorrect behavior that is fixed
by the "unnecessary" chunk.

\subsubsection{RQ?}

Under highest granularity, we are able to identify above 40\% of the partial repairs in Defects4j
and above 20\% of the partial repairs in Bears. Only about 30\% of Defects4j partial repairs and 50\%
of Bears partial repairs are neutral and below 22\% of Defects4j partial repairs and below 20\% 
of Bears partial repairs are negatively identified. This is pretty good. \todo{change "this is pretty good"}

The experiment result shows that higher granularity levels are better at identifying partial repairs positively, 
but they also increase the chance of identifying partial repairs negatively.
This tradeoff can behave differently on different datasets. For example,
Assertion Level Granularity performed much better than Method Level Granularity
in Defects4j (up to 10\% more positively identified partial repairs, with less than
1\% more negatively identified), but this is not the case in Bears (less than 2\%
more positively identified partial repairs, with 3-8\% more negatively identified).
Thus, it is recommended that researchers should 
carefully balance the tradeoffs in granularity level in test case based validation
in APR research.


\subsection{Limitations}

This experiment breaks up full repairs into chunks and treats each chunk as a 
single edit action. However, in reality most APR techniques mutate code at 
a more granular level.
Also, this experiment only checks whether fitness functions 
can identify partial repairs, and not concerned with how to come up with these 
partial repairs (depending on specifics of the APR 
techniques, some may not be in the search space).

Regardless of its limitations, this experiment is provides valid and valuable 
information to tackling the challenge of automatically repairing bugs that 
requires multiple edit actions to fully repair 
and provides insight in future APR research.


\section{Related Work}

Qi et al.~\cite{patch-correctness} evaluated the patches generated 
by three G\&V repair tools~\cite{genprog, ae, rsrepair} and presented 
Kali, a G\&V tool that exclusively deletes functionality. They found the 
vast majority of generated patches to be incorrect and equivalent to 
a single functionality deletion. Moreover, they found that Kali, whose 
smaller search space consists entirely of functionality-removing 
operations, generates at least as many correct patches as the 
other three tools. Later work found patch incorrectness to be 
also problematic in Defects4J~\cite{d4j-eval} and in semantics-based 
repair techniques~\cite{Le2018}.

% I know I'm referring to the work by the authors' names,
% but I can't find a good way to refactor their names out without writing awkwardly.
% I would need to refactor all of the usages of "they" to remove their names.
Long and Rinard~\cite{long-search-spaces} studied the prevalence of 
correct and incorrect plausible patches in the search spaces of SPR~\cite{spr} 
and Prophet~\cite{prophet}. They found incorrect plausible patches to outnumber 
correct patches by orders of magnitude. When they increased the search space 
by adding additional mutation operations, they found an increased number of 
correct patches, but APR tool performance might actually degrade due to a 
simultaneous increase in incorrect plausible patches and the combinatorial 
growth of the search space.

Zhong and Su~\cite{zhong2015} did an empirical study on real bug fixes. 
They studied the difficulty of fault localization, the complexity of fixing bugs, 
necessary mutation operators, importance of API knowledge, the types of buggy files, 
and addition/deletion of files in bug fixing on over 9000 real-world bugs collected via BUGSTAT, 
and identifies key insights on fault localization, faulty code fix, search space and non-source bugs. 
Both our paper and Zhong and Su's paper aims to provide useful guidance and insights for 
improving state-of-the-art APR techniques through empirical studies of bugs and bug fixes. 
In contrast, our study focuses on one specific category of bugs: 
source file bugs that requires multiple edit actions to successfully repair, 
drawing insights on their behaviors in fault localization, fitness evaluations and dependency.

Wang et al.~\cite{wang2018} did an empirical study of multi-entity changes in real bug fixes 
(where each entity is a class, method or field). Their research questions mostly focused on 
how often and why do real-world bug fixes have multi-entity changes, the relationship 
between co-changed entities, and the recurring patterns of those multi-entity changes. 
Through analyzing 2854 real-world bugs from four projects, they found that 66\%-76\% 
multi-entity fixes are closely related to each other via syntactic dependencies, 
and they identified three major recurring patterns that connects co-changed entities. 
They suggested a potential way to close the gap between APR fixes and real fixes by 
enhancing APR to incorporate multi-entity changes. In contrast, our study on bugs that
requires multiple edits to fix, where the edits may be in the same entity. We define atomic 
changes (single edit) differently, and we studied interactions between edits 
(i.e. the lines that changed in the bug fix) instead of entire entities.

Previous efforts in G\&V program repair to derive more search-guiding information 
during candidate patch evaluation 
include using program invariants~\cite{better-fitness, dinglyu}, 
intermediate program values~\cite{source-code-checkpoint}, 
and online mutation-based fault localization~\cite{mut-analysis}.
Some approaches require additional input, such as suspicious variables~\cite{source-code-checkpoint} 
or known patches for the bug under repair~\cite{better-fitness}, 
while others exhibit limited performance improvements~\cite{dinglyu, mut-analysis}.
\todo{Describe how our work can spark future work in better evaluating candidate patches.}

Schulte et al.~\cite{schulte} did an empirical study on software mutation robustness 
(i.e. how often do code mutations remain neutral in test results). 
They found that in a large collection of off-the-shelf softwares the mutation robustness is about 37\%, 
and discussed potential application of mutation robustness to proactive bug repair. 
In contrast, our study focuses on automatically fixing current bugs (i.e. bugs that fail an existing unit test), 
and we do not restrict our repair actions to neutral variants of the program.

\section{Limitations}

\subsection{Possibly Unnecessary Edits in Patches}

We found instances in our data set where not all edits are required to 
make the faulty program pass all tests. One possibility is that the extra 
edits satisfy untested specifications. Another possibility is that the edits 
do not affect program functionality (e.g.: refactoring edits). In the test case based
validation section, we addressed this issue by minimization. We assume 
that all edits are functionally relevant. We intend to study such seemingly
extraneous edits in future work.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
