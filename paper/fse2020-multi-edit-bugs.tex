\documentclass[sigconf, timestamp-false, anonymous=true]{acmart}

\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{fancybox}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}

\acmConference[ESEC/FSE '20]{ESEC/FSE '20: ACM Joint European Software Engineering Conference and Symposium 
on the Foundations of Software Engineering}{November 8-13, 2020}{Sacramento, CA, USA}
\acmYear{2020}

\newcommand\todo[1]{\textcolor{red}{#1}}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{A Study of Multi-Edit Bug Patches}

%add authors & shortauthors if we are lucky :)

\begin{abstract}
  \todo{note that this is probably full of falsehoods, I'm just thinking by
    typing.} Automatic program repair is a promising approach for reducing the
    cost of quality assurance practices and faulty software. To date, most
    techniques proposed for test-driven automatic repair have succeeded
    primarily on bugs that benefit from short, single-edit patches. Techniques
    that succeed on multi-edit bugs often do so by patching them in an
    alternative, single-edit way, or by targeting particular multi-edit bug
    patterns. Empirical studies of real-world similarly tend to focus on the
    patterns exhibited by single-edit bugs, and have not examined repairability
    of multi-edit bugs in detail. We present a comprehensive empirical analysis
    of multi-edit bugs in open source Java programs, focusing on static and
    dynamic properties that define the repair search space for a given bug (and
    thus, in turn, the challenges that apply to automatically addressing them).
    This analysis focuses on the key challenges of the dynamic program repair
    problem: the \emph{mutations and fix code} used to repair multi-edit bugs;
    the \emph{fault locations} and their relationships; and the \emph{objective
      function}, and in particular how and to what degree test cases can be used
    (or not) to identify partial repairs. We identify key takeaways and
    challenges, with implications for future work in expressive, multi-chunk bug
    repair.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011102</concept_id>
<concept_desc>Software and its engineering~Software defect analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011784</concept_id>
<concept_desc>Software and its engineering~Search-based software engineering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software defect analysis}
\ccsdesc[500]{Software and its engineering~Search-based software engineering}

\keywords{software bugs, program repair}

\maketitle


\newcommand{\rqorinsight}[2]{
  \setlength{\fboxsep}{0.8em}
  \vspace{0.5em}
  \begin{center}
  \Ovalbox{\begin{minipage}{0.9\linewidth}
    \textbf{Research Question:} #1
    \end{minipage}}
  \end{center}
  \vspace{0.5em}}

\section{Introduction}

Buggy software has a significant cost in terms of both social impact~\cite{tricentis} 
and developer effort~\cite{cambridge-study}. This
motivates significant research in techniques to automatically find and fix
them~\cite{whatever}, with the ultimate goal of reducing Quality Assurance costs
while improving software quality. 

A significant class of program repair techniques in both
research~\cite{genprog,angelix,Le17, Xuan17} and practice~\cite{sapfix} use test cases to guide
patch construction, typically by following what's known as a
generate-and-validate paradigm. At a high level, these techniques use test cases
to localize a defect --- identified by at least one failing test --- to a set of
likely-suspicious program locations. Then, they use a variety of techniques to
construct (or \emph{generate}) patch candidates for the bug in question,
checking each to see (or \emph{validate}) if any of them cause the program to
pass all of the provided test cases.  
%
Patches are constructed in a variety of ways, ranging from heuristic, syntactic
program manipulation~\cite{par,genprog,rsrepair,ae,prophet,hdrepair}, to specially adapted program
synthesis techniques~\cite{Konighofer11,Konighofer12,semfix,DeMarco14,angelix}. These techniques have successfully
repaired real, meaningful defects in large, complex
programs~\cite{angelix,genprog-eight-dollars,prophet,sapfix}.

Practically speaking, these techniques are typically limited in the types and
variety of defects they can repair. Often this is by design: techniques may
limit the repair search space to single-edit patches for \todo{(JL: ick)
  tractability}~\cite{rsrepair,ae,hdrepair}, while others only target certain
classes of bugs~\cite{Xuan17,sapfix,DeMarco14,par}. However, even techniques
that can in principle generate multi-edit repairs typically
don't~\cite{genprog,others}.

This leaves a large proportion of real-world bugs unrepairable by modern
research techniques in program repair.  Over half of the bugs in popular bug
benchmark Defects4J are patched by humans using multi-part
patches~\cite{d4j-dissection}. Approximately 70\% of buggy source files in a
large study of bugs in Apache projects required two or more edit
actions~\cite{zhong2015}.

A key tension in the design of an automated repair technique is the balance
between giving users confidence in patch correctness by maximizing its
subjective quality while managing a trivially-infinite search space. This space
is typically parameterized along several axes: (1) the \emph{fault space}:
potential program locations to be modified, (2) the \emph{mutation space}: which
modifications may be applied at a location, and (3) the \emph{fix space}: code
that may be instantiated for a specific mutation. For example, a repair
technique might identify the location of a null-pointer dereference (exploring
the fault space), decide to insert new code (mutation space), and synthesize
code to initialize the pointer (fix space). Many dynamic repair techniques can
be compared in terms of their choices along each of these axes, and traversal
strategies have first-order implications for scalability and for the type and
quality of patches produced. \todo{JL: I think a CT MBFL cite might be good here-ish.}

Multi-edit repair poses distinct challenges for each of these axes for repair.
Spectrum-based fault localization~\cite{ochiai}, the most prevalent class of
fault localization techniques used in program repair, does not specifically
identify sets of locations that might be related or repaired together; indeed,
the evaluation of most fault localization techniques typicaly assumes that a bug
is localized if any one buggy line is identified~\cite{fl-survey-wong}.
Researchers have observed that some bugs are
repaired in multiple locations using very similar
code~\cite{saha2019harnessing,jiang2019cmsuggester}, informing novel techniques
that constrain the \emph{fix space} of possible multi-edit repairs accordingly.
One key question for applicability of these types of techniques is how prevalent
such bugs are, and how often multi-edit bugs instead require multiple
coordinating (but ultimately different) edits or pieces of fix code.  Over 50\% of the fixes in four 
Apache projects involve two or more entities -- i.e., a Java class, method, or field -- and 66\%-76\% of 
those multi-entity fixes involved syntactic dependencies.~\cite{wang2018}. 
Test cases are used to evaluate candidate repairs in virtually all dynamic
generate-and-validate repair techniques, but, anecdotally, may not be effective
at identifying partial repairs in a multi-edit
context~\cite{fitness10,maybeeric}.  
Although multi-edit repair has been discussed in the context of other analyses
that study bug fix characteristics in general~\cite{examples} as well as for
repair applicability specifically~\cite{moar,examples}, to the best of our
knowledge there has been no significant previous study of the characteristics of
multi-edit repairs in terms of their implications for repairability or, more
broadly, program repair.  

In this paper, we present a systematic study of real-world multi-edit bugs,
looking specifically at their characteristics with respect to the problem of the
automatic repair search space.  
We study the bugs curated in two
real-world datasets that support program repair research: Defects4J~\cite{defects4j}
and BEARS~\cite{bears}, in total, 578 bugs in 40 projects. 
More than half of these bugs were repaired by a
human developer using a multi-chunk repair (as we define in
Section~\ref{sec:background}).  We look at characteristics along each of the
relevant axes of the program repair search problem: fault locations, mutation
operators, fix code, and evaluation (or fitness or objective) using test cases.  Our 
contributions are
as follows:

\begin{itemize}
\item An analysis of multi-edit bug patches in Defects4J and Bears.
\item A study characterizing the ways that failing tests cover faulty locations as a means of 
testing whether the underlying hypothesis in current fault localization techniques still hold 
in multi-edit bugs. We find that 58\% of 
bugs with multi-edit patches and multiple failing tests have faulty locations that are not 
covered by all failing tests, thus the underlying assumptions do not necessarily hold.
\item A study characterizing fix code.  We find that half of 
bug patches contain dependent edits and \todo{N\%} contain 
code clones.
\item A new fitness function that measures patch quality using the number of 
	passing and failing assertions within test cases.
\item study of test cases as a means to evaluation partial and full
  multi-site repairs.  We find that there are significant amount of edit chunks that
  is not captured by test cases, as a third of Defects4j multi-chunk bugs and over a half of 
  Bears multi-chunk bugs do not require all edit chunks in their provided human patches to
  pass all test cases. Other than that, we found that, under highest granularity (assertion level), test case
  based validation methods can positively identify over 40\% of partial repairs, 
  while less than 40\% partial repairs are neutral in test cases and less than 20\% are identified
  negatively. Moreover, we found that higher granularity will increase the number of
  positively identified partial repairs, but it will also increase the number of negatively 
  identified partial repairs, and the amount of increase/decrease may vary between
  datasets.
\item  study of code clones. We find that over 30\% multi-chunk bugs
  have almost same edits applied to different locations. Moreover, bugs that have 
  "disjoint" coverage result tend to be much more likely (19 out of 23) to have
  code clones.
\item \todo{A set of recommendations?}
\end{itemize}

\todo{This paragraph is a lie and some people hate them so maybe we'll cut it,
  it just feels comforting to me to write them as I'm working on a paper draft...}
The rest of this paper is organized as follows.  We provide background on
generate-and-validate program repair, with a focus on the key parameters of the
program repair search problem, in Section~\ref{sec:background}.  We describe the results of our analysis in
Section~\ref{sec:results}, with discussion of implications as well as
limitations in Section~\ref{sec:discussion}.  Related work is discussed in
Section~\ref{sec:related}; Section~\ref{conclusions} concludes.

\section{Key Concepts and Research Questions}
\label{sec:background}

\paragraph{Search spaces in automatic program repair.} Automatic program repair (APR) techniques aim to find patches for bugs in
programs.  We restrict attention to dynamic or test-case guided program repair,
which describes the majority of research advancements over the past ten years.
Such techniques take as input a program and a set of test cases that serve as
the oracle for program correctness.  At least one of those tests should be
failing, corresponding to the bug to be repaired.  

At a high level, any APR technique aims to solve a search or optimization
problem, seeking a set of edits (or patch) to the program that will lead it to
pass all of the provided tests.  Although The repair search is classically defined along
the following axes:
\begin{itemize}
\item \emph{Fault space.} The first problem in fixing a bug concerns
  \emph{where} in the code modifications should be applied. All dynamic repair
  techniques begin by using the provided test cases as input to a fault
  localization technique. Such techniques identify (and typically score)
  suspicious code based on which test cases execute which pieces of code.
  Although the particulars of the fault localization employed can vary, most APR
  use some variant of spectrum-based fault localization (SBFL)~\cite{refs} in
  this first step. The resulting computation defines the fault space, or the
  candidate locations or code entities considered for repair. This first step is
  key to reducing the search space.

\item \emph{Mutation space.} The mutation space is the set of applicable 
modifications at a program location. Examples include GenProg's append, 
replace, and delete operators over statements~\cite{genprog-operators} and 
Nopol's condition replacement over if-statements and missing precondition 
addition over non-branch and non-loop statements~\cite{Xuan17}.
A larger mutation space generates 
a richer search space with potentially more repairs, but
the resulting combinatorial growth of the search space can also hinder 
the search process~\cite{long-search-spaces}.

\item \emph{Fix code.}

\item \emph{Objective.} 

\end{itemize}



\paragraph{Edit granularity in bug-fixing patches.}  Developer patches for
program bugs naturally vary in length and complexity.  Estimates range from
\todo{FIXME} to \todo{FIXME}\% of bugs are repaired using fewer than
\todo{FIXME} lines of code.  \todo{rephrpase/repeat stuff in intro about
  studies/prevalence of multi-edit bugs, but expand the rephrase of the results
  in that prior work to contrast with incidence of single-edit bugs.}  

There are indeed several plausible definitions of multi- vs. single-edit bugs,
with implications for how they are studied.  We consider edits at the granularity 
of lines and of chunks. We define a \emph{line~edit} as an added, removed, or modified 
line. We define a \emph{chunk} as a continuous sequence of line edits, with the exception 
that if one chunk adds a \texttt{\{} whose corresponding \texttt{\}} is in a different chunk, 
then we merge the two chunks together. 
We ignore changes to comments, whitespace, or import statements.

Alternative definitions may have bearing on our results.  We argue that this
definition is natural for the purposes of our study, which focuses in particular
on understanding larger bugs---especially multi-chunk bugs---for the purposes of 
adapting, extending, or applying new program repair techniques to address them. 
\todo{A couple more sentences re: why this is reasonable. Might be solved by 
pointing to low rates of repair for multi-chunk bugs.}

\paragraph{Research questions.}  The parameterization of the program repair
search problems into multiple subspaces informs our empirical study of those
spaces for the purposes of understanding multi-edit repair.  We address the
following research questions:

\begin{enumerate}
\item \emph{RQ1: How prevalent are multiedit human patches are in Defects4J and Bears?}
\item \emph{RQ2: Do failing tests cover different portions of the fault?}
\item \emph{RQ3: How prevalent are dependencies between edited code?}
\item \emph{RQ4: How often do code clones occur in multi-chunk bugs? Are existence of 
code clones in human patch correlated with specific patterns of fault localization?}
\item \emph{RQ5: Do test cases fully capture the effects of multiple edits?}
\item \emph{RQ6: How well do test case based validation methods identify partial repairs?}



\end{enumerate}

\section{Dataset Characteristics}
\label{sec:data-rq1}

\begin{table*}
\begin{center}
\begin{tabular}{l | rrr | rr | rr | rr | rr}
\toprule
\multicolumn{12}{c}{\textbf{Defects4J}} \\
\midrule
Project & Bugs & Src (kloc) & Test (kloc) & \multicolumn{2}{c}{Multi-line} & \multicolumn{2}{c}{Multi-chunk} 
		& \multicolumn{2}{c}{Multi-test} & \multicolumn{2}{c}{Multi-chunk \&}\\
&&&&\multicolumn{2}{c}{bugs}&\multicolumn{2}{c}{bugs}&\multicolumn{2}{c}{bugs}&\multicolumn{2}{c}{Multi-test bugs}\\
\midrule
JFreeChart  & 26 & 193.3 & 74.6 & 17 & 65\% & 11 & 42\% & 10 & 38\% & 7 & 27\%\\
Closure compiler & 133 & 150.6 & 112.6 & 102 & 77\% & 55 & 41\% & 58 & 44\% & 31 & 23\%\\
Apache commons-lang & 65 & 57.8 & 47.4 & 51 & 78\% & 32 & 49\% & 17 & 26\% & 13 & 20\%\\
Apache commons-math & 106 & 45.0 & 41.5 & 80 & 75\% & 53 & 50\% & 28 & 26\% & 22 & 21\%\\
Mockito & 38 & 23.0 & 28.5 & 29 & 76\% & 16 & 42\% & 18 & 47\% & 8 & 21\%\\
Joda-Time & 27 & 82.9 & 70.4 & 24 & 89\% & 17 & 63\% & 13 & 48\% & 9 & 33\%\\
\midrule
All (Defects4J) & 395 & 552.6 & 375.0 & 303 & 77\% & 184 & 47\% & 144 & 36\% & 90 & 23\%\\
\midrule
\multicolumn{12}{c}{\textbf{Bears (single-module)}} \\
\midrule
FasterXML-jackson-databind & 26 & 95.7 & 53.6 & 24 & 92\% & 17 & 65\% & 4 & 15\% & 2 & 8\%\\
INRIA-Spoon & 62 & 66.2 & 30.8 & 56 & 90\% & 39 & 63\% & 23 & 37\% & 18 & 29\%\\
spring-data-commons & 15 & 45.8 & 28.8 & 14 & 93\% & 9 & 60\% & 6 & 40\% & 2 & 13\%\\
traccar-traccar & 42 & 47.9 & 8.6 & 34 & 81\% & 24 & 57\% & 3 & 7\% & 2 & 5\%\\
30 other projects & 38 & - & - & 28 & 74\% & 22 & 58\% & 36 & 95\% & 9 & 24\%\\
\midrule
All (Bears) & 183 & >255.6 & >121.8 & 156 & 85\% & 111 & 61\% & 72 & 39\% & 33 & 18\% \\
\midrule
Combined (Defects4J \& Bears) & 578 & >808.2 & >496.8 & 459 & 79\% & 295 & 51\% & 216 & 37\% & 123 & 21\%\\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:dataset-characteristics} Characteristics of the Defects4J (top) and Bears (bottom) datasets.}
\end{table*}

\paragraph{Datasets}  Our study requires a dataset of indicative, real-world,
multi-edit defects.  We study both the defects in Defects4J
Defects4J~\cite{defects4j} and Bears~\cite{bears}.  Table~\ref{tab:dataset-characteristics}
summarizes these datasets, both of which
consist of historical
bugs found in real world software projects. Defects4J contains 395 bugs from 
six Java software projects, and is currently very popular dataset for evaluating 
program repair tools that target Java~\cite{durieux-repair-them-all}.
The dataset's patches are manually minimized to isolate the bug fix 
and exclude edits such as refactorings, feature additions, et cetera.

With any dataset, however, there is a risk associated that tools may overfit
to the defects in question, and there is evidence that this situation applies to
program repair and Defects4j~\cite{durieux-repair-them-all}. 
We thus also study bugs from Bears~\cite{bears}, 
a set of Java bugs derived from failed Travis-CI builds of GitHub projects. 
Bears offers 251 bugs from 72 software projects, providing a greater diversity of 
projects compared to Defects4J. 
Several projects in Bears, however, are structured as multi-module projects, 
which are not currently compatible with our automation tools.
We thus limit our analysis of Bears to 183 bugs from 30 single-module projects.
We intend to address the remaining multi-module software projects in future work.

\subsection{Multi-edit Prevalence}
\rqorinsight{RQ1: How prevalent are multi-edit human patches are in Defects4J and Bears?}

We start our analysis of multi-edit patches by asking how many such patches 
exist in our benchmarks. We analyze edits at line and chunk granularities,
which we defined in Section~\ref{sec:background}.

\paragraph{Results} 
Table~\ref{tab:dataset-characteristics} lists the number and percentages of
multi-line and multi-chunk patches in Defects4J and Bears. 
Multi-line patches comprise a large majority of patches across both datasets, 
while multi-chunk patches comprise over half of Bears and almost half of Defects4J.
Although a multi-edit human patch for a bug does not imply the 
non-existence of a simpler patch, the high proportion of bugs that have 
multi-edit patches demonstrates the relevance of such bugs to fault localization and
program repair. 

At both levels of edit granularity, Bears contains a greater proportion of 
multi-edit patches compared to Defects4J. This may be the 
result of manual patch minimization in Defects4J~\cite{defects4j} 
and lack thereof in Bears.
Thus, some Bears patches may be multi-edit as a result of additional 
changes added for non-repair reasons.


\input{coverage.tex}



\section{Mutation Operators and Fix Code}
\label{sec:mutops}

Given suitably selected fault locations, APR techniques vary in the types of
mutation operators they consider, how they select between them, and how they
select new fix code to instantiate them, as necessary.  For example, a naive
approach with only \texttt{insert}, \texttt{replace}, and \texttt{delete}
operators must choose between them at a location and, in the case of
\texttt{insert} and \texttt{replace}, choose code to insert/replace at that
location.  
%
The few techniques that handle or at least enable multi-edit patches vary in their
handling of mutation operator selection and instantiation.  At one
extreme, semantics-based repair~\cite{s3,angelix} can represent dependent edits between multiple
locations as a conjunction of multiple constraints to simultaneously solve,
bounded by some number of edits that are computationally feasible, while
restricted to a relatively small library of possible code components for use in
the inductive synthesis problem.    At the other extreme, search-based or
evolutionary techniques~\cite{genprog,others} typically treat different mutation
operators independently.  That is, a modification in one location does not
inform the selection of a modifications to apply in a second location; instead,
the heuristic search is trusted to identify copacetic combinations.  The size of
the search space increases combinatorially in this context, however, rendering
the chances of finding suitable multi-edit repairs without additional guidance
quite low~\cite{ae,long2016}. Accordingly, heuristic techniques targeted at multi-edit
repair contexts~\cite{hercules,maybewang2018} make assumptions about the
shape of the search space to render it tractably constrained --- in particular,
targeting bugs that can be repaired by multiple syntactically similar pieces of
fix code.

These kinds of targeted technique surface the general questions about
multi-edit repairs that we seek to answer, with
implications for how they should be designed in new techniques movin forward.
In particular, we examine the \emph{relationship} between multiple edits, along
two dimensions:

\rqorinsight{
What are the dependencies (control flow- or data-) between multiple edits or
edit locations?
}

and

\rqorinsight{
What is the \emph{syntactic relationship} between multiple edits or edit
locations?}



\subsection{Dependencies}

We first examine control- and data-dependencies between multiple edit locations. 

\paragraph{Methodology.} For dependency analysis, 
in order to get more granular edit information,
we consider edits at the granularity of lines instead of chunks.
We consider a patch to contain dependent line edits if there exists 
control or data dependencies between statements---and their used or defined 
variables---in added, deleted, or modified lines. We analyze deleted/modified 
lines in the pre-patch code and added/modified lines in the post-patch code
for dependencies.
  
For practical performance and scalability reasons, 
we perform intraprocedural analysis. 
We do, however, gather some interprocedural data dependency information 
using the following heuristics:
\begin{itemize}
	\item If a statement invokes a method, then we assume that
	the statement reads all variables used in the method arguments.
	\item If a statement invokes a getter method \texttt{Class.getX()} 
	(for any \texttt{Class} and \texttt{X}), then we heuristically 
	assume that the statement reads \texttt{Class.X}. 
	Note that \texttt{Class.X} does not need to actually exist.
	\item If a statement invokes a setter method \texttt{Class.setX()}, 
	then we heuristically assume that the statement writes to \texttt{Class.X}. 
\end{itemize}

\paragraph{Results}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rr | rr | r}
		\toprule
		Dataset & \multicolumn{2}{c}{Dependent} & \multicolumn{2}{c}{Independent} & Total  \\
		\midrule
		Defects4J & 132  & 44\% & 171 & 56\% & 303 \\
		Bears & 95 & 61\% & 61 & 39\% & 156 \\
		Combined & 227 & 49\% & 232 & 51\% & 459 \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequencies and percentages of multi-line patches with(out) control or data 
	dependencies between edited lines.}
	\label{tab:dependency}
\end{table}

Table~\ref{tab:dependency} shows the 
frequencies and percentages of multi-line patches with control or data dependent 
line edits. \todo{Expand; reexplain, given precise numbers.}  We find a higher proportion of dependent patches in Bears compared to 
Defects4J, indicating higher patch complexity among multi-line patches in Bears.\todo{implications?}


\begin{table}
{\begin{center}
	\begin{tabular}{l | l | r r r r | r}
		\toprule
		Dataset & APR & Dependent & Independent & Total \\
		\midrule
		\multirow{2}{*}{Defects4J} & Success & 39 & 94 & 133 \\
		                                          & Failure   & 93 & 77 & 170 \\
		\midrule
		\multirow{2}{*}{Bears}       & Success &   9 &   7 &   16 \\
		                                          & Failure   & 86 & 48 & 134 \\
		\midrule
		\multirow{2}{*}{Combined}& Success & 48 &101& 149 \\
		                                          & Failure   &179&125& 304 \\
\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequency of multi-line patches with respect to the presence of 
	dependent line edits and whether an APR tool successfully 
	repaired the bug in~\cite{durieux-repair-them-all}.}
	\label{tab:dependency-repair-contingency-table}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rrr}
            	\toprule
		& Defects4J & Bears & Combined \\
		\midrule
		$P(\mbox{APR Success } | \mbox{ Dependent})$ & 30\% & 9\% & 21\% \\
		$P(\mbox{APR Success } | \mbox{ Independent})$ & 45\% & 13\% & 45\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Percentages of auto-repaired bugs in~\cite{durieux-repair-them-all} 
	that (do not) contain dependent line edits.\todo{Please combine this
          table with table 4 somehow, and/or explain to me why you shouldn't.
          Also, expand this caption, wherever it goes, to tell me what you mean
          and what the interpretation is; among other issues I don't understand how the cells are
          percentages when the rows are conditional probabilities.}
	\label{tab:dependency-repair-percents}}
\end{table}

\todo{Why are we asking this question/discussing these results?} 
Tables~\ref{tab:dependency-repair-contingency-table} and~\ref{tab:dependency-repair-percents}
show the frequencies and percentages of multi-line patches with respect to dependencies 
and whether an APR tool successfully repaired bug in~\cite{durieux-repair-them-all}.
We find that the presence of dependencies generally reduces the likelihood of APR tool success.
Using a $\chi^2$ test, we find statistically significant relationships ($p < 0.001$)
between APR success and control, data, and the disjunction of either dependencies 
for Defects4J and Defects4J $\cup$ Bears patches. We fail to find statistically 
significant relationships over only Bears patches, possibly due to the small number (16) of 
successfully auto-repaired Bears bugs with multi-line human patches.
\todo{What are the implications, in straightforward English prose?}


\subsection{Cloned code}

\rqorinsight{RQ4: How often do code clones occur in multi-chunk bugs? Are existence of 
code
clones in human patch correlated with specific patterns of fault localization?}

Previous research ~\cite{wang2018} suggested that one potential way to enhance
APR techniques is to allow them to apply a single edits to multiple locations.
This is based on the observation that human developers tend to make exactly the 
same edits in multiple locations when fixing bugs. Therefore, we want to study the
prevalence of code clones in human patches.

We will also attempt to look for patterns in fault localization of multi-chunk bugs
from the coverage experiment that may be correlated with existence of code clones in 
human patches.\todo{Why? I don't know that we actually need to motivate this
  here, we can do it in results/analysis, below.  But if we move this, we need
  to expand on the motivation.  Right now it reads as though we through a
  correlation analysis everywhere to see what it stuck to...what's the intuition
  behind why this test is reasonable?}
Possible intuition: if we can find some kind of correlation between fault localization results
and code clones, then if some APR research decided to follow Wang et al's suggestion and
include "repeat same edits at multiple location" operator, then our results may advise
the APR to be more likely to apply the repeat edit operator when the fault localization
result matches specific patterns.

\subsubsection{Method}
In this experiment, we will look at the existence of code clones in 
edit chunks (same definition as in fitness experiment) of multi-chunk bugs (selected
with same criteria as fitness experiment, but also includes Closure bugs in Defects4j).

Then we will compare the results with the results of the coverage experiment in
Section~\ref{secFL}. 

In this experiment, we call two chunks are code clones if it is one of the following four cases
\begin{enumerate}
\item Same Case: The two chunks are alpha-equivalent. 
\item Literal Case: The two chunks differ by at most one constant or arithmetic operator,
 or replacement of one constant with variable 
\item Composite Case: All edit lines in one chunk is inside another chunk in the same order (the other chunk may include
more lines), essentially the other chunk "contains" a copy of this chunk.
\item Move Case: Two chunks forms a "movement" of lines of code (i.e. one chunk inserts and the other deletes the same
  content at different locations, essentially moving the lines of code from one location to another)
\end{enumerate}

\subsubsection{Results and Conclusions}

\begin{table}
{\begin{center}
\begin{tabular} {| l | r | r | r |}
\hline
& Defects4j & Bears & Combined \\
\hline
Same & 35 & 9 & 44  \\ 
Literal & 11 & 2 & 13  \\
Composite & 7 & 1 & 8  \\
Move & 5 & 1 & 6  \\ \hline
Any & 56 & 13 & 69  \\ \hline
No clones & 96  &  51 & 147 \\ \hline
Total & 152 & 64 & 216 \\ \hline
\% with Clones & 36.8\% & 20.3\% & 31.9\% \\ \hline
\end{tabular}
\end{center}
}
\caption{Multi-chunk bugs with each kind of chunk-level code clones}
\end{table}

As shown in Table ?, out of 216 multi-chunk bugs in Bears (single module) and Defects4j (all) with between 2-6 chunks,
69 of them had at least two chunks that are code clones. Thus over 30\% of multi-chunk bugs seems
to have chunk-level code clones, indicating prevalence of code clones in human patches. Note that "Any" may not
be the sum of the previous four rows because some bug could contain multiple pairs of code clones that
belong to different categories.

\begin{table}
{\begin{center}
\begin{tabular} {| l | r r | r |}
\hline
& Disjoint & Non-Disjoint & Total \\
\hline
Has Clone & 19 & 37 & 56 \\
No Clone & 4 & 102 & 106 \\ \hline
Total & 23 & 139 & 162 \\ \hline
\end{tabular}
\end{center}
}
\caption{Multi-chunk bugs categorized on Disjointness and Code clones (without Closure)}
\end{table}

Out of all bugs we checked for code clone analysis (excluding the Closure ones because they were not in the coverage experiment), there are 23 bugs labeled as "Disjoint" in the coverage experiment, out of which
19 had code clones (14 Same, 4 Literal, 1 Composite). Full results available in Table ?. This indicates that a 
bug with disjoint coverage result is very likely to contain code clones in its
human patch. This result could suggest future APR techniques to apply the same edits to multiple locations
if the coverage result of the bug can be categorized as "Disjoint". 

\todo{YL: out of 69 code clones, 7 are in diff class, 26 are in same class diff method, and
36 are in same method. Do we want to include these info too?}


\section{Test case-based validation}

In search based program repair, one key area of research is to find ways to validate
candidate patches by
measuring how "close" a candidate patch is to a full repair (a fitness function),
and this is usually done via measuring unit test performance of the candidate patch. 

\rqorinsight{RQ5: Do test cases fully capture the effects of multiple edits?}

Sometimes not all edits in a human patch is necessary to pass all tests. This could be
due to human patch including refactoring or other changes that does not actually
change code behavior, or it could be due to the unit tests not being able to catch
an incorrect behavior that the remaining edits correct. We would like to explore
to what extent does this phenomenon affect the bugs in our benchmarks.

\rqorinsight{RQ6: How well do test case based validation methods identify partial repairs?}

Given a buggy program and a valid repair, if we apply a part of the valid repair to 
the buggy program (a partial repair), then semanticly the partial repair is closer
 to the full repair compared to the original. 
Ideally, we would want the fitness function to guide the search towards a full 
repair by identifying partial repairs as "closer" to full repair than original.

However, sometimes partial repairs will perform no different in unit tests compared 
to the original program, and other times they could perform worse. We want to find 
out how often each of these situations happen.

Moreover, unit test performance can be measured in different granularity levels. 
We would like to compare the performance of the fitness functions at identifying 
partial repairs at different granularity levels.


\subsection{Method}

To address RQ5 and RQ6, we designed the following experiment: for each bug in 
Defects4j 
(excluding Clojure bugs because they dont use JUnit) 
and Bears (single-module only), we generate all of its partial repairs using method described in 
the Partial Repairs subsubsection, and then run unit tests through all partial repairs
as well as the original buggy program on three different granularity levels (described
in details in the Granularity subsubsection), and compare the results.

\subsubsection{Partial Repairs}

To construct partial repairs for each bug, we apply a struct, non-empty subset 
of chunk-granularity edits from the human patch to the broken code.
In order to minimize the number of syntactically invalid partial repairs, 
we pre-add, prevent the deletion of, or prevent the narrowing of scope of 
import statements, helper methods, and variable declarations.
For example, if a patch contains a chunk that declares a new variable 
\texttt{var} and a second chunk that might use \texttt{var}, then we 
pre-add the declaration of \texttt{var} from the first chunk. 
We argue that the semantic behavior of partial edits is more interesting 
than their syntactic correctness, which prompts us to apply the 
aforementioned preprocessing.

We do not split chunks into two when applying any of our preprocessing steps. 
If a chunk, however, only contains edits that we pre-apply or remove during 
preprocessing, then we discard the now-empty chunk from the set of 
potential edits to apply. We eliminate bugs whose human patches contain only 
one chunk after preprocessing (for example, a 2-chunk patch that adds a new 
helper method in one chunk and invokes the helper method in the other). 

Due to the exponential growth of the number of partial repairs with respect 
to the number of available edit operations, we evaluate bugs whose patches 
contain between two and six chunks after preprocessing.
Moreover, we exclude Defect4J's Closure compiler bugs due to 
their nonstandard test suite setup.
We are left with 97 Defects4J and 64 Bears bugs.

\subsubsection{Granularity}

There are different ways to compare unit test results, and the most common ways 
are class-level granularity and method-level granularity. At Class-level granularity, 
we only look at which test classes passed and which failed; at method-level 
granularity, we look at which test methods passed and which failed.

Here we introduce a third level of granularity: assertion-level granularity. 
For each test method $M$, let $A(M)$ be the set of all assert statements in $M$. 
When $M$ is run, if an assertion failed, the failure is recorded and the method 
is allowed to continue to run (as opposed to normally the test method throws an 
error and terminates). After running the method, for each assert statement 
$a\in A(M)$, let $b(a)$ be 1 if $a$ never failed once during the running of $M$, 
and 0 otherwise. We define the assertion score of $M$ to be 
$AS(M)=\frac{\Sigma_{a\in A(M)}b(a)}{|A(M)|}$. If $M$ failed to run to completion 
due to timeouts or exceptions that are not related to assertions, then we define 
$AS(M)=0$. Thus by definition, $AS(M)=1$ if $M$ passes. If a program passed more 
assertions in $M$, there should be an increase in $AS(M)$.


\subsection{Results}

With 97 Defects4J and 64 Bears bugs, we evaluate
a total of 898 partial repairs for Defects4j and 444 partial repairs for Bears.
Out of these bugs, 32 out of 97 bugs in Defects4j and 
34 out of 64 bugs in Bears has at least one chunk that
is unnecessary to pass all unit tests.

Due to the phenomenon described in the previous section, we decided to present the
data of this experiment in two ways: always view full provided human patch as full repair
(Unminimized), and viewing minimum set of edit chunks in provided human patch necessary
to pass all unit tests as full repair (Minimized).
Some bugs ends up with chunk number 1 after minimization, 
so they are not included in the Minimized results. The minimized results 
include 75 remaining bugs (544 partial repairs) in defects4j and 38 remaining bugs (156 partial repairs) of Bears. 

Since each bug may have different 
number of partial repairs, we analyzed the results in two different ways: 
Unweighted (where each partial repair is weighted equally), and weighted (where each bug 
has equal weight, so if a bug has n partial repairs each of them has weight 
$\frac{1}{n}$). We record the percentage of partial repairs that performed better (positive), same (neutral)
or worse (worse) compared to original code in each assertion level. Non compiling
partial repairs do not count towards any of the three categories.

The full unminimized result is presented in Table 9 and minimized results in Table 10.

\begin{table}
{\begin{center}
\begin{tabular}{| l | l | r | r | r | r | r | r |}
\hline
\multicolumn{2}{|c|}{Dataset} &\multicolumn{2}{|c|}{Defects4j} & \multicolumn{2}{|c|}{Bears} \\
\hline
\multicolumn{2}{|c|}{Weightedness} & U & W & U & W  \\
\hline
Positive & Class & 11.36\% & 20.46 \% & 29.05 \% & 27.60\%  \\
Positive & Method & 35.41\% & 41.25 \% & 37.61 \% & 31.29\%  \\
ositive & Assertion & 40.42\% & 48.25 \% & 38.96 \% & 31.60\%  \\ 
\hline
Neutral & Class & 68.04\% & 65.53 \% & 65.09 \% & 56.84\% \\
Neutral & Method & 36.64\% & 39.33 \% & 55.63 \% & 51.60\%  \\
Neutral & Assertion & 27.62\% & 31.73 \% & 49.32 \% &  47.48\%  \\ 
\hline
Negative & Class & 12.03\% & 7.18 \% & 4.05 \% & 6.70\%  \\
Negative & Method & 19.27\% & 12.52 \% & 4.95 \% & 8.26\%  \\
Negative & Assertion & 21.83\% & 12.90 \% & 9.68 \% &  11.29\%  \\ 
\hline
\end{tabular}
\end{center}}
\caption{Full Unminimized Results at all granularity levels, "U" stands for Unweighted, "W" stands for weighted}
\end{table}


\begin{table}
{\begin{center}
\begin{tabular}{| l | l | r | r | r | r | r | r |}
\hline
\multicolumn{2}{|c|}{Dataset} &\multicolumn{2}{|c|}{Defects4j} & \multicolumn{2}{|c|}{Bears} \\
\hline
\multicolumn{2}{|c|}{Weightedness} & U & W & U & W  \\
\hline
Positive & Class & 5.30\% & 9.78 \% & 12.18 \% & 10.59\%  \\
Positive & Method & 34.81\% & 36.80 \% & 23.72 \% & 19.71\%  \\
Positive & Assertion & 40.28\% & 45.91 \% & 27.56 \% & 20.24\%  \\ 
\hline
Neutral & Class & 77.56\% & 73.56 \% & 73.72 \% & 73.75\% \\
Neutral & Method & 40.11\% & 39.95 \% & 62.18 \% & 64.62\%  \\
Neutral & Assertion & 31.98\% & 30.30 \% & 49.36 \% &  59.19\%  \\ 
\hline
Negative & Class & 9.89\% & 7.64 \% & 10.90 \% & 10.84\%  \\
Negative & Method & 17.67\% & 14.13 \% & 10.90 \% & 10.84\%  \\
Negative & Assertion & 18.55\% & 14.39 \% & 19.23 \% &  14.44\%  \\ 
\hline
\end{tabular}
\end{center}}
\caption{Full minimized Results at all granularity levels, "U" stands for Unweighted, "W" stands for weighted}
\end{table}

    
\subsection{Conclusions}

\subsubsection{RQ6}
About a third of Defects4j bugs and over half of
Bears bugs in this experiment do not need all chunks in the provided human patches to pass
all unit tests. This suggests that significant portion of multi-chunk bugs contains some
chunk that has completely no effect on the test case behavior. This could be due to some
edit chunk is just human developer refactoring code that doesn't change code behavior,
or it could be due to test cases not being able to detect incorrect behavior that is fixed
by the "unnecessary" chunk.

\subsubsection{RQ7}

Under highest granularity, we are able to identify above 40\% of the partial repairs in Defects4j
and above 20\% of the partial repairs in Bears. Only about 30\% of Defects4j partial repairs and 50\%
of Bears partial repairs are neutral and below 22\% of Defects4j partial repairs and below 20\% 
of Bears partial repairs are negatively identified. This is pretty good. \todo{YL: I don't know how to phrase this better than
"This is pretty good"... Also the numbers are hand-wavy because we still need to decide on weightedness of data analysis}

The experiment result shows that higher granularity levels are better at identifying partial repairs positively, 
but they also increase the chance of identifying partial repairs negatively.
This tradeoff can behave differently on different datasets. For example,
Assertion Level Granularity performed much better than Method Level Granularity
in Defects4j (up to 10\% more positively identified partial repairs, with less than
1\% more negatively identified), but this is not the case in Bears (less than 2\%
more positively identified partial repairs, with 3-8\% more negatively identified).
Thus, it is recommended that researchers should 
carefully balance the tradeoffs in granularity level in test case based validation
in APR research.


\subsection{Limitations}

This experiment breaks up full repairs into chunks and treats each chunk as a 
single edit action. However, in reality most APR techniques mutate code at 
a more granular level.
Also, this experiment only checks whether fitness functions 
can identify partial repairs, and not concerned with how to come up with these 
partial repairs (depending on specifics of the APR 
techniques, some may not be in the search space).

Regardless of its limitations, this experiment is provides valid and valuable 
information to tackling the challenge of automatically repairing bugs that 
requires multiple edit actions to fully repair 
and provides insight in future APR research.




\section{Related Work}

\cite{zou2019empirical}: Empirical study on fault localization, SBFL is the most effective standalone 
technique, except for faults with crashes, in which stack trace analysis is the most effective. They 
also combined different fault localization techniques by training a machine learning model, 
specifically using a learning to rank model. Their learning to rank model outperformed all the 
individual fault localization techniques. In their analysis, they considered a multi-edit bug to be 
localized if any faulty line is localized.

\cite{pearson2017evaluating}: Empirical study of multiple SBFL and mutation based faultlocalization 
techniques on Defects4J. Since many of these techniques were evaluated with artificial faults, the 
authors look at whether their effectiveness on artificial faults translates to effectiveness on real 
faults. This is not the case. They also show that SBFL outperforms mutation based fault localization.

Qi et al.~\cite{patch-correctness} evaluated the patches generated 
by three G\&V repair tools~\cite{genprog, ae, rsrepair} and presented 
Kali, a G\&V tool that exclusively deletes functionality. They found the 
vast majority of generated patches to be incorrect and equivalent to 
a single functionality deletion. Moreover, they found that Kali, whose 
smaller search space consists entirely of functionality-removing 
operations, generates at least as many correct patches as the 
other three tools. Later work found patch incorrectness to be 
also problematic in Defects4J~\cite{d4j-eval} and in semantics-based 
repair techniques~\cite{Le2018}.

% I know I'm referring to the work by the authors' names,
% but I can't find a good way to refactor their names out without writing awkwardly.
% I would need to refactor all of the usages of "they" to remove their names.
Long and Rinard~\cite{long-search-spaces} studied the prevalence of 
correct and incorrect plausible patches in the search spaces of SPR~\cite{spr} 
and Prophet~\cite{prophet}. They found incorrect plausible patches to outnumber 
correct patches by orders of magnitude. When they increased the search space 
by adding additional mutation operations, they found an increased number of 
correct patches, but APR tool performance might actually degrade due to a 
simultaneous increase in incorrect plausible patches and the combinatorial 
growth of the search space.

Zhong and Su~\cite{zhong2015} did an empirical study on real bug fixes. 
They studied the difficulty of fault localization, the complexity of fixing bugs, 
necessary mutation operators, importance of API knowledge, the types of buggy files, 
and addition/deletion of files in bug fixing on over 9000 real-world bugs collected via BUGSTAT, 
and identifies key insights on fault localization, faulty code fix, search space and non-source bugs. 
Both our paper and Zhong and Su's paper aims to provide useful guidance and insights for 
improving state-of-the-art APR techniques through empirical studies of bugs and bug fixes. 
In contrast, our study focuses on one specific category of bugs: 
source file bugs that requires multiple edit actions to successfully repair, 
drawing insights on their behaviors in fault localization, fitness evaluations and dependency.

Wang et al.~\cite{wang2018} did an empirical study of multi-entity changes in real bug fixes 
(where each entity is a class, method or field). Their research questions mostly focused on 
how often and why do real-world bug fixes have multi-entity changes, the relationship 
between co-changed entities, and the recurring patterns of those multi-entity changes. 
Through analyzing 2854 real-world bugs from four projects, they found that 66\%-76\% 
multi-entity fixes are closely related to each other via syntactic dependencies, 
and they identified three major recurring patterns that connects co-changed entities. 
They suggested a potential way to close the gap between APR fixes and real fixes by 
enhancing APR to incorporate multi-entity changes. In contrast, our study on bugs that
requires multiple edits to fix, where the edits may be in the same entity. We define atomic 
changes (single edit) differently, and we studied interactions between edits 
(i.e. the lines that changed in the bug fix) instead of entire entities.

Previous efforts in G\&V program repair to derive more search-guiding information 
during candidate patch evaluation 
include using program invariants~\cite{better-fitness, dinglyu}, 
intermediate program values~\cite{source-code-checkpoint}, 
and online mutation-based fault localization~\cite{mut-analysis}.
Some approaches require additional input, such as suspicious variables~\cite{source-code-checkpoint} 
or known patches for the bug under repair~\cite{better-fitness}, 
while others exhibit limited performance improvements~\cite{dinglyu, mut-analysis}.
\todo{Describe how our work can spark future work in better evaluating candidate patches.}

Schulte et al.~\cite{schulte} did an empirical study on software mutation robustness 
(i.e. how often do code mutations remain neutral in test results). 
They found that in a large collection of off-the-shelf softwares the mutation robustness is about 37\%, 
and discussed potential application of mutation robustness to proactive bug repair. 
In contrast, our study focuses on automatically fixing current bugs (i.e. bugs that fail an existing unit test), 
and we do not restrict our repair actions to neutral variants of the program.

\section{Conclusions}
\label{sec:conclusions}

\todo{Write me, plz, summarize takeaways and why we care.  We have established a
  roadmap of research directions for multi-edit repair.}

In conclusion, \todo{Fill sec 3 conclusion}

\todo{Fill sec 4 conclusion}

\todo{Fill sec 5.1 conclusion}.

We found that chunk level code clones are prevalent in human patches. Over 30\%
of multi-chunk bugs have some kind of code clones. Moreover, we found that multi-chunk bugs with Disjoint
coverage result is much more likely to have code clones.

We found that test case based validation methods cannot capture all effects of human patches,
as between a third and a half of multi-chunk bugs do not require all chunks in human patch to
past all tests.

We found that test case based validation methods can be reasonably good at identifying partial
repairs at higher granularity levels such as method level or assertion level. Moreover, 
we observed that the number of both positively and negatively identified partial repairs
increase as we increase the granularity, so we advice a careful balance of the tradeoffs of
granularity levels in future search based APR development.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
