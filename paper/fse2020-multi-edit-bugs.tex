\documentclass[sigconf, timestamp-false, anonymous=true]{acmart}

\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{fancybox}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}

\acmConference[ESEC/FSE '20]{ESEC/FSE '20: ACM Joint European Software Engineering Conference and Symposium 
on the Foundations of Software Engineering}{November 8-13, 2020}{Sacramento, CA, USA}
\acmYear{2020}

\newcommand\todo[1]{\textcolor{red}{#1}}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{A Study of Multi-Edit Bug Patches}

%add authors & shortauthors if we are lucky :)

\begin{abstract}
  \todo{note that this is probably full of falsehoods, I'm just thinking by
    typing.} Automatic program repair is a promising approach for reducing the
    cost of quality assurance practices and faulty software. To date, most
    techniques proposed for test-driven automatic repair have succeeded
    primarily on bugs that benefit from short, single-edit patches. Techniques
    that succeed on multi-edit bugs often do so by patching them in an
    alternative, single-edit way, or by targeting particular multi-edit bug
    patterns. Empirical studies of real-world similarly tend to focus on the
    patterns exhibited by single-edit bugs, and have not examined repairability
    of multi-edit bugs in detail. We present a comprehensive empirical analysis
    of multi-edit bugs in open source Java programs, focusing on static and
    dynamic properties that define the repair search space for a given bug (and
    thus, in turn, the challenges that apply to automatically addressing them).
    This analysis focuses on the key challenges of the dynamic program repair
    problem: the \emph{mutations and fix code} used to repair multi-edit bugs;
    the \emph{fault locations} and their relationships; and the \emph{objective
      function}, and in particular how and to what degree test cases can be used
    (or not) to identify partial repairs. We identify key takeaways and
    challenges, with implications for future work in expressive, multi-chunk bug
    repair.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011102</concept_id>
<concept_desc>Software and its engineering~Software defect analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011784</concept_id>
<concept_desc>Software and its engineering~Search-based software engineering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software defect analysis}
\ccsdesc[500]{Software and its engineering~Search-based software engineering}

\keywords{software bugs, program repair}

\maketitle


\newcommand{\rqorinsight}[1]{
  \setlength{\fboxsep}{0.8em}
  \vspace{0.5em}
  \begin{center}
  \Ovalbox{\begin{minipage}{0.9\linewidth}
    \textbf{Research Question:} #1
    \end{minipage}}
  \end{center}
  \vspace{0.5em}}

\section{Introduction}

Buggy software has a significant cost in terms of both social impact~\cite{tricentis} 
and developer effort~\cite{cambridge-study}. This
motivates significant research in techniques to automatically find and fix
them~\cite{whatever}, with the ultimate goal of reducing Quality Assurance costs
while improving software quality. 

A significant class of program repair techniques in both
research~\cite{genprog,angelix,Le17, Xuan17} and practice~\cite{sapfix} use test cases to guide
patch construction, typically by following what's known as a
generate-and-validate paradigm. At a high level, these techniques use test cases
to localize a defect --- identified by at least one failing test --- to a set of
likely-suspicious program locations. Then, they use a variety of techniques to
construct (or \emph{generate}) patch candidates for the bug in question,
checking each to see (or \emph{validate}) if any of them cause the program to
pass all of the provided test cases.  
%
Patches are constructed in a variety of ways, ranging from heuristic, syntactic
program manipulation~\cite{syntax,examples}, to specially adapted program
synthesis techniques~\cite{examples}. These techniques have successfully
repaired real, meaningful defects in large, complex programs~\cite{examples}.

Practically speaking, these techniques are typically limited in the types and
variety of defects they can repair~\cite{for,example}. Some of this limitation
is by design. Some techniques specifically target certain classes of
bugs~\cite{nopol,sapfix}, and it has been argued that doing so is necessary for
repair to reach true practical applicability~\cite{maybe}. Fundamentally, the
vast majority of program repair techniques can only produce single-edit repairs,
often by design~\cite{rsrepair, ae, hdrepair}. \todo{more recent citations?}
Even those that can in principle produce multi-edit bugs typically
don't~\cite{poor,genprog}.

This leaves a large proportion of real-world bugs unrepairable by modern
research techniques in program repair.  Over half of the bugs in popular bug
benchmark Defects4J are patched by humans using multi-part
patches~\cite{d4j-dissection}. Approximately 70\% of buggy source files in a
large study of bugs in Apache projects required two or more edit
actions~\cite{zhong2015}.

A key factor or tension in how these techniques are constructed is in their
management of the trivially-infinite search space of possible patches for a
given bug, while constructing patches in which a user can have confidence of
their quality or correctness.  This space has been parameterized~\cite{refs}
along several axes: (1) the \emph{fault space}, or the potential locations that
may be changed, (2) the \emph{mutation space}, or the possible modifications
that may be tried at a location, and (3) \emph{fix code space}, or the code that may
be instantiated as part of a mutation (e.g., if the mutation seeks to
\emph{insert} code, the fix code is the code that is inserted).  Many dynamic
repair techniques can be compared or described in terms of their choices along
each of these axes; the ways each technique constructs and then traverses the
space has first-order implications for scalability, and for the type and quality
of patches produced.  This has been studied many ways~\cite{examples}.

Multi-edit repair poses distinct challenges for each of these axes for repair.  
Spectrum-based fault localization, the most
prevalent class of fault localization techniques used in program repair, does
not specifically identify sets of locations that might be related or repaired
together; indeed, the evaluation of most fault localization
techniques typicaly assumes that a bug is localized if any one buggy line is
identified~\cite{fl-survey-wong}.  Researchers have observed that some bugs are
repaired in multiple locations using very similar
code~\cite{saha2019harnessing,jiang2019cmsuggester}, informing novel techniques
that constrain the \emph{fix space} of possible multi-edit repairs accordingly.
One key question for applicability of these types of techniques is how prevalent
such bugs are, and how often multi-edit bugs instead require multiple
coordinating (but ultimately different) edits or pieces of fix code.  Over 50\% of the fixes in four 
Apache projects involve two or more entities -- i.e., a Java class, method, or field -- and 66\%-76\% of 
those multi-entity fixes involved syntactic dependencies.~\cite{wang2018}. 
Test cases are used to evaluate candidate repairs in virtually all dynamic
generate-and-validate repair techniques, but, anecdotally, may not be effective
at identifying partial repairs in a multi-edit
context~\cite{fitness10,maybeeric}.  
Although multi-edit repair has been discussed in the context of other analyses
that study bug fix characteristics in general~\cite{examples} as well as for
repair applicability specifically~\cite{moar,examples}, to the best of our
knowledge there has been no significant previous study of the characteristics of
multi-edit repairs in terms of their implications for repairability or, more
broadly, program repair.  

In this paper, we present a systematic study of real-world multi-edit bugs,
looking specifically at their characteristics with respect to the problem of the
automatic repair search space.  
We study the bugs curated in two
real-world datasets that support program repair research: Defects4J~\cite{defects4j}
and BEARS~\cite{bears}, in total, 646 bugs in 78 projects \todo{do we include the multi-module bugs 
in bears?} and
\todo{NUMLOC} total lines of code.  Of these, \todo{NUMULTI} were repaired by a
human developer using a multi-edit or -chunk (as we define in
Section~\ref{somewhere}) repair.  We look at characteristics along each of the
relevant axes of the program repair search problem: fault locations, mutation
operators, fix code, and evaluation (or fitness or objective) using test cases.  
\todo{we also examine symptoms, need to fit that one in.}  Our contributions are
as follows:

\begin{itemize}
\item \todo{fixme} A study on X and Y.
\item \todo{fixme} characterizing of fault localization.  We find that FOO.
\item \todo{fixme} characterizing fix types and code.  We find that BAR.
\item \todo{fixme} study of test cases as a means to evaluation partial and full
  multi-site repairs.  We find that BAZ.
\item \todo{fixme} whatever else.
\item \todo{A set of recommendations?}
\end{itemize}

\todo{This paragraph is a lie and some people hate them so maybe we'll cut it,
  it just feels comforting to me to write them as I'm working on a paper draft...}
The rest of this paper is organized as follows.  We provide background on
generate-and-validate program repair, with a focus on the key parameters of the
program repair search problem, in Section~\ref{sec:background}.  We describe the results of our analysis in
Section~\ref{sec:results}, with discussion of implications as well as
limitations in Section~\ref{sec:discussion}.  Related work is discussed in
Section~\ref{sec:related}; Section~\ref{conclusions} concludes.

\section{Key Concepts and Research Questions}
\label{sec:background}

\paragraph{Search spaces in automatic program repair.} Automatic program repair (APR) techniques aim to find patches for bugs in
programs.  We restrict attention to dynamic or test-case guided program repair,
which describes the majority of research advancements over the past ten years.
Such techniques take as input a program and a set of test cases that serve as
the oracle for program correctness.  At least one of those tests should be
failing, corresponding to the bug to be repaired.  

At a high level, any APR technique aims to solve a search or optimization
problem, seeking a set of edits (or patch) to the program that will lead it to
pass all of the provided tests.  Although The repair search is classically defined along
the following axes:
\begin{itemize}
\item \emph{Fault space.} The first problem in fixing a bug concerns
  \emph{where} in the code modifications should be applied. All dynamic repair
  techniques begin by using the provided test cases as input to a fault
  localization technique. Such techniques identify (and typically score)
  suspicious code based on which test cases execute which pieces of code.
  Although the particulars of the fault localization employed can vary, most APR
  use some variant of spectrum-based fault localization (SBFL)~\cite{refs} in
  this first step. The resulting computation defines the fault space, or the
  candidate locations or code entities considered for repair. This first step is
  key to reducing the search space.

\item \emph{Mutation space.} The mutation space is the set of applicable 
modifications at a program location. Examples include GenProg's append, 
replace, and delete operators over statements~\cite{genprog-operators} and 
Nopol's condition replacement over if-statements and missing precondition 
addition over non-branch and non-loop statements~\cite{Xuan17}.
A larger mutation space generates 
a richer search space with potentially more repairs, but
the resulting combinatorial growth of the search space can also hinder 
the search process~\cite{long-search-spaces}.

\item \emph{Fix code.}

\item \emph{Objective.} 

\end{itemize}



\paragraph{Edit granularity in bug-fixing patches.}  Developer patches for
program bugs naturally vary in length and complexity.  Estimates range from
\todo{FIXME} to \todo{FIXME}\% of bugs are repaired using fewer than
\todo{FIXME} lines of code.  \todo{rephrpase/repeat stuff in intro about
  studies/prevalence of multi-edit bugs, but expand the rephrase of the results
  in that prior work to contrast with incidence of single-edit bugs.}  

There are indeed several plausible definitions of multi- vs. single-edit bugs,
with implications for how they are studied.  We consider edits at the granularity 
of lines and of chunks. We define a line edit as an added, removed, or modified 
line. We follow~\cite{d4j-dissection} in our definition of a chunk as a continuous 
sequence of line edits. We solely consider edits to source code; thus, we ignore 
edits to whitespace or comments.

Alternative definitions may have bearing on our results.  We argue that this
definition is natural for the purposes of our study, which focuses in particular
on understanding larger bugs---especially multi-chunk bugs---for the purposes of 
adapting, extending, or applying new program repair techniques to address them. 
\todo{A couple more sentences re: why this is reasonable. Might be solved by 
pointing to low rates of repair for multi-chunk bugs.}

\paragraph{Research questions.}  The parameterization of the program repair
search problems into multiple subspaces informs our empirical study of those
spaces for the purposes of understanding multi-edit repair.  We address the
following research questions:

\begin{enumerate}
\item \emph{RQ1: What is the incidence of multi-edit repairs in real-world bug data?}
\item RQ2: ...
\item \todo{WHAT ARE OUR RESEARCH QUESTIONS?}
\item RQ7: How well can test case based validation identify partial repairs at each granularity level?
\end{enumerate}

\paragraph{Datasets.}  

\section{RQ1: Multi-Edit Prevalence}
\label{sec:data-rq1}

\begin{table}
\begin{tabular}{l | lll}
\toprule
\multicolumn{3}{c}{\textbf{Defects4j}} \\
\midrule
project & bugs & src(kloc) & test(kloc) \\
\midrule
JFreeChart  & 26 & 193.3 & 74.6 \\
Closure compiler & 133 & 150.6 & 112.6 \\
Apache commons-lang & 65 & 57.8 & 47.4 \\
Apache commons-math & 106 & 45.0 & 41.5 \\
Mockito & 38 & 23.0 & 28.5 \\
Joda-Time & 27 & 82.9 & 70.4 \\
\midrule
\multicolumn{3}{c}{\textbf{Bears (single-module)}} \\
\midrule
FasterXML-jackson-databind & 26 & 95.7 & 53.6 \\
INRIA-Spoon & 62 & 66.2 & 30.8 \\
spring-data-commons & 15 & 45.8 & 28.8 \\
traccar-traccar & 42 & 47.9 & 8.6 \\
30 other projects & 62 & - & -\\
\bottomrule
\end{tabular}
\caption{\label{tab:data} Characteristics of the Defects4J (top) and Bears (bottom) datasets.}
\end{table}


\rqorinsight{RQ1: How many multi-edit patches are in Defects4J and Bears?}
We start our analysis of multi-edit patches by asking how many such patches 
exist in our benchmarks. We analyze edits at line and chunk granularities,
which we defined in Section~\ref{sec:background}.

\paragraph{Datasets}  Our study requires a dataset of indicative, real-world,
multi-edit defects.  We study both the defects in Defects4J
Defects4J~\cite{defects4j} and Bears~\cite{bears}.  Table~\ref{tab:data}
summarizes these datasets, both of which
consist of historical
bugs found in real world software projects. Defects4J contains 395 bugs from 
six Java software projects, and is currently very popular dataset for evaluating 
program repair tools that target Java~\cite{durieux-repair-them-all}.
However, there is a risk associated with any such dataset that tools may overfit
to the defects in question, and there is evidence that this situation applies to
program repair and Defects4j~\cite{durieux-repair-them-all}. 
We thus also study bugs from Bears~\cite{bears}, 
a set of Java bugs derived from failed Travis-CI builds of GitHub
projects. 
Bears offers 251 bugs from 72 software projects, providing a greater diversity of 
projects compared to Defects4J. Several projects, however, are structured as 
multiple modules, which are not compatible with our current automation tools.
We thus limit our analysis of Bears to 181 bugs from 30 single-module projects.
We intend to address the remaining multi-module software projects in future work.

\todo{Questions: what does ``tool compatability reasons'' mean?  Why doesn't it
  apply to D4J?}

\todo{Consider adding a column to the dataset table above with ``subset
  considered'' for the defects; is this worth doing?  How many bugs do we
  exclude? Do we include Closure?}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rrrr | r}
		\toprule
		Dataset & 1-line & Multi-line & 1-chunk & Multi-chunk & Total \\
		\midrule
		\multirow{ 2}{*}{Defects4J} & 92 & 303 & 298 & 97 & 395 \\
		& 23\% & 77\% & 75\% & 25\% & 100\% \\
		\multirow{ 2}{*}{Bears} & 31 & 150 & 117 & 64 & 181 \\
		& 17\% & 83\% & 65\% & 35\% & 100\% \\
		\midrule
		\multirow{ 2}{*}{Combined} & 123 & 453 & 415 & 161 & 576 \\
		& 21\% & 79\% & 72\% & 28\% & 100\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequencies and percentages of patches with multi-line and
          multi-chunk edits. \todo{What is the difference? What's the takeaway?
            Also please reorganize to have everything in a column have the same
            type -- either percentage or count. It's fine to break up things by
            count or percentage but have them next to each other, not on top of
            one another.}}
	\label{tab:multiedit-frequencies}
\end{table}

\paragraph{RQ1: Results.} 
Table~\ref{tab:multiedit-frequencies} lists the number and percentages of
multi-line and multi-chunk patches. For both datasets, over 75\% of patches are multi-line, 
and $\geq$25\% are multi-chunk. At both edit granularities, multi-edit bugs 
comprise a majority or a substantial minority of both datasets.

\todo{Maybe add some data to show repair rates of multi-line \& multi-chunk bugs}

\todo{CLG says: ...I honestly don't understand the ``frequencies'' table or the
  2--3 sentences above. What are we counting for ``lines''? What is the
  difference between ``lines'' and ``chunks''? And: What is the interpretation?
  Why do I care?}


\input{coverage.tex}



\section{Dependency Analysis}

\rqorinsight{
	RQ?: How many multi-line patches contain control or data dependent edits, 
	and are bugs with dependent edits more difficult for APR tools to repair?
}

The presence of control or data dependencies in a patch's edits 
indicates coupling between the dependent edits.
Such coupled edits might need to be repaired simultaneously.
\todo{Some motivating examples might be nice.}
We analyze control and data dependencies in bug patches along 
with their relationship to APR tool success.

\subsection{Methodology}

A patch contains dependent edits if there exists control or data dependencies 
between added, removed, or changed lines in the pre- or post-patch
source code. For practical reasons, we perform intraprocedural analysis, 
although we heuristically consider function arguments as reads 
and invocations of getter and setter methods as reads and writes.

\subsection{Results}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rrrr | r}
		\toprule
		Dataset & Control & Data & Either & Neither & Total \\
		\midrule
		Defects4J & 123 & 78 & 132 & 171 & 303 \\
		& 40\% & 26\% & 44\% & 56\% & 100\% \\
		Bears & 83 & 63 & 95 & 55 & 150 \\
		& 53\% & 42\% & 63\% & 37\% & 100\% \\
		\midrule
		Combined & 206 & 141 & 227 & 226 & 453 \\
		& 45\% & 31\% & 50\% & 50\% & 100\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Frequencies of multi-line patches with control and data dependent line edits.}
	\label{tab:dependency}
\end{table}

Table~\ref{tab:dependency} shows the 
frequencies and percentages of multi-line patches with control or data dependent 
line edits. We find a higher proportion of dependent patches in Bears compared to 
Defects4J, indicating higher patch complexity among multi-line patches in Bears.


\begin{table}
{\begin{center}
	\begin{tabular}{l | l | r r r r | r}
		\toprule
		Dataset & APR & Control & Data & Either & Neither & Total \\
		\midrule
		\multirow{2}{*}{Defects4J} & Success & 34 & 21 & 39 & 94 & 133 \\
		                                          & Failure   &  89 & 57 & 93 & 77 & 170 \\
		\multirow{2}{*}{Bears}       & Success &    7 &   6 &   9 &   7 &   16 \\
		                                          & Failure   &  76 & 57 & 86 & 48 & 134 \\
		\midrule
		\multirow{2}{*}{Combined}& Success &  41 & 27 & 48 &101& 149 \\
		                                          & Failure   &165 &114&179&125& 304 \\
	\end{tabular}
 \end{center}
}
	\caption{Frequency of multi-line patches with respect to the presence of 
	control/data dependent line edits and whether an APR tool successfully 
	repaired the bug in~\cite{durieux-repair-them-all}.}
	\label{tab:dependency-repair-contingency-table}
\end{table}

\begin{table}
{\begin{center}
	\begin{tabular}{l | rrr}
            	\toprule
		& Defects4J & Bears & Combined \\
		\midrule
		$P(\mbox{APR Success } | \mbox{ Control})$ & 28\% &  8\% & 20\% \\
		$P(\mbox{APR Success } | \neg \mbox{ Control})$ & 55\% & 13\% & 44\% \\
		$P(\mbox{APR Success } | \mbox{ Data})$ & 27\% & 10\% & 19\%\\
		$P(\mbox{APR Success } | \neg \mbox{ Data})$ & 50\% & 11\% & 39\% \\
		$P(\mbox{APR Success } | \mbox{ Either})$ & 30\% & 9\% & 21\% \\
		$P(\mbox{APR Success } | \mbox{ Neither})$ & 45\% & 13\% & 45\% \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{Percentages of auto-repaired bugs in~\cite{durieux-repair-them-all} 
	that (do not) contain control/data dependent line edits.}
	\label{tab:dependency-repair-percents}
\end{table}

Table~\ref{tab:dependency-repair-contingency-table} shows
the frequencies of multi-line patches with respect to the presence of 
dependent edits and whether an APR tool successfully auto-repaired the bug in~\cite{durieux-repair-them-all}.
Using $\chi^2$ tests, we find statistically significant relationships between APR  
success and control, data, and the disjunction of either dependencies 
($p < 0.001$ for all).
We find that bugs whose human patches are free of dependencies are
more likely to be successfully fixed by an APR tool.

Tables~\ref{tab:dependency-repair-contingency-table} and~\ref{tab:dependency-repair-percents}
show the frequencies and percentages of multi-line patches with respect to edit dependency 
and whether an APR tool successfully repaired bug in~\cite{durieux-repair-them-all}.
We find that edit dependency generally reduces the likelihood of APR tool success.
Using $\chi^2$ tests, we find statistically significant relationships ($p < 0.001$ for all)
between APR success and control, data, and the disjunction of either dependencies 
for Defects4J and Defects4J $\cup$ Bears patches. We fail to find statistically 
significant relationships over only Bears patches, possibly due to the small number (16) of 
successfully auto-repaired Bears bugs with multi-line human patches.


\section{Test case-based validation}

In search based program repair, one key area of research is to find ways to 
measure how "close" a candidate patch is to a full repair (a fitness function),
and this is usually done via measuring unit test performance of the candidate patch. 
Given a buggy program and a valid repair, if we apply a part of the valid repair to 
the buggy program (a partial repair), then semanticly the partial repair is closer
 to the full repair compared to the original. 
Ideally, we would want the fitness function to guide the search towards a full 
repair by identifying partial repairs as "closer" to full repair than original.
However, sometimes partial repairs will perform no different in unit tests compared 
to the original program, and other times they could perform worse. We want to find 
out how often each of these situations happen.

Moreover, unit test performance can be measured in different granularity levels. 
The most common ones used in existing APR tools are class level and method level. 
It is commonly believed that the more granular a fitness functions is, the better 
it is because it collects more information and is less likely to plateau. Thus, 
we introduce a third granularity level: assertion level, which is more granular 
than method level granularity.
We would like to compare the performance of the fitness functions at identifying 
partial repairs at different granularity levels.

The results may provide valuable information to future search-based program 
repair tool designs.

\subsection{Partial Repairs}

For each bug in Defects4j (excluding Clojure bugs) and Bears (single-module only), 
we look at the provided correct patch and divide it 
into edit chunks using the following steps:

1. Each consecutive block of edits (including both insertion and deletion) 
is labeled as one chunk.

2. Discard all edits that does not actually affect the program 
(such as comment change, fixing spaces, etc)

3. If a set of matched brackets that are both inserted or deleted in two different 
chunk, merge those two chunks into one; if a bracket is deleted in one chunk but 
inserted back in another chunk (matched to the same opposite bracket), merge these 
two chunks into one

4. We introduce a new set of edits, Base, initially empty. If a variable declaration 
is deleted, discard the deletion edit; if a variable declaration is inserted, move this
 edit to Base; if a variable declaration is moved upwards in code, move the variable 
moving edits to Base; if a variable declaration is moved downwards in code, 
discard the moving edits. Declaration modifications (i.e. changing parts of the 
variable declaration, but not the variable name or the location of the declaration), 
however, does not belong to any of the cases above.

5. Any insertion of imports or additional helper methods are moved to Base; 
any deletion of imports or helper methods is discarded

After the steps above, we count the number of remaining chunks (not including Base) 
and call it the chunk number of the bug, label the chunks with positive numbers, and 
make a power set of the chunks excluding the empty set and the complete set. 
Then, for each subset of the chunks, we apply all edits in chunks in this subset 
and Base to the original buggy code, and we define this as a partial repair. 
For a bug with chunk number n, it should have $2^n-2$ partial repairs. 

In this experiment, each edit chunk will be viewed as a single edit. Thus, only bugs 
with chunk number between 2 and 6 (inclusive) is selected, as bugs with chunk number 
more than 6 are rare but has too many partial repairs to evaluate.

Note that the reason of steps 4 and 5 is to make sure that all declarations are 
present in all partial repairs such that the partial repairs compile (thus can be 
tested). Since there are almost no cases where a declaration in patch have side 
effects, it is okay to include redundant variable/method declarations in partial 
repairs, as they will not affect the program's performance in unit tests. 

\subsection{Unit Testing Granularity}

There are different ways to compare unit test results, and the most common ways 
are class-level granularity and method-level granularity. At Class-level granularity, 
we only look at which test classes passed and which failed; at method-level 
granularity, we look at which test methods passed and which failed.

Here we introduce a third level of granularity: assertion-level granularity. 
For each test method $M$, let $A(M)$ be the set of all assert statements in $M$. 
When $M$ is run, if an assertion failed, the failure is recorded and the method 
is allowed to continue to run (as opposed to normally the test method throws an 
error and terminates). After running the method, for each assert statement 
$a\in A(M)$, let $b(a)$ be 1 if $a$ never failed once during the running of $M$, 
and 0 otherwise. We define the assertion score of $M$ to be 
$AS(M)=\frac{\Sigma_{a\in A(M)}b(a)}{|A(M)|}$. If $M$ failed to run to completion 
due to timeouts or exceptions that are not related to assertions, then we define 
$AS(M)=0$. Thus by definition, $AS(M)=1$ if $M$ passes. If a program passed more 
assertions in $M$, there should be an increase in $AS(M)$.


\subsection{Unminimized Results}

After doing the steps in section 2.1, there are 97 bugs in Defects4j and 64 bugs 
in Bears that has chunk number between 2 and 6. Since each bug may have different 
number of partial repairs, we analyzed the results in two different ways: 
Unweighted (where each partial repair is weighted equally), and weighted (where each bug 
has equal weight, so if a bug has n partial repairs each of them has weight 
$\frac{1}{n}$). We record the percentage of partial repairs that performed better, same
or worse compared to original code in each assertion level.

    

\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted &Unweighted& Weighted \\ \hline
    Class & 11.36\% & 20.46\% & 29.05\% & 27.60\%\\
    Method & 35.41\% & 41.25\% & 37.61\% & 31.29\% \\
    Assertion & 40.42\% & 48.25\% & 38.96 \% & 31.60\% \\
    \hline
    
    \end{tabular}
\end{center}}
\caption{percentage of partial repairs that are identified positively at each granularity level, in unminimized experiment result}
\end{table}

    
\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted & Unweighted & Weighted \\ \hline
    Class & 68.04\% & 65.53\% & 65.09\% & 56.84\%\\
    Method & 36.64 \% & 39.33\% & 55.63\% & 51.60\% \\
    Assertion & 27.62\% & 31.73\% & 49.32 \% & 47.48\% \\
    \hline
    
    \end{tabular}
\end{center} }
\caption{percentage of partial repairs that performs no different at each granularity level, in unminimized experiment result}
\end{table}   
    
\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted & Unweighted & Weighted \\ \hline
    Class & 12.03\% & 7.18\% & 4.05\% & 6.70\%\\
    Method & 19.27 \% & 12.52\% & 4.95\% & 8.26\% \\
    Assertion & 21.83\% & 12.90\% & 9.68 \% & 11.29\% \\
    \hline
    
    \end{tabular}
\end{center}}
\caption{percentage of partial repairs that are identified negatively at each granularity level, in unminimized experiment result}
\end{table}
    
    

\subsection{Minimized Results}

During the experiment, we found that sometimes not all edit 
chunks of a bug is necessary to pass all tests, 
because one or more of its partial repairs passed all tests. This means that 
some chunks in the provided correct patch is redundant. 

Since we're only interested in edits that are necessary for the repair, 
we processed the data to ignore redundant chunks and all partial repairs 
that included them. 32 out of 97 defects4j bugs and 34 out of 64 Bears 
bugs are affected. Some bugs ends up with chunk number 1 after minimization, 
so they are not included in the Minimized results. The minimized results 
include 75 remaining bugs in defects4j and 38 remaining bugs of Bears. 
The full result is presented in tables.

    
\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted & Unweighted & Weighted \\ \hline
    Class & 5.30\% & 9.78\% & 12.18\% & 10.59\%\\
    Method & 34.81\% & 36.80\% & 23.72\% & 19.71\% \\
    Assertion & 40.28\% & 45.91\% & 27.56 \% & 20.24\% \\
    \hline
    
    \end{tabular}
\end{center}}
\caption{percentage of partial repairs that are identified positively at each granularity level, in minimized experiment result}
\end{table}
    
\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted & Unweighted & Weighted \\ \hline
    Class & 77.56\% & 73.56\% & 73.72\% & 73.75\%\\
    Method & 40.11 \% & 39.95\% & 62.18\% & 64.62\% \\
    Assertion & 31.98\% & 30.30\% & 49.36 \% & 59.19\% \\
    \hline
    
    \end{tabular}
    
\end{center}}
\caption{percentage of partial repairs that performs no different at each granularity level, in minimized experiment result}
\end{table}

    
\begin{table}
{\begin{center}
    \begin{tabular}{| l | l | l | l | l |} \hline
     & Defects4j & Defects4j & Bears & Bears  \\ \hline
     & Unweighted & Weighted & Unweighted & Weighted \\ \hline
    Class & 9.89\% & 7.64\% & 10.90\% & 10.84\%\\
    Method & 17.67 \% & 14.13\% & 10.90 \% & 10.84\% \\
    Assertion & 18.55 \% & 14.39 \% & 19.23 \% & 14.44\% \\
    \hline
    
    \end{tabular}
    
\end{center}}
\caption{percentage of partial repairs that are identified negatively at each granularity level, in minimized experiment result}
\end{table}
\subsection{Interpretations}

1. As expected, the more granular the fitness function gets, the more 
likely a partial repair gets identified positively or negatively.

2. In both datasets, method level granularity performed a lot better 
than class level granularity. Assertion level granularity performed 
better than method level granularity in Defects4j (significant increase 
in positively identifying partial repairs and minimal increase in negatively 
identifying partial repairs) and not so well in Bears (almost no increase in 
positively identifying partial repairs while significantly 
increasing negatively identifying partial repairs). 

3. Note that the percentage of
partial repairs that are identified positively in class level granularity is a lot
less in minimized results compared to unminimized results, that is because 
a lot of partial repairs in unminimized results that passed more test classes than
original buggy code actually passed all tests, thus is no longer considered 
a partial repair in minimized results.

4. In general, it seems that unit test results are less sensitive to granularity 
level in Bears compared to Defects4j. Fitness function with class level 
granularity performs approximately the same on both datasets in minimized results 
(and in unminimized results it actually did better in Bears), but as we get more 
granular, the fitness functions performs much better with Defects4j bugs compared 
to Bears bugs in both minimized and unminimized results.


\subsection{Limitations}

This experiment breaks up full repairs into chunks and treats each chunk as a 
single edit action. However, in reality most APR techniques edits a line or part 
of a line at a time. Also, this experiment only checks whether fitness functions 
can identify partial repairs, and not concerned with how to come up with these 
partial repairs (depending on specifics of the APR 
techniques, some may not be in the search space).

Regardless of its limitations, this experiment is provides valid and valuable 
information to tackling the challenge of automatically repairing bugs that 
requires multiple edit actions to fully repair 
and provides insight in future APR research.


\section{Related Work}

Qi et al.~\cite{patch-correctness} evaluated the patches generated 
by three G\&V repair tools~\cite{genprog, ae, rsrepair} and presented 
Kali, a G\&V tool that exclusively deletes functionality. They found the 
vast majority of generated patches to be incorrect and equivalent to 
a single functionality deletion. Moreover, they found that Kali, whose 
smaller search space consists entirely of functionality-removing 
operations, generates at least as many correct patches as the 
other three tools. Later work found patch incorrectness to be 
also problematic in Defects4J~\cite{d4j-eval} and in semantics-based 
repair techniques~\cite{Le2018}.

% I know I'm referring to the work by the authors' names,
% but I can't find a good way to refactor their names out without writing awkwardly.
% I would need to refactor all of the usages of "they" to remove their names.
Long and Rinard~\cite{long-search-spaces} studied the prevalence of 
correct and incorrect plausible patches in the search spaces of SPR~\cite{spr} 
and Prophet~\cite{prophet}. They found incorrect plausible patches to outnumber 
correct patches by orders of magnitude. When they increased the search space 
by adding additional mutation operations, they found an increased number of 
correct patches, but APR tool performance might actually degrade due to a 
simultaneous increase in incorrect plausible patches and the combinatorial 
growth of the search space.

Zhong and Su~\cite{zhong2015} did an empirical study on real bug fixes. 
They studied the difficulty of fault localization, the complexity of fixing bugs, 
necessary mutation operators, importance of API knowledge, the types of buggy files, 
and addition/deletion of files in bug fixing on over 9000 real-world bugs collected via BUGSTAT, 
and identifies key insights on fault localization, faulty code fix, search space and non-source bugs. 
Both our paper and Zhong and Su's paper aims to provide useful guidance and insights for 
improving state-of-the-art APR techniques through empirical studies of bugs and bug fixes. 
In contrast, our study focuses on one specific category of bugs: 
source file bugs that requires multiple edit actions to successfully repair, 
drawing insights on their behaviors in fault localization, fitness evaluations and dependency.

Wang et al.~\cite{wang2018} did an empirical study of multi-entity changes in real bug fixes 
(where each entity is a class, method or field). Their research questions mostly focused on 
how often and why do real-world bug fixes have multi-entity changes, the relationship 
between co-changed entities, and the recurring patterns of those multi-entity changes. 
Through analyzing 2854 real-world bugs from four projects, they found that 66\%-76\% 
multi-entity fixes are closely related to each other via syntactic dependencies, 
and they identified three major recurring patterns that connects co-changed entities. 
They suggested a potential way to close the gap between APR fixes and real fixes by 
enhancing APR to incorporate multi-entity changes. In contrast, our study on bugs that
requires multiple edits to fix, where the edits may be in the same entity. We define atomic 
changes (single edit) differently, and we studied interactions between edits 
(i.e. the lines that changed in the bug fix) instead of entire entities.

Previous efforts in G\&V program repair to derive more search-guiding information 
during candidate patch evaluation 
include using program invariants~\cite{better-fitness, dinglyu}, 
intermediate program values~\cite{source-code-checkpoint}, 
and online mutation-based fault localization~\cite{mut-analysis}.
Some approaches require additional input, such as suspicious variables~\cite{source-code-checkpoint} 
or known patches for the bug under repair~\cite{better-fitness}, 
while others exhibit limited performance improvements~\cite{dinglyu, mut-analysis}.
\todo{Describe how our work can spark future work in better evaluating candidate patches.}

Schulte et al.~\cite{schulte} did an empirical study on software mutation robustness 
(i.e. how often do code mutations remain neutral in test results). 
They found that in a large collection of off-the-shelf softwares the mutation robustness is about 37\%, 
and discussed potential application of mutation robustness to proactive bug repair. 
In contrast, our study focuses on automatically fixing current bugs (i.e. bugs that fail an existing unit test), 
and we do not restrict our repair actions to neutral variants of the program.

\section{Limitations}

\subsection{Possibly Unnecessary Edits in Patches}

We found instances in our data set where not all edits are required to 
make the faulty program pass all tests. One possibility is that the extra 
edits satisfy untested specifications. Another possibility is that the edits 
do not affect program functionality (e.g.: refactoring edits). In the test case based
validation section, we addressed this issue by minimization. We assume 
that all edits are functionally relevant. We intend to study such seemingly
extraneous edits in future work.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
