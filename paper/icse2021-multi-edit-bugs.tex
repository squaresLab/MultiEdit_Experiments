\documentclass[10pt, conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{fancybox}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[export]{adjustbox}
\usepackage{xspace}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125em X}}


\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{dkred}{rgb}{0.5,0,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{cyan(process)}{rgb}{0.0, 0.72, 0.92}
\definecolor{safetyorange}{rgb}{1.0, 0.4, 0.0}
\definecolor{javagreen}{rgb}{0.25,0.5,0.35}
\definecolor{metalgrey}{rgb}{0.43, 0.5, 0.5}
 % comments
%basicstyle=\ttfamily\bfseries\footnotesize,


\lstdefinestyle{JavaStyle}{
    language=Java,      % choose the language of the code
    keywords=[2]{View, LayoutInflater, ViewGroup, Bundle, ListView, Fragment, Activity},
    keywords=[3]{onCreateView, inflate, getActivity, DLE, findViewById, setAdapter},
    keywords=[4]{@Override, listView, DirList},
    basicstyle=\ttfamily\bfseries,
    keywordstyle=\color[RGB]{69,97,189},
    keywordstyle=[2]{\color{cyan(process)}},
    keywordstyle=[3]\color{javagreen},
    keywordstyle=[4]\color{metalgrey},
    %safetyorange
    commentstyle=\itshape\color{green!60!black},
    moredelim=[l][\itshape\color{gray}]{//}, %<--- overrides line-comment style
    stringstyle=\color[RGB]{192,8,8},
    numberstyle=\itshape\color{yellow!50!black},
%   backgroundcolor=\color{lbcolor},
    tabsize=4,
%   rulecolor=,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=false,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    numbers=left,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    %autodedent,%<--- removes indentation
}
\lstdefinestyle{examplestyle}{
    language=C,      % choose the language of the code
    basicstyle=\ttfamily\footnotesize\bfseries,
    tabsize=4,
    aboveskip={.25\baselineskip},
    belowskip={.25\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=false,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=none,
    numbers=none,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    %autodedent,%<--- removes indentation
}

\lstset{style=JavaStyle}

\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand\bears{\textsc{Bears}\xspace}

\begin{document}

\title{A Study of Multi-Location Bug Patches}

\author{Anonymous Authors}

%% JL: This is the author template from the IEEE example tex file.
%%
%% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% \and
%% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% \and
%% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% \and
%% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% \and
%% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% \and
%% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%% \textit{name of organization (of Aff.)}\\
%% City, Country \\
%% email address or ORCID}
%% }

\maketitle

\begin{abstract}
    Automatic program repair is a promising approach for reducing the
    cost of quality assurance practices and faulty software. To date, most
    techniques proposed for test-driven automatic repair have succeeded
    primarily on bugs that benefit from short, single-location patches. Techniques
    that successfully generate multi-location patches often do so in an
    alternative, single-edit way, or by targeting particular multi-location bug
    patterns. Empirical studies of real-world similarly tend to focus on the
    patterns exhibited by single-location bug patches, and have not examined repairability
    of multi-location patches in detail. We present a comprehensive empirical analysis
    of multi-location patches for bugs in open source Java programs, focusing on static and
    dynamic properties that define the repair search space for a given bug.
    This analysis focuses on the key challenges of the dynamic program repair
    problem: the \emph{mutations and fix code} used to repair bugs in multiple locations;
    the \emph{fault locations} and their relationships; and the \emph{objective
      function}, and in particular how and to what degree test cases can be used
    (or not) to identify partial repairs. We identify key takeaways and
    challenges, with implications for future work in expressive, multi-location bug
    repair.
\end{abstract}

\begin{IEEEkeywords}
  software bugs, program repair
\end{IEEEkeywords}

\newcommand{\rqorinsight}[2]{
  \setlength{\fboxsep}{0.8em}
  \vspace{0.5em}
  \begin{center}
  \Ovalbox{\begin{minipage}{0.9\linewidth}
    \textbf{RQ#1:} #2
    \end{minipage}}
  \end{center}
  \vspace{0.5em}}

\section{Introduction}

%\renewcommand\todo[1]{}

\todo{TODO THROUGHOUT: virtually all of the figure/table captions are
  inadequate; they need takeaways.  Update those for which you're responsible,
  and make sure all tables/figures have a responsible person.}
Buggy software has a significant economic cost~\cite{cambridge-study}, and
software failures are estimated to have affected half of the world's
population~\cite{tricentis}.  This motivates research in
techniques to automatically find and fix bugs.  Some of these techniques have
begun to be integrated  which have become integrated into
real-world development practices~\cite{iceland,sapfix}.\footnote{https://engineering.fb.com/developer-tools/getafix-how-facebook-tools-learn-to-fix-bugs-automatically/}

A significant class of program repair techniques in both
\todo{update this citation list to include stuff published since 2017, plz}
research~\cite{genprog,angelix,Le17, Xuan17} and practice~\cite{sapfix} use test
cases to guide patch construction. At a high level, these techniques use test cases
to localize a defect --- identified by at least one failing test --- and to
validate which (if any) of the generated patch candidates lead
the program to pass all tests.
%
Patches are constructed in a variety of ways, ranging from heuristic, syntactic
\todo{as above, let's join the roaring 20s}
program manipulation~\cite{par,genprog,rsrepair,ae,prophet,hdrepair}, to specially adapted program
synthesis techniques~\cite{Konighofer11,Konighofer12,semfix,DeMarco14,angelix}. These techniques have successfully
repaired real, meaningful defects in large, complex
programs~\cite{angelix,genprog-eight-dollars,prophet,sapfix}.

Practically speaking, though, these techniques are typically limited in the types and
complexity of defects they can repair. Often this is by design: techniques may
limit the repair search space to single-location patches for
tractability~\cite{rsrepair,ae,hdrepair}, while others only target certain
classes of bugs~\cite{Xuan17,sapfix,DeMarco14,par}. However, even techniques
that can in principle generate repairs with multi-location patches typically
don't~\cite{patch-correctness}.
%
Meanwhile, many real-world bugs require multi-edit patches~\cite{d4j-dissection,zhong2015}, leaving a large
propertion of them  unrepairable by modern
research techniques in program repair.  
%Over half of the bugs in the popular bug
%benchmark Defects4J are patched by humans using multi-part
%patches~\cite{d4j-dissection}. Approximately 70\% of buggy source files in a
%large study of bugs in Apache projects required edits at two or more
%locations~\cite{zhong2015}.

This is a difficult problem.
A key tension in the design of an automated repair technique is the balance
between giving users confidence in patch correctness by maximizing its
subjective quality while managing a trivially-infinite search space. This space
is typically parameterized along several axes: (1) the \emph{fault space}:
potential program locations to be modified, (2) the \emph{mutation space}: which
modifications may be applied at a location, and (3) the \emph{fix space}: code
that may be instantiated for a specific mutation. For example, a repair
technique might identify the location of a null-pointer dereference (exploring
the fault space), decide to insert new code (mutation space), and synthesize
code to initialize the pointer (fix space). Many dynamic repair techniques can
be compared in terms of their choices along each of these axes, and traversal
strategies have first-order implications for scalability and for the type and
quality of patches produced.

Multi-location repair poses distinct challenges for each of these repair axes.
Spectrum-based fault localization~\cite{ochiai}, the most prevalent class of
fault localization used in program repair, does not specifically
identify sets of potentially-related locations; indeed,
the evaluation of most fault localization techniques typicaly assumes that a bug
is localized if any one buggy line is identified~\cite{fl-survey-wong}.
Some bugs \emph{are}
repaired in multiple locations using very similar
code~\cite{saha2019harnessing,jiang2019cmsuggester}, informing novel techniques
that constrain the \emph{fix space} of possible multi-location repairs accordingly.
But, one key question for applicability of these types of techniques is how prevalent
such bugs are, relative to bugs that require multiple
coordinating (but ultimately different) edits.
Note for example that over 50\% of the fixes in four 
Apache projects involve two or more entities -- i.e., a Java class, method, or field -- and 66\%-76\% of 
those multi-entity fixes involved syntactic dependencies~\cite{wang2018}. 
Test cases are used to evaluate candidate repairs in dynamic
repair techniques, but, anecdotally, may not be effective
at identifying partial repairs in a multi-location
context~\cite{better-fitness}.  
Although multi-location repair has been discussed in the context of other analyses
that study bug fix characteristics in general~\cite{d4j-dissection} as well as for
repair applicability specifically~\cite{zhong2015, wang2018}, to the best of our
knowledge there has been no significant previous study of the characteristics of
multi-location repairs in terms of their implications for automatic
repairability.  \todo{Something like: Instead, we chip away at pieces like the
  blind man at the elephant, or some some dumb analogy.  But punchy.}

To fill in this gap, and lay groundwork for more informed, systematic research
efforts in the space of multi-edit repair, we conduct a systematic study of real-world bugs with
multi-location patches.  We
look specifically at their characteristics with respect to the problem of the
automatic repair search space.  
We study bugs curated in two
real-world datasets that support program repair research: Defects4J~\cite{defects4j}
and \bears~\cite{bears}, in total, 1018 bugs in 51 projects.
% CLG asks: are these numbers accurate
More than half of these bugs were repaired by a
human developer using edits at multiple locations.  We look at characteristics along each of the
relevant axes of the program repair search problem: fault locations, mutation
operators, fix code, and evaluation (or fitness or objective) using test cases,
and find both important and, in some cases, unintuitive, implications for
automated techniques.  Our findings and contributions are:

\begin{itemize}
%\item An analysis and enumeration of multi-location patches in Defects4J and
%\bears.
% I think the above is addressed by the summary in the preceding paragraph.
\item \textbf{Fault Localization.}  We find that 58\% of multi-edit bugs have faulty locations that are not
  covered by all failing tests.  This is important because the assumptions underlying spectrum-based
  fault localization may not hold when used off-the-shelf for
  multi-edit bug repair, motivating the development of novel localization
  techniques. 
\item \textbf{Edit dependency.} We 
find that 45\% of bug patches contain dependent edits, and that such bugs
may be more difficult for current, state-of-the-art
techniques to automatically repair.  This means that APR techniques assuming edit
independence~\cite{hercules,theotherone} are applicable to a nontrivial propertion of complex bugs, while
still motivating development of new analyses that can reason about multiple,
related edits at once. 
\item \textbf{Cloning in multi-location repair} We find that over 30\% of bugs with multi-location
patches have very similar edits applied to different locations, suggesting the viability of 
program repair techniques designed to utilize code clones. Moreover, importantly, bugs in which 
none of the faulty locations are covered by all failing tests tend to include
such clones in their patches.  This suggests a path forward for predicting, a
priori, which type of technique may apply to a given bug. 
\item \textbf{Test cases for multi-edit patch validation.} We find that over 40\% of bugs with multi-location
  patches in Defects4J and over half in \bears do not require the edits at every
  location in their provided human patches to pass all test cases, suggesting
  that either patches contain unnecessary edits or that test cases
  do not fully capture the desired behavioral specification.  \todo{Think about
    how to ephrase this a bit more intuitively, and consider breaking up this
    bullet into multiple findings.} We also find that test 
  case based validation methods can positively identify close to 40\% of partial repairs,
  while less than
  20\% of partial repairs cause more test assertions to fail. Additionally, the 
  \emph{granularity} at which test suite behavior is measured (i.e., at the class,
  method, or assertion level) influences ability to identify partial
  repairs. 
\item dataset replication etc \todo{FIXME: Our dataset and analysis results,
    linked here anonymously, will be prettier for the camera ready we promise.}
\end{itemize}

In Section~\ref{sec:background}, we overview the problem domain to
motivate/outline our research questions, and characterize our datasets.
% We overview the key
%characteristics of the datasets that we use in our study in
%Section~\ref{sec:data-rq1}.  
We then describe the results of our study along the
three key axes of program repair: fault localization (Section~\ref{secFL}),
mutation operators and fix code (Section~\ref{sec:mutops}), and test cases as
the patch validation mechanism (Section~\ref{sec:tests}).  We conclude by
outlining limitations (Section~\ref{sec:limits}), related work
(Section~\ref{sec:related}).  Finally, 
% don't give up on this yet.  It's a complicated paper, roadmaps might actually
% meaningfully help
we summarize our takeaways for 
multi-location repair (Section~\ref{sec:takeaways}).

\section{Problem Definition}
\label{sec:background}

In this section we overview the background of automated program repair (APR) and
introduce key concepts that directly motivate our research questions.

\subsection{Automated Program Repair: a Search Problem}

The central goal of source-level automated program repair (APR) is to
automatically generate patches for bugs in programs. We restrict attention to
\emph{dynamic} or \emph{test-case guided} program repair, a prevalent class of
research techniques over the past decade~\cite{cacm19}.  Dynamic APR techniques
are characterized by their use of test cases that serve as the oracle for
program correctness, at least one of which is failing (i.e., exercises the bug
to be repaired).

At a high level, source level program repair is a \emph{search problem}, where
the objective is a set of source-level program edits that will cause it to pass
all of the provided tests. To do this, they first \emph{generate} a candidate
patch, then run the test suite to \emph{validate} it; for this reason they are
often called \emph{generate and validate} repair techniques. Generating a patch
requires navigating a search space, which is typically defined along the
following axes~\cite{ae,sqjo}:
\begin{enumerate}[wide]

\item \emph{Fault space.} The first problem with generating a patch concerns
  \emph{where} in the code a patch should be applied. Most dynamic repair
  techniques begin by using test cases as input to a fault localization
  technique. Such techniques identify (and typically score) suspicious code
  based on which test cases execute which pieces of code. Although the
  particulars of the fault localization employed can vary, most APR techniques
  use some variant of spectrum-based fault localization (SBFL)~\cite{ochiai} in
  this stage. The resulting computation defines the \emph{fault space}, or the
  candidate code locations considered for repair.

\item \emph{Mutation space.} After identifying a faulty program location, an APR
  technique must choose from the set of applicable modifications at the given
  point. Examples include GenProg's \emph{append}, \emph{replace}, and
  \emph{delete} mutation operators over statements~\cite{genprog-operators};
  PAR's human patch inspired edit templates~\cite{par}; and Nopol's condition
  replacement over \texttt{if} statements~\cite{Xuan17}. Although a larger
  mutation space allows the generation of a wider variety of patches with
  potentially more repairs, adding more mutations grows the entire generation
  search space combinatorially~\cite{long-search-spaces}.

\item \emph{Fix space.} Certain mutations must be instantiated with generated or
  selected code before they can be applied. For example, if a technique elects
  to insert a null pointer check, the specific object being checked for
  \texttt{null} must be chosen to generate a patch. This search space that must
  be navigated to instantiate a patch is referred to as the \emph{fix space}.
  The fix space can be quite large (or even infinite), and different APR
  techniques use different strategies to make it tractable. Some techniques
  tackle the problem by taking advantage of the \emph{plastic surgery
    hypothesis}~\cite{plastic}, which assumes that a bug can be repaired by code
  available in other parts of the same program. By contrast, learning-based
  approaches use models of code or repairs to inform
  modifications~\cite{prophet}, whereas synthesis-based approaches constrain a
  synthesis engine to a small vocabulary of fix ingredients, possibly informed
  by the code near the selected faulty location~\cite{angelix,s3}.
\end{enumerate}

After generation, patches must be validated. Candidate patches are typically
validated using the provided test cases as the objective function. In techniques
that require one, test cases can also be used as the \emph{fitness
  function}~\cite{genprog}; although some have supplemented the test case
objective with partial correctness measures like program
invariants~\cite{dinglyu}, memory snapshots~\cite{source-code-checkpoint}, or
similarity to human edits~\cite{hdrepair}, with maximization of tests passed
maintained as one of possibly many objectives.
%% JL: This might be too early, since we haven't talked about multi-location yet.
%% In a multi-edit context, ideally,
%% multiple tests would be able to idnetify partial repairs that could be composed.
%% However, evidence suggests that this does not always
%% apply~\cite{better-fitness,schulte}.

A practical implication of the size and complexity of this search space is
that APR techniques must engage in trade offs to render the problem tractable. 
Some techniques limit the \emph{fault space},
searching explicitly for single-edit patches~\cite{rsrepair,ae,hdrepair}.
\todo{let's add techniques post 2017 to this paragraph wherever they belong
  plz.}
All consider a constrained mutation space, varying in how constrained that space
is; some implicitly limit the mutation or fix space by targeting only certain
classes of bugs~\cite{Xuan17,sapfix,DeMarco14,par}. Regardless, even techniques
that can in principle generate repairs with multi-location patches typically
don't~\cite{patch-correctness}.
%
Meanwhile, many real-world bugs require multi-edit patches: over half of the
bugs in the popular bug benchmark Defects4J~\cite{defects4j} are patched by
humans using multi-part patches~\cite{d4j-dissection}. Approximately 70\% of
buggy source files in a large study of bugs in Apache projects required edits at
two or more locations~\cite{zhong2015}.

Our goal in this study is to understand what makes the generation of larger
patches difficult, and identify properties of these bugs and their associated
patches that can be exploited by future APR tools.  
We define this problem precisely and characterize our datasets next.

\subsection{Multi-Location Patches and Dataset}


We focus in this study on the properties of \emph{multi-location} patches. There are
several plausible definitions of multi- vs. single-location patches, with
implications for how they are studied. For the purpose of this study, we define
a \emph{patch location} as a contiguous sequence of edited lines of code.  We
combine two locations if one simply opens or closes a syntactic block inserted
by the other. That is, conceptually:
\lstset{escapeinside={<@}{@>}}
\begin{lstlisting}[style=examplestyle]
<@\textcolor{dkgreen}{+ Some\_edit \{}@>
<@\textcolor{dkgreen}{+ New code}@>
  Existing code
<@\textcolor{dkgreen}{+\}}@>
\end{lstlisting}
is treated as a single-location edit in our study. We ignore changes to
comments, whitespace, or \texttt{import} statements.
%
This intuitive definition is reasonably consistent with the general APR
paradigm. 
\todo{is it worth elaborating on which we exclude, anything that might be
  counterintuitive? Do we include declarations as separate edits? I know our
  philosophy is different from D4J, which is why I ask.}
%As a shorthand, we refer to bugs with multi-location human-written
%patches as \emph{multi-location bugs}.

\begin{table*}
\begin{center}
\begin{tabular}{l  rrr | rr | rr | rr}
\toprule
\multicolumn{10}{c}{\textbf{Defects4J}} \\
\midrule
Project & Bugs & Src (kloc) & Test (kloc) & \multicolumn{2}{c}{Multi-location} 
		& \multicolumn{2}{c}{Multi-test} & \multicolumn{2}{c}{Multi-location \&}\\
&&&&\multicolumn{2}{c}{bugs}&\multicolumn{2}{c}{bugs}&\multicolumn{2}{c}{Multi-test bugs}\\
\midrule
JFreeChart  & 26 & 193.3 & 74.6  & 11 & 42\% & 10 & 38\% & 7 & 27\%\\
Closure compiler & 174 & 150.6 & 112.6 & 55 & 41\% & 58 & 44\% & 31 & 23\%\\
Apache commons-lang & 64 & 57.8 & 47.4  & 32 & 49\% & 17 & 26\% & 13 & 20\%\\
Apache commons-math & 106 & 45.0 & 41.5 & 53 & 50\% & 28 & 26\% & 22 & 21\%\\
Mockito & 38 & 23.0 & 28.5 & 16 & 42\% & 18 & 47\% & 8 & 21\%\\
Joda-Time & 26 & 82.9 & 70.4 & 17 & 63\% & 13 & 48\% & 9 & 33\%\\
Apache commons-cli & 39 & 5.7 & 4.6 & 19 & 49\% & 13 & 33\% & 8 & 21\%\\
Apache commons-codec & 18 & 5.8 & 5.8 & 4 & 22\% & 9 & 50\% & 2 & 11\% \\
Apache commons-collections & 4 & 64.9 & 47.6 & 1 & 25\% & 0 & 0\% & 0 & 0\%\\
Apache commons-compress & 47 & 12.8 & 3.1 & 24 & 51\% & 9 & 19\% & 6 & 13\%\\ 
Apache commons-csv & 16 & 2.6 & 3.9 & 3 & 19\% & 3 & 19\% & 2 & 12\%\\
Gson & 18 & 16.8 & 12.8 & 6 & 33\% & 9 & 50\% & 5 & 28\%\\
FasterXML jackson-core & 26 & 27.9 & 8.1 & 13 & 50\% & 8 & 31\% & 5 & 19\%\\
FasterXML jackson-databind & 112 & 80.7 & 41.2 & 58 & 52\% & 14 & 12\% & 9 & 8\%\\
FasterXML jackson-dataformat-xml & 6 & 7.5 & 6.2 & 5 & 83\% & 2 & 33\% & 2 & 33\%\\
Jsoup & 93 & 7.9 & 2.2 & 32 & 34\% & 29 & 31\% & 21 & 23\%\\
Apache commons-jxpath & 22 & 28.7 & 7.6 & 16 & 73\% & 9 & 41\% & 8 & 36\%\\
\midrule
All (Defects4J) & 835 & 813.9 & 518.1 & 365 & 44\% & 249 & 30\% & 158 & 20\%\\
\midrule
\multicolumn{10}{c}{\textbf{\bears (single-module)}} \\
\midrule
FasterXML jackson-databind & 26 & 80.7 & 41.2 & 17 & 65\% & 4 & 15\% & 2 & 8\%\\
INRIA Spoon & 62 & 66.2 & 30.8  & 39 & 63\% & 23 & 37\% & 18 & 29\%\\
spring-data-commons & 15 & 45.8 & 28.8  & 9 & 60\% & 6 & 40\% & 2 & 13\%\\
traccar-traccar & 42 & 47.9 & 8.6 & 24 & 57\% & 3 & 7\% & 2 & 5\%\\
30 other projects & 38 & --- & --- & 22 & 58\% & 36 & 95\% & 9 & 24\%\\
\midrule
All (\bears) & 183 & $>$240.6 & $>$109.4 & 111 & 61\% & 72 & 39\% & 33 & 18\% \\
\midrule
Combined (Defects4J \& \bears) & 1018 & $>$1054.5 & $>$627.5 & 476 & 47\% & 321 & 32\% & 191 & 19\%\\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:dataset-characteristics} Characteristics of the Defects4J
  (top) and \bears (bottom) datasets.\todo{multi-location vs multi-test: don't
    belabor the point, but this caption should say SOMETHING about how to read
    the table.}}
\end{table*}

Our study requires a dataset of indicative, real-world,
multi-location defects.  We study both the defects in
Defects4J v2.0.0~\cite{defects4j} and \bears~\cite{bears}.  Table~\ref{tab:dataset-characteristics}
summarizes these datasets, both of which
consist of historical
bugs found in real world software projects. Defects4J contains 835 bugs from 
17 Java software projects, and is a popular dataset for evaluating 
program repair tools that target Java~\cite{durieux-repair-them-all}.\todo{add
  more references, there are other papers that use d4j to evaluate}
The dataset's patches are manually minimized to isolate the bug fix 
and exclude non-repair edits such as refactorings and feature additions.

With any dataset, however, there is a risk that tools may overfit
to the defects in question, and there is evidence that this situation applies to
program repair and Defects4J~\cite{durieux-repair-them-all}. 
We thus also study bugs from \bears~\cite{bears}, 
a set of Java bugs derived from failed Travis-CI builds of GitHub projects. 
\bears offers 251 bugs from 72 software projects, providing a greater diversity of 
projects compared to Defects4J. 
Several projects in \bears, however, are structured as multi-module projects, 
which are not currently compatible with our automation tools.
\todo{Is it worth expanding slightly on why/what this means?}
We thus limit our analysis of \bears to 183 bugs from 34 single-module projects.
\todo{what's going on with multi-location versus multi-test}

We start our analysis of multi-location patches by asking:
\rqorinsight{1}{How prevalent are human-written multi-location patches?}

Table~\ref{tab:dataset-characteristics} lists the numbers and percentages of
multi-location patches in Defects4J and \bears. 
We find that multi-location patches comprise 61\% of \bears and 44\% of Defects4J.
Although a multi-location human patch for a bug does not imply the 
non-existence of a simpler patch, the high proportion of bugs that have 
multi-location patches demonstrates the relevance of such bugs to fault localization and
program repair. 

\bears contains a greater proportion of 
multi-location patches compared to Defects4J. This may be the 
result of manual patch minimization in Defects4J
and lack thereof in \bears.
Thus, some \bears patches may be multi-location due to
non-corrective edits (e.g.: refactorings, feature additions).

Having established both that multi-location edits are common in real-world
programs, and established a suitable, plausibly indicative dataset to study
them, we decompose dynamic APR into multiple subproblems, driven by the
shape of the search problem it seeks to solve. We 
investigate properties of the multi-location fault space (Section~\ref{secFL}),
mutation and fix spaces (Section~\ref{sec:mutops}), and validation
(Section~\ref{sec:tests}). 

\section{Fault localization} \label{secFL}

%% what are the claims, what are we studying, why are we studying it

Spectrum-based fault localization (SBFL) is a commonly-studied dynamic
fault localization technique, and includes techniques like Tarantula~\cite{tarantula},
Ochiai~\cite{ochiai} and DStar~\cite{wong2013dstar}, among others.
%, and is often more effective than alternatives (like mutation-based fault 
%localization~\cite{mut-analysis} or dynamic program
%slicing~\cite{zou2019empirical}). 
Fault localization in general is a first key step to characterizing the
\emph{fault space} in automatic program repair; the majority of dynamic APR
techniques use some variant of SBFL for this purpose.

SBFL techniques compute a suspiciousness score by
measuring how often a line is executed by failing tests as compared to passing
tests, typically via some (linear) combination of their respective
coverage of a line. 
%SBFL techniques include but are not limited to Tarantula~\cite{tarantula},
%Ochiai~\cite{ochiai} and DStar~\cite{wong2013dstar}; in general, though more
%recent approaches are typically more effective, such approaches tend to localize similar
%faults~\cite{zou2019empirical}.
These techniques are often evaluated by asking: how often does a given technique assign a high 
score for a buggy line of code? This is straightforward for single location repairs, but for 
multi-location repairs, evaluations
typically consider a technique ``successful'' if it highly ranks or scores \emph{any} line in a
set of buggy lines. 
While appropriate for the question being asked in such evaluations, this does not address
suitability for multi-location program repair.%  Identifying one of several buggy
%locations is generally inadequate in a context where multiple locations must be
%modified. 

\todo{Does this paragraph make a strong enough case for why we focus on just the failing 
tests?}
To figure out how to evaluate SBFL and other fault localization techniques 
in multi-location contexts, we start by assessing the 
relationship of multi-location repairs to the assumptions underlying SBFL. 
Fundamentally, the core assumption underlying SBFL is that \emph{failing tests
execute buggy portions of the code relatively more often than passing tests.} Relatedly, we can 
often assume that \emph{code executed by all failing tests are more likely to be buggy.} Though 
this second assumption is not 
always true, we derive it in order to explore the relationship of failing test executions with each 
other.
% Thus, if all failing tests execute a particular line of code, then that line of
% code is highly suspicious.  


% One of
% the oldest and commonly studied SBFL technique is Tarantula~\cite{tarantula}. In
% Tarantula, the suspiciousness score for a line $s$ is calculated by:

% $$\mathit{susp(s)} 
% =\frac{\mathit{\%F(s)}}{\mathit{\%F(s)} + \mathit{\%P(s)}}$$

% where  $\mathit{\%F(s)}$ and $\mathit{\%P(s)}$ are, respectively, the percentage of failing 
% tests and passing tests that execute $s$. There are newer, more effective 
% SBFL techniques that calculate this score differently, such as Ochiai~\cite{ochiai} and 
%DStar~\cite{wong2013dstar}. 


From these assumptions, we derive our research question: 

\rqorinsight{2}{Do failing tests of a multi-location bug execute all the same patch locations or 
all different patch locations?}

We focus especially on bugs that are associated with multiple failing tests: a bug
with only a single failing test trivially and equally implicates all the lines
that test executes. 

%If multiple tests cover the modified locations well, then multi-location repair can expect to 
%effectively
%make use of the off-the-shelf ranking these techniques currently provide
%(indeed, this has been tried~\cite{angelix}). If not---that is, if multiple
%tests exercise \emph{different} portions of the buggy code---SBFL off-the-shelf
%will by definition be less effective in guiding APR to correctly modifying
%multiple buggy locations at once.
%Fundamentally, we ask whether multiple failing tests cover exactly the same
%patch locations, exactly different patch locations, or some combination.

In addition, RQ2 determines which multi-location bugs may be more suited for SBFL 
based on how well the failing tests cover different patch locations. However, to pragmatically 
make use of this classification and be able to dynamically choose fault localization and program 
repair techniques, we want to determine which bugs are well suited for SBFL before knowing 
the patch locations. Since our proxy for suitability is based on whether failing tests execute 
the same patch locations, we ask:

\rqorinsight{2.5}{Do failing tests that cover different (resp. same) fault locations also cover 
different 
(resp. same) code locations in general?}

\subsection{Methodology}

Between both datasets, there are 191 total bugs that both require multi-location
patches and contain multiple failing tests. However, we were not able to obtain coverage 
data for one of the bugs in Bears, leaving 190 total bugs: 158 in Defects4J, and 32 in
Bears. 
For each of these bugs, we used JaCoCo\footnote{https://www.eclemma.org/jacoco/}
to determine which lines of code in both the buggy and patched versions were executed
by each failing test.

To answer RQ2, we used the patch lines that were executed by the tests to categorize 
each bug into three \emph{localization categories}: \emph{holds}, \emph{contradicts}, and 
\emph{partially holds}. These three localization categories refer to the relationship each bug has 
with 
the assumption that code executed by all failing tests are more likely to be buggy. Each bug is 
categorized using the following criteria:

\begin{itemize}
\item \emph{holds}:  all failing tests execute the exact same set of patch lines.
\item \emph{contradicts}: no patch line is covered by all failing tests.
\item \emph{partially holds}: some patch lines are covered by all failing tests, but some are only 
covered by a subset of the failing tests.
\end{itemize}

In our experiments, we classify bugs using coverage of the patch lines, as opposed to 
coverage of 
patch locations (where a patch location is considered covered if any line at that patch location 
is 
executed). For the purposes of fault localization, coverage of patch locations is more valuable 
than coverage of patch lines, and we analyzed 66 Defects4J bugs\footnote{Only the 
bugs in Defects4J version 1, because of the way we calculate coverage information.}
 and all the Bears bugs, a total of 98 bugs, to see whether categorization 
based on patch line 
coverage matched categorization based on patch location coverage.

To answer RQ2.5, we took all code locations executed by tests in the buggy version and
calculated the percentage of lines that were executed by all failing tests. 
A lower percentage indicates that the failing tests execute different portions of the buggy 
code, whereas a higher percentage indicates that the failing tests all execute a similar
set of code, corresponding to bugs we expect to be in \emph{contradicts} and \emph{holds},
respectively.

After calculating the percentages, we split the bugs based on whether we categorized the bug 
as \emph{contradicts}, \emph{holds}, or \emph{partially holds} in the previous experiment.
We qualitatively observed the degree to which the percentage corresponded to its coverage 
pattern. \todo{Use a real statistical measure}\todo{Yes, really.}

\todo{Combine these three figures into 1}

\begin{figure}
	\includegraphics[width=\linewidth]{img/coverage_hist_all.pdf}
	\caption{Distribution of localization categories for bugs with multiple failing
      tests that are repaired with multi-location patches. In general, the categories are relatively 
      evenly populated, with a slightly higher proportion of bugs in \emph{partially holds}. In 
      addition, the two datasets produce different distributions, reflecting the different 
      ways these datasets were collected.}
	\label{fig:coverage-all}
\end{figure}

\subsection{Results}

\subsubsection{Distribution of Localization Categories} \label{sec:cov_patterns}

Figure~\ref{fig:coverage-all} shows results for the overall distribution of localization categories, 
answering RQ2. 
For a 29\% bugs, none of the patch lines were executed by all failing tests as they were 
categorized as \emph{contradicts}.  An almost equal proportion of bugs, 32\%, were classified 
as 
\emph{holds}, indicating that all failing tests executed the same patch lines.
In addition, another 39\% were classified as \emph{partially holds}.

Note, however, that the behavior varies considerably by dataset;
Figure~\ref{fig:coverage-datasets} shows results. In Defects4J, the patterns all have similar 
numbers of bugs, while in Bears, there are fewer bugs in \emph{contradicts} and more  bugs in 
\emph{partially holds}.
We hypothesize that this may be due to differences in how the two  
datasets were selected and constructed:
The Defects4J dataset specifically enforces a requirement that the patches in the 
dataset be isolated, i.e., not containing any refactorings or new features, to improve the 
usability of the dataset. The authors specifically chose patches that met this requirement, 
and in some cases, manually isolated the bug themselves~\cite{defects4j}. By contrast, the 
bugs in 
Bears are scraped directly from continuous integration systems, and the 
only requirements for inclusion is that the bug must be reproducible and that
the patch must be written by a human. In addition, Bears was designed to be evolvable 
and relatively easily expanded as a dataset, which is at odds with manual inspection and isolation of 
bugs~\cite{bears}.
Given that these two datasets were designed with different values and demonstrate very 
different behavior, these findings highlight the importance of using diverse datasets in 
evaluating program repair techniques.

Out of the 98 bugs we checked, only seven bugs had differing localization categories when 
classified based on coverage of patch lines vs. coverage of patch locations. All seven of these 
bugs 
were classified as \emph{partially holds} when classified by line coverage, but were classified as 
\emph{holds} when classifying by patch coverage, indicating that all the failing tests were 
executing different paths within the same patch locations. \todo{Is this discussion confusingly 
worded? Because patch locations refer to the patch chunk, made up of one or more lines of 
code.}

Overall, we cannot assume that the failing tests in multi-location repairs will execute all the 
faulty locations -- in fact our results suggests that assumption only holds 32\% of the time. 
\todo{The original paragraph had this claim, but IDK if we can say something like this anymore:
Our results suggest that off-the-shelf SBFL techniques are not
well-suited to guiding APR techniques that conform to the dominant paradigm to repairing
these types of multi-location, multi-test bugs.}

\begin{figure}
	\includegraphics[width=\linewidth,left]{img/coverage-buggy.pdf}
	\caption{Boxplots representing the distribution of bugs based on it's percentage of lines 
	executed by all failing tests, measuring the degree to which failing tests execute the same 
	code. The distributions for the \emph{contradicts} and \emph{partially holds} are more or 
	less uniformly distributed in the percent of code executed by all failing tests. The 
	distribution for the \emph{holds} category skews towards higher percentages, but the range 
	still spans across 5\% to 100\%. Thus, there seems to be little correlation between 
	localization categories and whether failing tests all execute the same code in the buggy 
	program.}
	\label{fig:coverage-buggy}
\end{figure}

\subsubsection{Generalization of localization categories}
Our results in Figure \ref{fig:coverage-buggy} answer RQ2.5. Here, we see three distributions, 
separated by localization category, and plotted based on the percentage of lines executed by all 
failing tests. 

The distributions of bugs in \emph{contradicts} (and to a lesser extent, bugs in \emph{partially 
holds}) are 
distributed almost uniformly across the 0\% to 100\%, indicating that \emph{contradicts} bugs 
are 
not more or less likely to have failing tests that cover the same or different parts of the code 
base. Note that a bug categorized as \emph{contradicts} can be scored 100\% as the 
patch can introduce if statements that change the control flow and cause all failing tests to 
eventaully execute all different parts of the patch.

In contrast, we can qualitatively observe that bugs in \emph{holds} are more likely to have 
failing tests that execute the same lines of code, as we might expect. However, this is not a 
definitive characteristic, as \emph{holds} bugs in our dataset scored anywhere between 
5\% to 100\%.

From these observations, we conclude that the degree to which failing tests cover the buggy 
code are not effective predictors for the eventual localization category for the bug. 

\section{Mutation Operators and Fix Code}
\label{sec:mutops}

Given suitably selected fault locations, APR techniques vary in the types of
mutation operators they consider, how they select between them, and how they
select new fix code to instantiate them, as necessary. 
%  For example, a na{\"i}ve
% approach with only \texttt{insert}, \texttt{replace}, and \texttt{delete}
% operators must choose between them at a location and, in the case of
% \texttt{insert} and \texttt{replace}, choose code to insert/replace at that
% location.  
%
The few techniques that handle or at least enable multi-edit patches vary in their
handling of mutation operator selection and instantiation.  At one
extreme, semantics-based repair~\cite{s3,angelix} can represent dependent edits between multiple
locations as a conjunction of multiple constraints to simultaneously solve,
bounded by some number of edits and code components that are computationally feasible. 
At the other extreme, search-based or
evolutionary techniques~\cite{genprog,par} typically treat different mutation
operators independently, relying on the search to 
% That is, a modification in one location does not
%inform the selection of a modifications to apply in a second location; instead,
%the heuristic search is trusted to 
identify copacetic combinations.  The size of
the search space increases combinatorially in this context, however, rendering
the chances of finding suitable multi-edit repairs without additional guidance
quite low~\cite{ae,long-search-spaces}. Accordingly, heuristic multi-location
techniques~\cite{saha2019harnessing} make assumptions about the 
shape of the search space, 
targeting in particular bugs that can be repaired by multiple syntactically similar pieces of
fix code.

Note that the types of edits included in human patches has been
studied~\cite{examples}, with references for both search space size generally
and, potentially, patch quality~\cite{moreexamples}.  Here, we are especially
concerned about the 
\emph{relationships} between multiple edits, because such relationships have
key implications for how edits should be designed, selected, and instantiated 
in automatic multi-location repair. 

\subsection{Dependencies}

One of the dimensions of the relationship between multi-location patches is potential
\emph{dependency} between the edits, because edit dependency can and should
inform how those edits are selected and applied (i.e., consider the difference
between a multi-location
bugs repaired by applying the same independent edit in four places, as
compared to a bug repaired by applying  four control- and data-dependent
modifications).  Specifically, we ask:
\rqorinsight{4}{How prevalent and hard to construct are
patches with intra-patch dependencies?}


\paragraph{Methodology}
We consider a patch to contain \emph{dependent edits} if there exist 
control or data dependencies between added, deleted, or modified statements.
Since we analyze dependent statements, we broaden the scope of multi-location patches to 
include all patches containing at least two added, removed, or modified lines, 
again ignoring edits to comments, whitespace, or imports.
We denote such patches as \emph{multi-line patches}.
This expanded dataset contains 659 Defects4J v2 and 151 \bears 
multi-line patches.
% We explicitly got rid of Table 1 stats on multi-line 
% patches for ESEC/FSE. We did so since we didn't wish to divert attention 
% from multi-location patches during our discussion of the datasets in Sec 2.
% Dependency analysis is the only experiment that analyzes patches at the
% edit granularity of lines, rather than locations.
For practical performance and scalability reasons, 
we perform intraprocedural analysis. 
We do, however, gather some interprocedural data dependency information 
using the following heuristics:
\begin{itemize}
	\item We treat all method call statements as dependent on all variables
          passed as arguments. 
	\item If a statement invokes a getter method \texttt{Class.getX()} 
          we heuristically assume that the statement reads \texttt{Class.X},
          whether or not \texttt{Class.X} exists.
	\item If a statement invokes a setter method \texttt{Class.setX()}, 
	we again heuristically assume that the statement writes to \texttt{Class.X}. 
\end{itemize}

These heuristics are not sound, but they derive from common 
practices in Java. We use these heuristics to estimate interprocedural 
dataflow while retaining the scalability of intraprocedural analysis.
We provide our dependency analyzer in our replication package.

We estimate the difficulty of auto-constructing edit-dependent patches 
by examining whether APR techniques have successfully patched the
bug in question. We use APR success data from 
RepairThemAll~\cite{durieux-repair-them-all}, an experiment 
running 11 APR tools on 5 benchmarks, including Defects4J v1
(containing a subset of Defects4J v2) and \bears.

\paragraph{Results}

\begin{table}
{\begin{center}
	\begin{tabular}{l  rr  rr  rr}
		\toprule
		\multicolumn{7}{c}{\textbf{Patches with Dependent Edits}} \\
		\midrule
		APR (RepairThemAll) & \multicolumn{2}{c}{Defects4J} & \multicolumn{2}{c}{\bears} & \multicolumn{2}{c}{Combined} \\
		\midrule
		Success by any tool & 43 & 32\% & 9 & 9\% & 52 & 23\% \\
		Failure by all tools & 93 & 68\% & 86 & 91\% & 179 & 77\% \\
		Not evaluated & 130 & --- & 0 & --- & 130 & --- \\
		\midrule
		Total evaluated & 136 & 100\% & 95 & 100\% & 231 & 100\% \\
		Total & 266 & --- & 95 & --- & 361 & --- \\
		\midrule
		\multicolumn{7}{c}{\textbf{Patches without Dependent Edits}} \\
		\midrule
		Success by any tool & 96 & 57\% & 7 & 13\% & 103 & 46\% \\
		Failure by all tools & 73 & 43\% & 49 & 87\% & 122 & 54\% \\
		Not evaluated & 224 & --- & 0 & --- & 224 & --- \\
		\midrule
		Total evaluated & 169 & 100\% & 56 & 100\% & 225 & 100\% \\
		Total & 393 & --- & 56 & --- & 449 & --- \\
		\bottomrule
	\end{tabular}
 \end{center}
}
	\caption{APR success of 
	bugs with(out) edit dependencies in their multi-line patches. 
	Bugs with intra-patch dependencies are harder to auto-repair. }
	\label{tab:dependency-repair-contingency-table}
\end{table}

Table~\ref{tab:dependency-repair-contingency-table}
shows frequencies of multi-line patches with respect to edit dependence 
and whether an APR tool successfully repaired the bug.
With 45\% of multi-line patches exhibiting intra-patch dependencies, 
our data substantiates prior research~\cite{zhong2015} on 
such dependencies.


We find the presence of edit dependencies 
reduces APR tool success.
Using a $\chi^2$ test, we find a statistically significant relationship ($p < 0.001$)
between APR success and intra-patch dependencies.
The generally lower auto-repairability of bugs with edit dependent patches compared 
to their non-edit dependent brethren suggest that such dependencies indeed 
add complexity to the search for a repair.
Dependent edits, however, may 
be an opportunity to constrain the search space by creating constraints between 
otherwise independent mutations. There is an opportunity to profit from edit 
dependencies to repair a large class of difficult bugs.


\subsection{Cloned code}

If multi-location patches composed of multiple, dependent edits constitute one
extreme, at the other extreme lies patches consisting of the same code applied
in several locations (of course, between those extremes lie patches consisting
of a mix of both). Previous techniques have targeted exectly these types of
bugs~\cite{wang2018,saha2019harnessing}.  We evaluate how often this relationship applies to
multi-location bugs, and thus the usefulness of techniques that assume it, by asking: 

\todo{I don't love mushing these two questions into 1.  It's not obvious why
  we're asking the second question, nor why it's so closely tied to the first
  question that it doesn't deserve it's own number.}
\rqorinsight{4}{How often do code clones occur in multi-location bugs? Is the existence of 
	code
	clones in human patches correlated with specific patterns of fault localization?}

\todo{This whole section needs to be updated to match Leo's rephrase of that section.  And, I
  advocate a decoupling of the two research questions, as above, so this will
  need to be updated to flow accordingly.}
We further attempt to correlate the coverage patterns outlines in
Section~\ref{secFL} with the existence of code clones in human patches.  Our 
intuition is that a \emph{contradicts} bug may be more likely to occur when a
developer needs to apply the same fix at multiple independent locations, that
can or should be tested separately.  We therefore 
 hypothesize that \emph{contradicts} bugs will have a higher incidence of code
clones. By contrast, bugs categorized as \emph{partially holds} or \emph{holds} may have
more inter-related parts that are not merely the same statements copied to
multiple locations.

%Possible intuition: if we can find some kind of correlation between fault localization results
%and code clones, then if some APR research decided to follow Wang et al's suggestion and
%include "repeat same edits at multiple location" operator, then our results may advise
%the APR to be more likely to apply the repeat edit operator when the fault localization
%result matches specific patterns.

\subsubsection{Methodology}
\label{sec52}%...sec 52? wha?
Our dataset includes all multi-location bugs excluding those with more than 6 faulty locations to 
constrain the search space, leaving 216 bugs to analyze. We determine the existence of code 
clones through manual inspection.

% We cross validate our list of bugs with clones with results from previous work proposing and 
%evaluating an APR tool named Hercules~\cite{saha2019harnessing}. Hercules is a generate and 
%validate approach that could also exploit similar code locations to generate patches with code 
%clones. It was evaluated with the bugs in Defects4J version 1, which consists of 395 total bugs 
%and 187 multi-location bugs. 

In comparing these results with the results of the coverage experiment in
Section~\ref{secFL}, we focus on the multi-test and multi-location bugs used in the coverage 
experiment and exclude bugs with more than 6 faulty locations, leaving 166 bugs.

We call the edits at two locations code clones if it is one of the following four cases:
\begin{enumerate}
\item Same: The two locations are alpha-equivalent (i.e. the two locations are identical or would be
identical if we substitute all occurrances of a variable in one location with another same-type variable)
\item Near-same: The two locations are alpha-equivalent except the difference by at most one constant or arithmetic operator,
or replacement of one constant with variable
\item Composite: The patch at one location is exactly copied and contained within the patch at 
the 
second location (the second location  has additional lines in the patch).
\item Move: Two locations forms a "movement" of code (i.e., one location is an insertion of 
code 
while the other is a deletion of the same code, essentially moving the code from one location to 
another).
\end{enumerate}

\subsubsection{Results}

\begin{table}
{\begin{center}
\begin{tabular} {lrrrrrr}
\toprule
&&\multicolumn{2}{c}{Dataset} &\multicolumn{3}{c}{Locality}\\
& Total & Defects4J & \bears & Method & Class & Neither\\
\midrule
Same      &  80 &  71 & 9 & 35 & 33 & 12 \\
Near-same   &  25 &  23 & 2 & 12 &  8 &  5 \\
Composite &  12 &  11 & 1 &  4 &  8 &  0 \\
Move      &  12 &  11 & 1 &  9 &  1 &  2 \\
\midrule
Any       & 121 & 108 & 13\\
Total     & 372 & 308 & 64\\
\bottomrule
\end{tabular}
\end{center}
}
\caption{\todo{JL: Finish fixing this table and also delete the other table that
    I took the locality stuff from.} Code clone information for multi-location bugs. \todo{I advocate minor
    renaming as mentioned above; must update this table/text/caption accordingly.}
     \emph{Same}, \emph{Near-same}, \emph{Composite} and \emph{Move} correspond to the
    four types of code clones considered, as described in Section~\ref{sec52}. ``Any'' means the bug
    contains any of the four types of clone; some bugs contain
    pairs of clones that belong to different categories. 
    Over
    30\% of multi-location bugs has code clones.}
\label{tab:clones}
\end{table}

As shown in Table~\ref{tab:clones}, out of the 372 multi-location bugs,
121 of them, or  32.5\%, included at least one type of cloning between fault locations, indicating a 
significant 
prevalence of code clones in multi-location human patches.
Note that "Any" may not
be the sum of the previous four rows because some bug could contain multiple pairs of code clones 
that
belong to different categories.


\begin{table}
{\begin{center}
\begin{tabular} {lrrrr}
\toprule
& Same Method & Same Class & Diff Class & Total\\
\hline
Same & 35 & 33 & 12 & 80 \\ 
Near-same & 12 & 8 & 5 & 25 \\
Composite & 4 & 8 & 0 & 12 \\
Move & 9 & 1 & 2 & 12 \\
\midrule
Total & 60 & 50 & 19  & 129\\
\bottomrule
\end{tabular}
\end{center}
}
\caption{
    Relative locations of sets of code clones in each category (defined in \ref{sec52}).
Same Method denotes that the code clones occur in the same method, Same Class denotes that the code
clones are in the same class but not in the same method, and Diff Class denotes that the
code clones are in different classes.
The majority of code clones occur within the same method, and are of
the ``Same'' category.}
\label{tab:clones_loc}
\end{table}

As shown in Table \ref{tab:clones_loc}, close to half of code clones are in the same method, while less than
15\% of the code clones occur in different classes. Moreover, out of the 129 sets of code clones,
80 of them belongs to the ``same'' category, which means that 62\% of sets of code clones have completely
alpha-equivalent edit locations within them.

% code clones <-> hercules data
% idk how what the conclusion from this information is so I'm just gonna put numbers in
% Hercules successfully repaired 15 multi-location bugs in their dataset. We found 56 
%multi-location bugs with clones. 14 of the bugs that Hercules successfully repaired were among 
%the bugs with code clones we found. The remaining bug that Hercules successfully repaired 
%was 
%a bug whose patch required 6 or more locations, and in going back, the human patch also 
%contained code clones. 

\begin{table}
  {\begin{center}
      \begin{tabular} {lrrrr}
        \toprule
        & Contradicts & Partially Holds & Holds & Total \\
        \midrule
        Has Clone & 25 & 12 &  7 &  44 \\
        No Clone  & 23 & 48 & 51 & 122 \\
        \midrule
        Total     & 48 & 60 & 58 & 166 \\
        \bottomrule
      \end{tabular}
    \end{center}
  }
  \caption{Multi-location and multi-test bugs categorized by coverage pattern
    and presence of clones; definitions for Contradicts, Partially Holds, and
    Holds are taken from our experiments on fault localization.}
  \label{tab:cov_clones}
\end{table}

In Table \ref{tab:cov_clones}, we show the incidence of code clones among the three 
coverage patterns identified in Section \ref{secFL}. Note that out of 372 bugs evaluated for code clones, only 166 of them have multiple failing tests, so only these bugs are included in the table.  Out of 48 bugs labeled as 
\emph{contradicts} in the coverage, over half of them has code clones. In contrast, the 
\emph{partially holds} and \emph{holds} categories respectively had 20\% and 12\% bugs with code clones, 
This indicates that a 
bug with \emph{contradicts} coverage result is more likely to contain code clones in its
human patch than bugs with other coverage results. 

This result supports the hypothesis that \emph{contradicts} bugs are more likely to contain 
code clones, while the other two classifications have comparatively less code
clones. This has potential implications for technique design: that is, if there
appears to be relatively less coverage overlap between multiple tests, it is
possible that, if multi-location, the bug may be \emph{contradicts}.  If so,
applying the same edits at multiple locations may be more likely to succeed.  We
leave a concrete investigation of this possibility to future work.



\section{Test case-based validation}
\label{sec:tests}

Dynamic program repair uses test cases to validate patch plausibility, and tests
are often used as a proxy for a full correctness specification. Tests are
imperfect for this task, and virtually all APR techniques can \emph{overfit} to
them~\cite{Smith15fse}, but they remain accessible and widely-used in practice.
%patches that satisfy the provided tests, but do not generalize to the
%higher-level specification~\cite{Smith15fse}.  
%
%Indeed, not all edits in a human patch are always necessary to pass all tests. This could be
%due to a human patch including refactoring or other changes that does not actually
%change code behavior~\cite{api-refactoring, tangledchanges}, or it could be a sign that the test
%suite is inadequate.  
In evolutionary or search-based approaches, test case quality is even more
pressing, as tests are typically used to define an objective or fitness
function. In these contexts, ideally, test cases could usefully identify partial
solutions~\cite{better-fitness}, or subsets of edits that comprise the eventual
patch, though whether tests are suitable for this purpose is a matter of
debate~\cite{ae,rsrepair}.  Sometimes, tests perform no differently on partial
repairs than they do on the original program~\cite{chris-thesis,
  source-code-checkpoint}; they have also been anecodotally observed to perform
worse~\cite{gecco09}.  

We study this question in a principled fashion to characterize whether or how
tests can support multi-location patch construction. To illustrate the insight
behind this line of analysis, assume a multi-edit patch. If we apply only one of
several of the edits in this patch to the buggy program, the result should
ideally be identifiable as ``closer'' to the full repair as compared to the
original buggy program. On average, do tests behave accordingly? We formalize
this question as:

\rqorinsight{6}{How well do test case based validation methods identify partial repairs?}

If tests can in general identify partial repairs, this suggests the feasibility
of techniques that construct multi-location repairs by identifying and composing
partial repairs into full repairs.  

% CLG: ...can we get away without mentioning granularity here? Let's find out....
% Moreover, unit test performance can be measured in different granularity levels. 
% We would like to compare the performance of the fitness functions at identifying 
% partial repairs at different granularity levels.


\subsection{Methodology}
\label{sec:partial-repair-methodology}

In this experiment, we effectively apply all partial repairs for
a subset of multi-location bugs, and
evaluate how well the partially-patched programs perform 
as compared to both the original and the fully-patched versions. 

\subsubsection{Partial Repairs and Bug Selection}
We apply each non-empty subset of all location-level edits for a given bug to
its buggy program.  To minimize semantically invalid partial repairs, we \emph{always}
include edits that add import statements, helper methods/classes, or variable declarations
in a partial patch. 
For example, if a 3-edit patch contains a declaration of new variable
\texttt{var} at one location that is used at a second location, but not a third,
we include the first edit in all partial patches.  Note that this can reduce the total
number of partial patches considered per patch, and that we eliminate from
consideration human patches that contain only 
one edit location after this preprocessing step (e.g., a two-location patch that introduces a
helper method in one location and calls it in the other).  

Due to the exponential growth of partial patches with respect to overall patch size,
we evaluate multi-location bugs whose patches 
contain between two and six edit locations after preprocessing.
Also, we exclude any bugs from Closure project in Defects4J since they use non-standard testing setups
Overall, we generated and evaluated 1884 Defects4J and 596 \bears partial repairs,
derived from 240 Defects4J and 74 \bears bugs.

\subsubsection{Test Granularity}

A subtlety arising in these experiments is that there are several ways to
evaluate and compare JUnit test results.  The standard use case for JUnit runs
an entire test \emph{class}, which ``passes'' if all test \emph{methods} in
that class also pass.  However, for the purposes of identifying partial repairs,
we can evaluate partial correctness at both the \emph{class-level} and
\emph{method-level} granularity. 
We also introduce and consider \emph{assertion-level} granularity, looking
roughly at how many assertions \emph{within} a method fail.  We define this as follows:
for each test method $M$, let $A(M)$ be the set of all assert statements in $M$. 
When $M$ is run, if an assertion failed, the failure is recorded and the method 
is continues to run (by contrast with the normal operation, where the test
method terminates with an error).  When the method completes, for each assertion
$a\in A(M)$, let $b(a)$ be 1 if $a$ never failed once during the running of $M$, 
and 0 otherwise. We define the assertion score of $M$ to be 
$AS(M)=\frac{\Sigma_{a\in A(M)}b(a)}{|A(M)|}$. If $M$ failed to run to completion 
due to timeouts or exceptions that are not related to assertions, then
$AS(M)=0$. Thus by definition, $AS(M)=1$ if $M$ passes. If a program passed more 
assertions in $M$, there should be an increase in $AS(M)$.


\subsection{Results}

Our first immediate takeaway is that not all edits
in a human patch are always necessary to pass all tests. 
Some patches 
include refactoring or other semantics-preserving changes~\cite{api-refactoring,
  tangledchanges}, while for others, test suites may be inadequate. 
We find that 41.3\%  (99 out of 240) of Defects4J bugs and 52.7\% (39 out of 74) of
\bears bugs do not need all edits in the human patches
to pass all tests (the difference between the two datasets is statistically
significant, $p < 0.001$, via a 
$\chi^2$ test). 
The relatively high proportion of Defects4J patches that are not 1-minimal is
somewhat surprising, as Defects4J maintainers manually minimize patches to exclude
changes deemed unrelated to the bugs in question~\cite{documentation-maybe}.
This suggests that the test suites are weak proxies for total correctness.
\bears patches are not minimized, which likely explains
their higher percentage of non-minimal patches (52.7\%).

We compute and present results for RQ6 using the 1-minimal set of edits
in the human patch with respect to the unit tests as the ``full'' patch.  
We do this for parsimony: including
unminimized edits introduces some noise but does not materially change the
the results.  And, from the point of view of an APR tool, only
minimal edits are identifiable or ``visible'' via test case behavior. 
We exclude bugs
minimized to a single edit, leaving
942 Defects4J and 232 \bears partial repairs, from
165 Defects4J and 44 \bears bugs.  

% We therefore present our results for RQ6 in two ways:
% treating the provided human patch as the full repair
% (Unminimized), and considering only the 1-minimal set of edits in the
% human patch with respect to the unit tests (Minimized).

Our overall goal is to identify how many partial repairs pass more test than the
original buggy program  (\emph{positive}), the  same number of tests as the
original buggy code (\emph{neutral}) or fewer tests than the buggy code
(\emph{negative}).  However, 
since each bug may have a different 
number of partial repairs (based on the number of implicated locations per
patch), scale the partial repair results such that each bug has equal weight.
That is, for a bug with $n$ edits, each edit has weight 
$\frac{1}{n}$. We do this for tests at each level of granularity.  Partial
repairs that do not compile are
excluded from the count.\footnote{e.g., by throwing errors like ``missing return statement.''}


%\begin{table}
%{\begin{center}
%\begin{tabular}{ll|rr|rr|rr}
%\toprule
%\multicolumn{2}{c}{}&\multicolumn{2}{c}{Defects4J} & \multicolumn{2}{c}{\bears} &\multicolumn{2}{c}{Combined}  \\
%\multicolumn{2}{c}{Minimized?} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes}  \\
%\midrule
%\multirow{3}{*}{Positive} & Class & 25.95\% & 12.77\% & 23.87\% & 9.22\% & 25.46\% & 12.02\% \\
% & Method & 40.82\% & 34.58\% & 28.41\% & 18.23\% & 37.90\% & 31.14\% \\
% & Assertion & 44.49\% & 39.93\% & 28.68\% & 18.69\% & 40.76\% & 35.46\% \\ 
%\midrule
%\multirow{3}{*}{Neutral} & Class & 59.56\% & 68.30\% & 58.97\% & 73.31\% & 59.42\% & 69.35\% \\
% & Method & 39.62\% & 40.05\% & 53.08\% & 64.30\% & 39.93\% & 40.83\% \\
% & Assertion & 33.73\% & 31.52\% & 49.52\% &  59.60\% & 37.45\% & 37.43\% \\ 
%\midrule
%\multirow{3}{*}{Negative} & Class & 9.30\% & 11.57\% & 8.78\% & 13.30\% & 9.18\% & 12.16\% \\
% & Method & 13.51\% & 17.16\% & 10.13\% & 13.30\% & 12.71\% & 16.35\% \\
% & Assertion & 15.37\% & 19.80\% & 12.75\% &  16.41\% & 14.75\% & 19.09\% \\ 
%\bottomrule
%\end{tabular}
%\end{center}}
%\caption{Weighed percent of partial repairs exhibiting a {\normalfont Positive}, {\normalfont Neutral},
%	or {\normalfont Negative} change in test passage, measured at different levels of granularity
%	and with both minimized and unminimized patches.\todo{big ole todo: cut
%          the ``unminimized'' results and turn this into a histogram; see my bad
%          sketch in Slack.}}
%\label{yiweitable}
%\vspace{-0.5in}
%\end{table}

\begin{figure}
        \includegraphics[width=1.05\columnwidth]{img/weighted_percent.pdf}
        \caption{Weighted percent of partial repairs exhibiting a {\normalfont Positive}, {\normalfont Neutral}, or {\normalfont Negative} change measured at different levels of granularity on minimized patches}
        \label{fig:fitness}
\end{figure}


Figure~\ref{fig:fitness} shows results.
Assertion level granularity correctively positively identifies
39.93\% of the partial repairs in Defects4J
and 18.69\% of the partial repairs in \bears.
Only 31.52\% of Defects4J partial repairs and 
59.60\% 
of \bears partial repairs are neutral. Less than 20\% of bugs in both datasets
(minimized or unminimized) produce negative test case behavior.
Although the rate of positive and correct identification of partial repairs varies between datasets,
tests are not often adversarial towards partial repairs, contrary to anecdotal
observations in prior work~\cite{gecco09}.

Finer granularity levels are better at identifying partial repairs positively,
but they also increase the chance of erroneously mis-identifying partial repairs.
Improvements between granularity levels vary across datasets --- the improvement 
in positively identified partial repairs between method and assertion level granularities
is negligible in Bears (0.46\%) but greater in Defects4J (5.35\%), while both had around 3\% increase in negatively identified partial repairs.
Moreover, evaluating patches at finer granularity levels can add overhead.
For example, assertion level granularity requires instrumenting JUnit with additional
logic to continue execution when assertions fail.
Such overhead lowers the number of
candidate patches that a G\&V tool can validate per unit time.
The choice of a granularity level is a balancing act with implications for
multiple axes of APR performance.

\subsection{Correlation studies between coverage results and parial repair validation}

As shown in the previous subsections, correct partial repairs of bugs sometimes has better test
performance than the original buggy code, but sometimes they perform worse. In a search based APR technique that
uses test results to guide the search, ideally we want to never discard a correct partial repair, so if the bug
is likely to have correct partial repairs that are neutral or negative in tests, the 
APR tool may want to loosen the selection criteria such that the correct partial repair may survive. 
One likely source of information about the bug is the fault localization.
Therefore, we did a
correlational study between the patch coverage of failing tests and the existence of strictly positively/negatively
identified partial repairs, in hopes of discovering some interesting correlations that may inform
Search based APR research in controlling selective pressure with fault localization informations.

\rqorinsight{7}{Are there correlations between patch coverage and existence of positively/negatively 
identified partial repairs in multi-test, multi-location bugs?}

\begin{table}
  {\begin{center}
      \begin{tabular} {llrrrr}
        \toprule
        \multicolumn{2}{c}{Partial Repair} & Contradicts & Partially Holds & Holds & Total \\
        \midrule
        \multirow{2}{*}{Positive} & $\exists$  & 40 & 30 & 12 &  82 \\
                                  & $\nexists$ &  1 &  4 & 16 &  21 \\
        \midrule
        \multirow{2}{*}{Negative} & $\exists$  &  2 &  8 & 16 &  26 \\
                                  & $\nexists$ & 39 & 26 & 12 &  77 \\
        \midrule
        Total                     &            & 41 & 34 & 28 & 103 \\
        \bottomrule
      \end{tabular}
    \end{center}
  }
  \caption{Comparison of patch coverage and partial repair test passage.}
  \label{tab:cov_fitness}
\end{table}

Table~\ref{tab:cov_fitness} shows the number of bugs with 2+ failing tests
and 2--6 edit locations in the minimized patch, grouped by patch coverage
under fault-revealing tests and the existence of positively/negatively test-identified
partial repairs at any granularity level.
The vast majority of analyzed multi-test, multi-location patches have at least
one positively identified partial repair.
Moreover, a $\chi^2$ test finds statistically significant relationships ($p < 0.001$)
between (non-)HOLDS patch coverage and the existence of positive and
negative partial repairs. Bugs with HOLDS coverage are less likely to have
positive partial repairs and more likely to have negative partial repairs.
Thus, HOLDS coverage correlates with an
\emph{adversarial} test-based fitness landscape.
Partial patches with holds coverage may be difficult for a test-based
fitness function to identify.

These connections between coverage and partial repair test passage
suggest that fault-revealing tests' patch coverage reflect the tests' ability
to detect components of a patch.
%text migrated from Discussion & Takeaways
Intuitively, if a patch can be
assembled from two components \textbf{A} and \textbf{B}, two tests that both
fully cover \textbf{A} and \textbf{B} are less effective at assessing partial
correctness than two tests that only cover \textbf{A} and \textbf{B} separately.
In the case of HOLDS coverage, the presence (or absence) of either \textbf{A} or 
\textbf{B} interferes with both tests; in the case of CONTRADICTS coverage, the presence of 
\textbf{A} or \textbf{B} affects only its covering test.
Coverage thus influences the accuracy of tests as a judge of partial correctness.

Of course, in actual APR we cannot classify the coverage into these three categories 
because we don't know the correct repair and its components. However, if we can infer the
likelihood of a bug belonging to the HOLDS category by the number of lines covered by all
failing tests (since HOLDS requires all failing tests to cover the entire repair), then we
may use that information to adjust tolerance of neutral or negative candidate patches, as
the correlation shows that HOLDS bugs are more likely to have correct partial repairs that
are neutral or negative in tests. \todo{YL: I don't know whether what I wrote in this paragraph
makes sense at all. I'm just trying to come up
with something that justifies this RQ. If it is not, we can either come up with a better justification
remove this entire Subsection}

\section{Limitations}
\label{sec:limits}

\noindent\textbf{Coverage.}
We use JaCoCo to collect coverage, but it has some
limitations. It cannot analyze coverage for inserted
cases for \texttt{switch} statements, \texttt{else} statements, method
signatures, and other code constructs that are compiled away during the
transformation to bytecode. Since we focus on how the coverage of different
failing tests compared to each other, we posit that JaCoCo's coverage was a
sufficient approximation.

\vspace{1ex}
\noindent\textbf{Dependency analysis.}
We analyzed dependencies intraprocedurally with some interprocedural 
heuristics, which make assumptions derived from common Java practices.
These assumptions are not always true. We use heuristics to add some
approximated interprocedural data while retaining interprocedural 
analysis's scalability to large, real world software.

\vspace{1ex}
\noindent\textbf{Code clones.}
We examined code clones at the location level.  However, clones also occur at the
granularity of lines~\cite{JiaClones} or
subexpressions~\cite{microclones}. We classified clones into four
categories (Same, Near-same, Composite, Move), but other categorizations 
are possible.
%
We did not distinguish between the number
of edit locations in a particular set of clones. It is possible that a set of
clones contains two or more locations with similar edits, which we do not examine. 

\vspace{1ex}
\noindent\textbf{Partial repair construction.}
We constructed partial repairs by breaking up multi-location patches
into single-location edits.
In reality, most APR techniques mutate code at a finer granularity, so
these edits may not be representative.  Additionally, we only observe if test suites
can identify partial repairs. Perhaps our evaluated partial repairs
are outside of the search space of a
particular APR technique.
%Regardless of this limitation, this experiment provides valuable 
%insight into the challenge of automatically constructing
%multi-location patches.

\section{Related Work}
\label{sec:related}

\todo{Does this include related work since ESEC/FSE?  Quickly check out this
  year's ICSE, if nothing else.}
\todo{Also, this is long, but we can do a cutting pass when we're focusing on length.}
Recent empirical studies on fault localization find that 
SBFL is more effective than various other techniques, such as 
mutation-based fault localization~\cite{pearson2017evaluating, mut-analysis}, program 
slicing, predicate switching,  information retrieval, and other techniques. 
However, a 
technique that outperforms all of these can be created using machine learning to combine 
multiple fault localization techniques, implying that different techniques can 
localize different kinds of faults~\cite{zou2019empirical}. Most empirical 
studies on fault localization evaluate techniques by determining whether a 
technique can localize any faulty line. This usefully evaluates single line or single 
location faults, but not necessarily multi-location faults.

Mutation testing contains parallels to program repair, as seen in G\&V
program repair~\cite{weimer2013leveraging} and fault 
localization techniques~\cite{metallaxis,muse,mbfl-survey}. 
There is recent work on higher order mutants, which is interested in finding 
certain combinations of mutants with particular behaviors. Higher order mutation testing 
shares similar difficulties with multi-location bugs, particularly due to 
exponential search space growth~\cite{long-search-spaces}. 
The current state of the art in finding specific classes of 
higher order mutants is using search based approaches; in particular, the best approach 
currently identified is a genetic search, which is reminiscent of program repair~\cite{homs, 
genprog}.

Qi et al.~\cite{patch-correctness} evaluated patches generated 
by three G\&V repair tools~\cite{genprog, ae, rsrepair}. 
The vast majority of generated patches were incorrect and equivalent to 
a single functionality deletion.  Patch \emph{overfitting} to the provided test
cases can be measured via the use of held-out test
suites~\cite{Smith15fse}, a technique that empirically validates the challenges of producing
high-quality repairs in response to a single test suite.   Later work found patch incorrectness to be 
also problematic in Defects4J~\cite{d4j-eval} and in semantics-based 
repair techniques~\cite{Le2018}.  We similarly find that test suites provide
imperfect semantic coverage over multi-location edits: many bugs can be fixed
with only a portion of a multi-location human patch.  We 
also find, however, that test cases can often identify partial patches. 

% I know I'm referring to the work by the authors' names,
% but I can't find a good way to refactor their names out without writing awkwardly.
% I would need to refactor all of the usages of "they" to remove their names.
Long and Rinard~\cite{long-search-spaces} studied
correct and incorrect plausible patches in the search spaces of SPR~\cite{spr} 
and Prophet~\cite{prophet}. They found incorrect plausible patches to outnumber 
correct patches by orders of magnitude. When they increased the search space 
by adding additional mutation operations, they found an increased number of 
correct patches, but APR tool performance might actually degrade due to a 
simultaneous increase in incorrect plausible patches and growth in search space.  
While we do not study this particular problem, 
we similarly seek to understand the 
massively larger search space of multi-location repair. 

A previous empirical study on real bug fixes~\cite{zhong2015} 
studied fault localization difficulty, bug fix complexity, necessary
mutations, relevance of API knowledge, and buggy file types
on more than 9000 real-world bugs.  
Both our paper and the previous study aim to provide useful guidance and insights for 
improving state-of-the-art APR techniques through empirical studies of bugs and bug fixes. 
Our study differs in that we focus on 
source code bugs patched by multiple edits in multiple locations. 
We draw insights in fault localization, fix mutations, and test-based 
patch evaluations. Additionally, we study the program repair 
benchmark datasets Defects4J 
and \bears; we believe that the program repair community would derive 
benefit from a study of APR benchmarks.

Wang et al.~\cite{wang2018} empirically studied multi-entity changes in bug patches 
(where each entity is a class, method or field). They queried
why and how often do patches have multi-entity changes, relationships 
between co-changed entities, and recurring patterns of multi-entity changes. 
By analyzing 2854 real-world bugs from four projects, they found that 66\%-76\% of
multi-entity fixes contain dependencies, 
and they identified three major recurring patterns that connects co-changed entities. 
They suggested a potential way to 
enhance APR by incorporating multi-entity changes. In contrast, we on bugs that
requires multiple edits to fix, where the edits may be in the same entity. In addition 
to reinforcing their earlier findings on intra-patch dependencies, we discover 
additional insights for enhancing APR in the axes of fault localization, 
fix mutations, and patch validation.

Prior work in program repair to use more search-guiding information 
during candidate patch evaluation 
include using program invariants~\cite{better-fitness, dinglyu}, 
intermediate program values~\cite{source-code-checkpoint}, 
and online mutation-based fault localization~\cite{mut-analysis}.
Some approaches require additional input, such as suspicious variables~\cite{source-code-checkpoint} 
or known patches for the bug under repair~\cite{better-fitness}, 
while others exhibit limited performance improvements~\cite{dinglyu, mut-analysis}.
We show the potential benefit of deriving more information from tests
with assertion level granularity.
Future work may explore more methods to mine information from tests 
beyond passage or failure.

Schulte et al.~\cite{schulte} found that 37\% of code mutations 
do not change test case passage and discussed potential applications 
of mutation robustness in proactive bug repair. 
Although our context differs --- they studied random mutations, 
we study changes associated with specific bug fixes --- we also find a 
non-trivial proportion of neutral edits in multi-location repairs.  


\section{Takeaways and Conclusions}
\label{sec:takeaways}

Our findings demonstrate that bugs repaired with multi-location patches have
fundamental differences from bugs repaired with single-location patches. These
differences should be considered for designers of future APR techniques that
generate multi-location bug fixes. These are our key takeaways:

\todo{I kind of think this formatting is ugly, but I get the motivation.  Do
  these takeaways include the data/experiments added for ICSE 2021?  Anyway,
  I'll reread at the end.}
\begin{itemize}[wide, labelindent=0pt]
\item \textbf{The \emph{core assumption} of spectrum-based fault
  localization does not always hold.}
The core assumption of spectrum-based fault localization---that faulty locations
are executed more often by negative test cases---was not true for a substantial
number of bugs in our study. In our study \todo{58\%} of bugs repaired with
multi-location patches and characterized by multiple failing tests had patch
locations that executed by some but not all tests. \todo{Additionally, \todo{27\%} of
these patches were disjoint (i.e., had no patch location covered by all failing
tests).}  In these cases SBFL is unlikely to be the most appropriate choice for a
fault localization technique. This finding suggests that there is still progress
to be made in fault localization for automated repair.

\item \textbf{Patches with dependencies are common and harder to construct.}
We found that 45\% of human-written multi-location patches contained
intra-patch control and data dependencies. These patches are less likely to be
generated by a broad range of repair techniques. 
% Semantic APR can generate dependent code, and can sometimes even 
% account for constraints imposed by dependencies.
% Whether semantic APR techniques are any better at practically generating 
% dependent patches, however, is a data analysis task for the journal extension paper.
This points to an opportunity to exploit dependencies by
existing or new techniques to improve APR for this large class of hard-to-repair
bugs.

\item\textbf{Code clones are prevalent in multi-location patches.}
We found code clones in \todo{32\%} of bugs with multi-location
patches. This observation supports recent work that leverages code clones to
generate repairs~\cite{saha2019harnessing}. \todo{Moreover, we found that there is a
correlation between negative tests with disjoint coverage and code clones in
human-written patches.} This suggests the existence of a heuristic for
identifying which bugs would benefit most from code clone-specific techniques.

\item\textbf{Correct partial repairs rarely cause more failing tests.}
Contrary to prior work~\cite{gecco09}, we found that test suites are infrequently
hostile to partial repairs. That is, partial application of a correct patch
usually does not increase the number of test case failures. This means that
generate and validate APR techniques can assemble correct patches from partial repairs.

\item\textbf{Patch coverage should be considered when determining the
  confidence of a test suite's assessment of partial correctness.}
Test cases are a more accurate metric of partial correctness when coverage
overlap of patch components is minimized.
We can reduce the possibility of overlapping coverage by decomposing tests into
smaller units. Potential methods to decompose tests include
refactoring~\cite{b-refactoring} or using a finer level of granularity for
measuring test suite success, such as the assertion level granularity proposed
in Section~\ref{sec:partial-repair-methodology}.

This also implies that class-level granularity validation has tradeoffs. Some
repair frameworks use class-level granularity for faster validation. This may
come at the cost of less accurate detection of partial patches. However, this
tradeoff may be worthwhile in Java, given the non-trivial challenges involved in
decomposing JUnit test classes into individual methods.

\item\textbf{Human patches are often not test-minimal.}
Many bugs do not need all patch locations to pass all tests,
offering further evidence of the incompleteness of test cases as a
proxy for correctness~\cite{patch-correctness} and the
presence of non-corrective changes (e.g., refactoring, enhancements)
in handwritten bug patches~\cite{api-refactoring, tangledchanges}.

\item\textbf{Techniques need to be evaluated on diverse benchmarks.}
Our two datasets, Defects4J and \bears, exhibited different characteristics.
These characteristics may explain why APR tools perform unevenly across
different benchmark datasets~\cite{durieux-repair-them-all}. Our findings
provide evidence reinforcing the call for the use of diverse benchmarks when
evaluating tools.
\end{itemize}

To date, most automated program repair techniques have been limited to bugs that
can be localized to and repaired at a single program location. However, we find
that 47\% of bugs in the Defects4J dataset and 61\% of bugs in the \bears dataset
were repaired by a human developer with multi-location patches. This motivated
our study on the possibility of extending current program repair work to
multi-location patches.

Our study's findings suggest deep implications for program repair targeting
multi-location patches, both in terms of the applicability of existing program
repair techniques and the design future repair techniques. \todo{To ensure
reproducibility and encourage further investigation, we intend to release a
replication package post-review.}
We hope that our findings and insights inspire future work to advance 
the state of the art of APR and take on this large, difficult class of 
multi-location bugs.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
\endinput
