
\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{fancybox}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[export]{adjustbox}
\usepackage{xspace}
\usepackage{siunitx}

\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{dkred}{rgb}{0.5,0,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{cyan(process)}{rgb}{0.0, 0.72, 0.92}
\definecolor{safetyorange}{rgb}{1.0, 0.4, 0.0}
\definecolor{javagreen}{rgb}{0.25,0.5,0.35}
\definecolor{metalgrey}{rgb}{0.43, 0.5, 0.5}
 % comments
%basicstyle=\ttfamily\bfseries\footnotesize,


\lstdefinestyle{JavaStyle}{
    language=Java,      % choose the language of the code
    keywords=[2]{View, LayoutInflater, ViewGroup, Bundle, ListView, Fragment, Activity},
    keywords=[3]{onCreateView, inflate, getActivity, DLE, findViewById, setAdapter},
    keywords=[4]{@Override, listView, DirList},
    basicstyle=\ttfamily\bfseries,
    keywordstyle=\color[RGB]{69,97,189},
    keywordstyle=[2]{\color{cyan(process)}},
    keywordstyle=[3]\color{javagreen},
    keywordstyle=[4]\color{metalgrey},
    %safetyorange
    commentstyle=\itshape\color{green!60!black},
    moredelim=[l][\itshape\color{gray}]{//}, %<--- overrides line-comment style
    stringstyle=\color[RGB]{192,8,8},
    numberstyle=\itshape\color{yellow!50!black},
%   backgroundcolor=\color{lbcolor},
    tabsize=4,
%   rulecolor=,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=false,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    numbers=left,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    %autodedent,%<--- removes indentation
}
\lstdefinestyle{examplestyle}{
    language=C,      % choose the language of the code
    basicstyle=\ttfamily\footnotesize\bfseries,
    tabsize=4,
    aboveskip={.25\baselineskip},
    belowskip={.25\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=false,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=none,
    numbers=none,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    %autodedent,%<--- removes indentation
}

\lstset{style=JavaStyle}

\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand\todo[1]{\textcolor{red}{#1}}
\newcommand\bears{\textsc{Bears}\xspace}


\begin{document}

\title{A Study of Multi-Location Bug Patches}

\author{Leo~Chen, Zhen~Yu~Ding, Yiwei~Lyu, Jeremy~Lacomis, and~Claire~Le~Goues}

% template authors, affiliations, footnotes
% \author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
%         John~Doe,~\IEEEmembership{Fellow,~OSA,}
%         and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332.\protect\\
% % note need leading \protect in front of \\ to get a newline within \thanks as
% % \\ is fragile and will error, could use \hfil\break instead.
% E-mail: see http://www.michaelshell.org/contact.html
% \IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}


% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


\IEEEtitleabstractindextext{%
\begin{abstract}
Automatic program repair is a promising approach for reducing the
    cost of quality assurance practices and faulty software. To date, most
    techniques proposed for test-driven automatic repair succeed
    primarily on bugs that benefit from short, single-location patches. Techniques
    that successfully generate multi-location patches often do so in an
    alternative, single-edit way, or by targeting particular multi-location bug
    patterns. Empirical studies of real-world bugs similarly tend to focus on the
    patterns exhibited by single-location bug patches, and have not examined repairability
    of multi-location patches in detail. We present a comprehensive empirical analysis
    of multi-location patches for bugs in open source Java programs, focusing on static and
    dynamic properties that define the repair search space for a given bug.
    This analysis focuses on key challenges of the dynamic program repair
    problem: the \emph{mutations and fix code} used to repair bugs in multiple locations;
    the \emph{fault locations} and their relationships; and the \emph{evaluation},
    and in particular how and to what degree tests can be used
    (or not) to identify partial repairs. We identify key takeaways and
    challenges, with implications for future work in expressive, multi-location bug
    repair.
\end{abstract}

\begin{IEEEkeywords}
  software bugs, program repair
\end{IEEEkeywords}}


\maketitle


\newcommand{\rqorinsight}[2]{
  \setlength{\fboxsep}{0.8em}
  \vspace{-0.5em}
  \begin{center}
  \Ovalbox{\begin{minipage}{0.95\linewidth}
    \textbf{RQ#1:} #2
    \end{minipage}}
  \end{center}
  \vspace{0.5em}
}

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

\IEEEPARstart{B}{uggy} software has a significant economic cost~\cite{cambridge-study}, and
software failures are estimated to have affected half of the world's
population~\cite{tricentis}.  This motivates research in
techniques to automatically find and fix bugs, some of which
began to be integrated into
real-world development practices~\cite{iceland,sapfix,getafix}.

A significant class of program repair techniques in both
research~\cite{genprog,angelix,Le17,Xuan17,arja-journal,prapr,saha2019harnessing,astor}
and practice~\cite{sapfix} use test
cases to guide patch construction. At a high level, these techniques use test cases
to localize a defect --- identified by at least one failing test --- and to
validate which (if any) of the generated patch candidates lead
the program to pass all tests.
Patches are constructed in a variety of ways, ranging from heuristic, syntactic
program manipulation~\cite{par,genprog,rsrepair,ae,prophet,hdrepair,arja-journal,confix,saha2019harnessing},
to specially adapted program synthesis 
techniques~\cite{Konighofer11,Konighofer12,semfix,DeMarco14,angelix,sosrepair,Xuan17}. 
These techniques have successfully
repaired meaningful defects in complex
programs~\cite{angelix,genprog-eight-dollars,sapfix,durieux-repair-them-all}.

Practically speaking, though, these techniques are typically limited in the types and
complexity of defects they can repair. Often this is by design: techniques may
limit the repair search space to single-location patches for
tractability~\cite{rsrepair,ae,hdrepair,capgen}, while others only target certain
classes of bugs~\cite{Xuan17,sapfix,DeMarco14,par}. However, even techniques
that can in principle generate repairs with multi-location patches typically
do not~\cite{patch-correctness}.
%
Meanwhile, many real-world bugs require multi-location 
patches~\cite{d4j-dissection,zhong2015}, leaving a large
proportion of them unrepairable by modern
research techniques in program repair.  
%Over half of the bugs in the popular bug
%benchmark Defects4J are patched by humans using multi-part
%patches~\cite{d4j-dissection}. Approximately 70\% of buggy source files in a
%large study of bugs in Apache projects required edits at two or more
%locations~\cite{zhong2015}.

This is a difficult problem.
A key tension in the design of an automated repair technique is the balance
between giving users confidence in patch correctness by maximizing its
subjective quality while managing a trivially-infinite search space. This space
is typically parameterized along several axes: (1) the \emph{fault space}:
potential program locations to be modified, (2) the \emph{mutation space}: which
modifications may be applied at a location, and (3) the \emph{fix space}: code
that may be instantiated for a specific mutation. For example, a repair
technique might identify the location of a null-pointer dereference (exploring
the fault space), decide to insert new code (mutation space), and synthesize
code to initialize the pointer (fix space). 

Multi-location repair poses distinct challenges for each of these repair axes.
Spectrum-based fault localization~\cite{ochiai}, the most prevalent class of
fault localization used in program repair, does not specifically
identify sets of potentially-related locations; indeed,
the evaluation of most fault localization techniques typically assumes that a bug
is localized if any one buggy line is identified~\cite{fl-survey-wong}.
Some bugs \emph{are}
repaired in multiple locations using very similar
code~\cite{saha2019harnessing,jiang2019cmsuggester}, informing novel techniques
that constrain the \emph{fix space} of possible multi-location repairs accordingly.
But, how prevalent are these bugs, 
relative to those that require multiple
coordinating edits?
Note for example that over 50\% of the fixes in four 
Apache projects involve two or more  Java classes, methods, or fields, and  66\%-76\% of 
those multi-entity fixes involved syntactic dependencies~\cite{wang2018}. 
Test cases are used to evaluate candidate repairs in dynamic
repair techniques, but may not be effective
at identifying partial repairs in a multi-location
context~\cite{better-fitness}.  
Although multi-location repair has been discussed in the context of other analyses
that study bug fix characteristics in general~\cite{d4j-dissection} as well as for
repair applicability specifically~\cite{zhong2015, wang2018}, 
% Do Motwani et al study this?
to our
knowledge there has been no previous significant study of the characteristics of
multi-location repairs and their implications for automatic
repair.

To lay groundwork for more informed, systematic research
efforts in the space of multi-edit repair, we conduct a systematic study of real-world bugs with
multi-location patches.  We study bugs curated in two
real-world datasets that support program repair research: Defects4J~\cite{defects4j}
and \bears~\cite{bears}, in total, 1018 bugs in 51 projects.
More than half of these bugs were repaired by a
human developer using edits at multiple locations.  We look at characteristics along each of the
relevant axes of the program repair search problem: fault locations, mutation
operators, fix code, and evaluation (or fitness or objective) using test cases,
and find both important and, in some cases, unintuitive, implications for
APR.  Our contributions are:

\begin{itemize}
%\item An analysis and enumeration of multi-location patches in Defects4J and
%\bears.
% I think the above is addressed by the summary in the preceding paragraph.
\item \textbf{Fault Localization.}  We find that 64\% of multi-location bugs have faulty 
locations that are not
  covered by all failing tests, violating a key assumption underlying
  spectrum-based fault localization. This potentially 
  motivates the development of novel localization techniques. 
  In addition, multi-location bugs whose failing tests cover the same faulty locations tend to 
  have failing tests that cover the same lines of code in general, a correlation
  that may inform the selection of fault localization for unknown bugs. 
\item \textbf{Edit dependency.} We 
find that 45\% of bug patches contain dependent edits, and that such bugs
may be more difficult for current, state-of-the-art
techniques to automatically repair.  This means that APR techniques assuming edit
independence~\cite{saha2019harnessing} are applicable to a nontrivial proportion of complex bugs, while
still motivating development of new analyses that can reason about multiple,
related edits at once. 
\item \textbf{Cloning in multi-location repair.} We find that close to 30\% of bugs with multi-location
patches have very similar edits applied to different locations, suggesting the viability of 
program repair techniques designed to utilize code clones. Moreover, bugs in which 
no faulty location is covered by every failing test tend to correlate with
clones in their patches.  
Clones are thus more likely to violate the assumptions of spectrum-based fault 
localization, but this also suggests a path forward for predicting 
which type of technique may apply to a given bug. 
\item \textbf{Test cases for multi-location patch validation.}
%% JL: I'm combining these into one number. 44% comes from (99+39)/(240+74)
%% We find that over 40\% of bugs with multi-location patches in Defects4J and
%% over half in \bears
  We find that 44\% of bugs with human-written multi-location patches do not
  require edits at every location to pass all test cases. This suggests that
  either patches contain superfluous edits or that test cases do not fully
  capture the desired specification. We also find that tests can be used to
  identify partial repairs, and the \emph{granularity} at which test suite
  behavior is measured (i.e., at the class, method, or assertion level)
  influences the ability to identify partial repairs. At the finest granularity
  (i.e., at the assertion level), test cases can identify 35\% of
  partially-applied patches as a positive improvement, while only 20\% of
  partial patches cause an increase in test failures. Moreover, we find 
  tests to measure partial correctness more accurately when patch coverage 
  overlap is minimized, suggesting the need to adjust confidence in tests
  in response to coverage.
\end{itemize}

Our code and data can be found at 
\url{https://github.com/squaresLab/MultiEdit_Experiments}.

In Section~\ref{sec:background}, we overview the problem domain to
motivate/outline our research questions, and characterize our datasets.
% We overview the key
%characteristics of the datasets that we use in our study in
%Section~\ref{sec:data-rq1}.  
We then describe the results of our study along the
three key axes of program repair: fault localization (Section~\ref{secFL}),
mutation operators and fix code (Section~\ref{sec:mutops}), and test cases as
the patch validation mechanism (Section~\ref{sec:tests}).  We conclude with
limitations (Section~\ref{sec:limits}), related work,
(Section~\ref{sec:related}), and a summary of
% don't give up on this yet.  It's a complicated paper, roadmaps might actually
% meaningfully help
our takeaways (Section~\ref{sec:takeaways}).

\section{Problem Definition}
\label{sec:background}


\begin{table*}
\begin{center}
\begin{tabular}{l  rrr | rr rr rr rr }
\toprule
\multicolumn{12}{c}{\textbf{Defects4J v2.0.0}} \\
\midrule
\multirow{2}{*}{Project} & \multirow{2}{*}{Bugs} & Source Size & Test Size & \multicolumn{2}{c}{Multi-Location} & \multicolumn{2}{c}{Multi-Location,}      & \multicolumn{2}{c}{2--6 Location} & \multicolumn{2}{c}{Multi-Line} \\
            &          &  (KLOC) & (KLOC)&
\multicolumn{2}{c}{Patches}              & \multicolumn{2}{c}{2+ Failed Tests}   & \multicolumn{2}{c}{Patches}             & \multicolumn{2}{c}{Patches} \\
\midrule
Apache Commons (8 projects) & 316 & 223.3 & 161.5 & 173 & 55\% & 69 & 22\% & 151 & 38\%
& 250 & 79\%\\
Closure Compiler & 174 & 150.6 & 112.6 & 99 & 57\% & 61 & 35\% & 80 & 46\% & 145
& 83\% \\
FasterXML (3 projects) & 144 & 116.1 & 55.5 & 89 & 62\% & 17 & 12\% & 69 & 40\% & 124 & 86\%\\
Gson & 18 & 16.8 & 12.8 & 7 & 39\% & 5 & 28\% & 6 & 33\% & 11 & 61\% \\
JFreeChart  & 26 & 193.3 & 74.6  & 11 & 42\% & 7 & 27\% & 10 & 38\% & 17 & 65\%\\
Joda-Time & 26 & 82.9 & 70.4 & 15 & 58\% & 7 & 27\% & 12 & 46\% & 22 & 85\% \\
Jsoup & 93 & 7.9 & 2.2 & 43 & 46\% & 22 & 24\% & 36 & 39\% & 60 & 65\% \\
Mockito & 38 & 23.0 & 28.5 & 18 & 47\% & 9 & 24\% & 14 & 37\% & 30 & 79\% \\
\midrule
All (Defects4J) & 835 & 813.9 & 518.1 & 455 & 54\% & 197 & 24\% & 378 & 45\% & 659 & 79\%\\
\midrule
\multicolumn{12}{c}{\textbf{\bears (single-module)}} \\
\midrule
FasterXML (jackson-databind) & 26 & 80.7 & 41.2 & 19 & 73\% & 3 & 12\% & 15 & 58\% & 23 & 88\% \\
INRIA Spoon & 62 & 66.2 & 30.8  & 40 & 65\% & 18 & 29\% & 24 & 39\% & 52 & 84\% \\
Spring Data Commons & 15 & 45.8 & 28.8  & 10 & 67\% & 3 & 20\% & 9 & 60\% & 14 & 93\% \\
Traccar & 42 & 47.9 & 8.6 & 24 & 57\% & 2 & 5\% & 24 & 57\% & 33 & 79\% \\
+30 other projects & 39 & --- & --- & 23 & 59\% & 9 & 23\% & 20 & 51\% & 29 & 74\% \\
\midrule
All (\bears) & 184 & $>$240.6 & $>$109.4 & 116 & 63\% & 35 & 19\% & 92 & 50\% & 151 & 82\% \\
\midrule
Combined (Defects4J \& \bears) & 1019 & $>$1054.5 & $>$627.5 & 571 & 56\% & 232 & 23\% & 470 & 46\% & 810 & 80\%\\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:dataset-characteristics} \small Summary of Defects4J and \bears
  datasets. \emph{Project} is the project name; \emph{Source Size} and
  \emph{Test Size} are the size of the source code and test suite in thousands
  of lines of code respectively. The right-hand columns categorize the bugs by
  the characteristics of their human-written patches (multi-location, 2--6
  location, and multi-line). The \emph{2+ Failed Tests} column only includes
  bugs that fail more than one test. Note that more than half of bugs are
  multi-location (571/1018).  }
\end{table*}

In this section we overview the background of automated program repair (APR) and
introduce key concepts that directly motivate our research questions
(Section~\ref{sec:apr}), and define precisely what we mean by multi-location
repair as well as the datasets we consider in our study (Section~\ref{sec:bugs})

\subsection{Automated Program Repair: a Search Problem}
\label{sec:apr}

The central goal of source-level automated program repair (APR) is to
automatically generate patches for bugs in programs. We restrict attention to
\emph{dynamic} or \emph{test-case guided} program repair, a prevalent class of
research techniques~\cite{cacm19}.  Dynamic APR techniques
are characterized by their use of test cases as a correctness oracle,
at least one of which is failing (i.e., exercises the bug
to be repaired).

\looseness-1
At a high level, source level program repair is a \emph{search problem}, where
the objective is a set of source-level program edits that will cause it to pass
all of the provided tests. To do this, techniques first \emph{generate} a candidate
patch, then run the test suite to \emph{validate} it. Generating a patch
requires navigating a search space, often defined along the
following axes~\cite{ae,sqjo}:
\begin{enumerate}[wide]

\item \emph{Fault space.} First, where 
  in the code should a patch be applied? Most dynamic repair
  techniques begin by using test cases as input to a fault localization
  technique to identify (and typically score) suspicious code
  based on test execution. Most APR techniques
  use some variant of spectrum-based fault localization (SBFL)~\cite{ochiai}. 
  The results define the \emph{fault space}, or the
  candidate code locations considered for repair.

\item \emph{Mutation space.} After identifying a faulty program location, an APR
  technique must choose from the set of applicable modifications at the given
  point. Examples include statement-level mutations like statement \emph{append}
  or \emph{delete}~\cite{genprog-operators}; hand-crafted edit
  templates~\cite{par}; or custom condition
  replacement over \texttt{if} statements~\cite{Xuan17}. Although more mutations
  admit a wider variety of patches, the search space grows combinatorially as
  they are added~\cite{long-search-spaces}.

\item \emph{Fix space.} Certain mutations must be instantiated with generated or
  selected code before they can be applied. For example, if a technique elects
  to insert a null check, an object must be chosen to check for 
  \texttt{null} to instantiate the patch. This defines
  the \emph{fix space}. Some techniques
  constrain the fix space by taking advantage of the \emph{plastic surgery
    hypothesis}~\cite{plastic}, assuming a bug can be repaired by code
  available in other parts of the same program. Learning-based
  approaches use models of code or repairs to inform
  modifications~\cite{prophet}; synthesis-based approaches constrain a
  synthesis engine to a small vocabulary of fix ingredients, possibly informed
  by the code near the selected faulty location~\cite{angelix,s3}.

\item \emph {Validation.}
Candidate patches are typically
validated using the provided test cases. In techniques
that require one, test cases can also be used as the \emph{fitness
  function}~\cite{genprog}; although some have supplemented the test case
objective with partial correctness measures like program
invariants~\cite{dinglyu}, memory snapshots~\cite{source-code-checkpoint}, or
similarity to human edits~\cite{hdrepair}.
%% JL: This might be too early, since we haven't talked about multi-location yet.
%% In a multi-edit context, ideally,
%% multiple tests would be able to idnetify partial repairs that could be composed.
%% However, evidence suggests that this does not always
%% apply~\cite{better-fitness,schulte}.

\end{enumerate}

A practical implication of the size and complexity of this search space is
that APR techniques must engage in trade offs to render the problem tractable. 
Some techniques limit the \emph{fault space},
searching explicitly for single-edit patches~\cite{rsrepair,ae,hdrepair,capgen}.
All consider a constrained mutation space. 
Some implicitly limit the mutation or fix space by targeting only certain
classes of bugs~\cite{Xuan17,sapfix,DeMarco14,par}. Regardless, even techniques
that can in principle generate repairs with multi-location patches typically
do not~\cite{patch-correctness}.
%
% Meanwhile, many real-world bugs require multi-edit patches: over half of the
% bugs in the popular bug benchmark Defects4J~\cite{defects4j} are patched by
% humans using multi-part patches~\cite{d4j-dissection}. Approximately 70\% of
% buggy source files in a large study of bugs in Apache projects required edits at
% two or more locations~\cite{zhong2015}.
%
Our goal is to understand what makes the generation of larger
patches difficult, and identify properties of these bugs and their associated
patches that can be exploited by future APR tools.  
%We define this problem precisely and characterize our datasets next.

\subsection{Dataset}
\label{sec:bugs}

Our study requires a dataset of indicative, real-world,
multi-location defects.  We study 
Defects4J v2.0.0~\cite{defects4j} and \bears~\cite{bears}.  Table~\ref{tab:dataset-characteristics}
summarizes these datasets, both of which
consist of historical
bugs found in real world software projects. Defects4J contains 835 bugs from 
17 Java software projects, and is a popular dataset for evaluating 
program repair tools that target 
Java~\cite{durieux-repair-them-all,pearson2017evaluating,saha2017elixir,saha2019harnessing,xin2017leveraging,b2016learning}.
The dataset's patches are manually minimized to isolate the bug fix 
and exclude non-repair edits such as refactorings and feature additions.

With any dataset, however, there is a risk that tools may overfit
to the defects in question, and there is evidence that this situation applies to
program repair and Defects4J~\cite{durieux-repair-them-all}. 
We thus also study bugs from \bears~\cite{bears}, 
a set of Java bugs derived from failed Travis-CI builds of GitHub projects. 
\bears offers 251 bugs from 72 software projects, providing a greater diversity of 
projects compared to Defects4J. 
Several projects in \bears, however, are structured as multi-module projects, a feature 
supported by Maven that allows developers to group multiple projects together. Multi-module 
projects are more complex to build and are not currently compatible with our automation tools.
We thus limit our analysis of \bears to 184 bugs from 34 single-module projects.

\subsubsection{Multi-Location Patches}

Not all the bugs in Defects4J and Bears have multi-location patches. 
Therefore, we need to identify which bugs in these datasets should be included in our experiments.
Since our experiments are isolating different parts of the repair process and are measuring different things, 
each experiment has slightly different criteria for the bugs that are included.
The goal of this section is to give our definition of a multi-location bug, and to give a high level overview 
of how each experiment applied further constraints to that definition.

There are several plausible definitions of multi- vs. single-location patches, with
implications for how they are studied. For the purpose of this study, we define
a \emph{patch location} as a contiguous sequence of edited lines of code.  We
combine two locations if there is code at the two locations that construct a syntactic 
block. That is, conceptually:
\lstset{escapeinside={<@}{@>}}
\begin{lstlisting}[style=examplestyle]
<@\textcolor{dkgreen}{+ Some\_edit \{}@>
<@\textcolor{dkgreen}{+ New code}@>
    Existing code
<@\textcolor{dkgreen}{+ \}}@>
<@\textcolor{dkgreen}{+ More new code}@>
\end{lstlisting}
is treated as a single-location edit in our study. Locations consisting only of changes to
comments, whitespace, or \texttt{import} statements were not counted.
We still consider a patch location valid if a contiguous sequence of edited lines contains 
only refactorings, variable/field declarations, or annotations. We do not perform any 
additional minimization or changes to the program beyond those the dataset authors may 
have performed.
This intuitive definition is reasonably consistent with the general APR
paradigm. 
%As a shorthand, we refer to bugs with multi-location human-written
%patches as \emph{multi-location bugs}.

Below, we list further constraints based on the needs of each experiment:

\begin{itemize}
  \item For fault localization experiments (Section \ref{secFL}), we include bugs based on the above definition, 
  with the exception of one of the experiments which adds a constraint of bug with two or more failing test cases.
  \item For mutation operators and fix code (Section \ref{sec:mutops}), we have two experiments with two different data sets. 
  The first, looking into the presence of dependencies in patches, uses all multi-location patches 
  (though we report results on dependencies between patch lines rather than dependencies between locations). 
  The second experiment, looking into the presence of code clones, uses the definition we provided previously, 
  but adds the constraint of using bugs with 6 or fewer locations for scalability reasons.
  \item For test case based validation (Section \ref{sec:tests}), we further constrained the definition of a 
  multi-location bug to ignore changes that introduced new helper methods or variable declarations to reduce 
  syntactically invalid location combinations.
  After applying this rule (which may reduce the number of patch locations for a given patch, 
  if one of the locations consists entirely of helper methods or variable declarations), we included all bugs with 2-6 patch locations
\end{itemize}


We start our analysis of multi-location patches by asking:

\rqorinsight{1}{How often do humans write multi-location patches?}

Table~\ref{tab:dataset-characteristics} lists the numbers and percentages of
multi-location patches in Defects4J and \bears. 
We find that multi-location patches comprise 63\% of \bears and 54\% of 
Defects4J.
%\footnote{There are 11 bugs that should be considered multi-location but were left 
%out of all analyses. We were unable to correct this before the deadline but will add them 
%post review. This version of the paper treats these bugs as single-location.}.
% CLG: I don't think this is a critical detail, and fails to honestly advocate
% for our work.  All it does is annoy reviewers without adding much...we can fix it in post. 
Although a multi-location human patch for a bug does not imply the 
non-existence of a simpler patch, the high proportion of bugs that have 
multi-location patches demonstrates the relevance of such bugs to fault localization and
program repair. 

\bears contains a greater proportion of 
multi-location patches compared to Defects4J. This may be the 
result of manual patch minimization in Defects4J
and lack thereof in \bears.
Thus, some \bears patches may be multi-location due to
non-corrective edits (e.g.: refactorings, feature additions).

Having established both that multi-location edits are common in real-world
programs, and established a suitable, plausibly indicative dataset to study
them, we decompose dynamic APR into multiple subproblems, driven by the
shape of the search problem it seeks to solve. We 
investigate properties of the multi-location fault space (Section~\ref{secFL}),
mutation and fix spaces (Section~\ref{sec:mutops}), and validation
(Section~\ref{sec:tests}). 

\section{Fault localization} \label{secFL}

%% what are the claims, what are we studying, why are we studying it

%% old explanation
% Fault localization is a first key step in program repair, defining the
% \emph{fault space}. The majority of dynamic APR
% uses some variant of spectrum-based fault localization (SBFL) (e.g.,Tarantula~\cite{tarantula},
% Ochiai~\cite{ochiai} and DStar~\cite{wong2013dstar}, among others) for this purpose.
% SBFL techniques compute a suspiciousness score by
% measuring how often a line is executed by failing tests as compared to passing
% tests, typically via some (linear) combination of their respective
% coverage of a line. 


%, and is often more effective than alternatives (like mutation-based fault 
%localization~\cite{mut-analysis} or dynamic program
%slicing~\cite{zou2019empirical}). 
Fault localization is a first key step in program repair, defining the
\emph{fault space}. The majority of dynamic APR
uses some variant of \emph{spectrum-based fault localization (SBFL)} (e.g.,Tarantula~\cite{tarantula},
Ochiai~\cite{ochiai} and DStar~\cite{wong2013dstar}, among others) for this purpose.
%
SBFL is based off the intuition that a line that is executed more often in failing tests than passing tests
are more likely to be faulty.
Thus, these techniques compute a suspiciousness score for each line in the program by
measuring how often a line is executed by failing tests as compared to passing
tests, typically via some (linear) combination of their respective
coverage of a line.
APR techniques typically use the lines of the program with the highest suspiciousness scores
as a starting point for their patching techniques.
%SBFL techniques include but are not limited to Tarantula~\cite{tarantula},
%Ochiai~\cite{ochiai} and DStar~\cite{wong2013dstar}; in general, though more
%recent approaches are typically more effective, such approaches tend to localize similar
%faults~\cite{zou2019empirical}.

Since SBFL is most commonly used in APR, this section will focus on the applicability of SBFL to multi-location repair.
We want to study two aspects of SBFL in relation to multi-location bugs. 

%% failing test ---------------------------------------------

The first aspect concerns the bugs with multiple failing tests.
If the bug has only one faulty location and multiple failing tests,
we can take for granted that multiple failing tests will 
directly execute the the one faulty location or eventual patched code. 
However, with multiple faulty locations and multiple failing tests,
it is possible that the different failing tests will execute different portions of the faulty or patched code. 
One straightforward scenario where this occurs could be situations where the same fix is copied 
across the codebase and multiple independent tests are written for each instance of the fix.

There are a few implications of this. Although the case where multiple failing tests are executing
independent fault locations does not necessarily indicate that SBFL will be less effective
(since SBFL also takes into account code coverage of passing tests), it still may lead to misleading results;
for example, situations in which mutliple faulty locations are ranked similarly or lower than non-faulty code
executed by all failing tests. It can also lead to insights on new techniques for localizing multi-location 
bugs, such as the one previously used to identify regions of code that are syntactically similar to a 
previously identified faulty region of code~\cite{saha2019harnessing}.

Thus, we ask, 

\rqorinsight{2}{
How often are all the faulty locations in a multi-location bug executed by all the failing tests,
rather than a each failing test executing a different or independent subset of faulty locations?}

%%%%% First three can be replicated by the previous data
%%%%% Last one has a possible heuristic that probably gets it right most of the time, but may identify some patches where different faulty locations are run by different tests when in fact the faulty locations were all run by the same tests.
%%%%% Number of bugs where any faulty location is executed by any failing test 699
%%%%% Number of bugs where all faulty locations are executed by any combination of failing tests 327
%%%%% Number of bugs where all faulty locations are executed by all failing tests 305
%%%%% Number of bugs where faulty locations are executed by different sets of failing tests 394 # may also want to check if we want to exclude the case where a location is not executed by any failing test
%%%%% Total number of bugs 952



% - separate from faulty locations -- the general case, do the failing tests cover the same code? (I already did this, for the most part)
% - faulty locations -- are they executed by separate tests?
% - Dig into the multiple failing locations executed independently -- there's a possibility that this might mislead APR
% - SBFL on each failing test individually -- does this get significantly different results than doing them together?


The second aspect that we study concerns the ability of SBFL to localize some or all faulty locations, and whether this impacts APR performance.
New SBFL techniques are often evaluated by asking: how often does a given technique assign a high 
score for a buggy line of code? This is straightforward for single-location repairs, but for 
multi-location repairs, evaluations
typically consider a technique ``successful'' if it highly ranks or scores \emph{any} line in a
set of buggy lines~\cite{zou2019empirical,pearson2017evaluating,golagha2020can}.
While appropriate for the question being asked in such evaluations, this does not address
suitability for multi-location program repair. Identifying one of several buggy
locations is inadequate in a context where multiple locations must be modified -- 
either we need be able to identify all the faulty locations through SBFL
or infer where other faulty locations must be based on the location of one.

Accordingly, we want to look more broadly at how SBFL interacts with 
multi-location bugs.
Previous work has introduced three debugging scenarios specific to localizing faults with multiple 
locations~\cite{pearson2017evaluating}, which I will enumerate here 
and slightly rephrase to match our location-level framing and SBFL focus:

\begin{enumerate}
  \item "Best case": only one faulty location needs to be localized with SBFL to fix the bug.
  \item "Worst case": all faulty locations need to be localized with SBFL to fix the bug.
  \item "Average case": half of the faulty locations need to be localized with SBFL to fix the bug.
\end{enumerate}

In that previous work, these debugging scenarios were used to evaluate fault localization techniques, but it is unclear
whether they represent different uses in the context of program repair.
(List APR papers where we know what kind of debugging scenario is used in their FL).
Most APR assumes the best case (verify the truth of this statement), and try to generate a patch, potentially with multiple locations, based off of one faulty location at a time.

From here, we ask two questions: 

\rqorinsight{3}{How often are bugs localized as worst or average case compared to the best case? }

and 

\rqorinsight{4}{Are bugs that are localizable in the worst case more likely to be repaired by APR tools?}

%% some potential question framing brainstorming -------

% - Since, (I presume) most APR techniques are using the best case debugging scenario, how many bugs that were
% localizable in a best case, worst case, and average case scenarios were able to be repaired?
% Note: Worst case $\subseteq$ Average case $\subseteq$ Best case.
% - What APR techniques even account for localization that is not best case?
% - We want to see if SBFL effectiveness when measured on a single-location (of many) vs. all the 
% locations corresponds to higher likelihood of repairs
% - We want to compare the SBFL effectiveness to single-location bugs -- are multi-location bugs 
% harder to localize?

% There has already been some work on this -- the paper on predicting the quality of SBFL showed there was no 
% change in effectiveness

%% end brainstorming -----




\subsection{Methodology}

\looseness-1
Throughout these experiments, we used JaCoCo~\cite{jacoco}
to determine which lines of code in both the buggy and patched versions were executed
by each failing test.
% Repair tools generally use Gzoltar~\cite{gzoltar} to perform SBFL. 
% Gzoltar uses JaCoCo to measure the spectra, so our usage of JaCoCo 

RQ2 concerns the behavior of different failing tests, so we narrow our dataset to bugs
with at least two failing tests. 
Between both datasets, there are 232 total bugs that both require multi-location
patches and contain multiple failing tests. 

We look for two kinds of patterns, corresponding to two extreme (and non-exhaustive) ways failing tests can behave.
The first is the case where all failing tests execute the same faulty locations, 
which we expect would work well with SBFL. That is:

\begin{equation}
\begin{aligned}
    \text{Same}(T_f) :=& \\
    \forall{t_{f_1}, t_{f_2}} &\in T_f : \text{exec}_f(t_{f_1}) = \text{exec}_f(t_{f_2})
    \end{aligned}
\end{equation}


The second is the case where the tests are executing completely 
separate parts of the faulty code, which we expect could benefit. 
That is, for the set of failing tests $T_f$ and function $\text{exec}_f$ 
that gives a set of faulty lines executed by that failing test: 

\begin{equation}
\begin{aligned}
    \text{Independent}&(T_f) := \neg \text{Same}(T_f) \wedge \\
    \forall{t_{f_1}, t_{f_2}} &\in T_f : \text{exec}_f(t_{f_1}) \ne \text{exec}_f(t_{f_2}) \implies \\
    &\text{exec}_f(t_{f_1}) \cap\text{exec}_f(t_{f_2}) = \emptyset 
\end{aligned}
\end{equation}

We also see if these patterns extend to non-faulty code -- that is, if failing tests are executing 
separate faulty locations, do they also execute separate parts of the broader codebase?
Are failing tests that execute all the same parts of the codebase also executing the same code generally?
This has implications for localizing bugs which may follow the independent or same patterns -- 
if an APR technique observes that all the failing tests are executing different parts of the codebase, 
and we know that this statistically means that the fault will follow the ``independent'' pattern, 
the APR technique can switch the localization and patching methods it applies.
To measure this, we use the formulas as above, but instead of using the function $\text{exec}_f$ to get the set of executed faulty locations,
we use $\text{exec}_a$ to get the set of all executed lines. % explain better, and also explain how we are correlating to the other results.

To answer RQ3, we performed SBFL on all multi-location bugs. Specifically, we use JaCoCo to generate the coverage information
and calculate the suspiciousness score for each line using the Ochiai formula \todo{(cite Ochiai)}.
From the ranked list of lines, we determined how
many locations were localized. Since a fault location can consist of multiple consecutive lines, we consider a 
fault location to be localized if any of the faulty lines that comprise the location are highly ranked with SBFL.
We also consider all lines that tied with the lowest rank line, as different implementations of SBFL may 
order these tied lines differently.

To calculate the best, worst, and average case, we take the top 100 most suspicious lines (including ties)
and calculate what proportion of faulty locations are covered within these lines, best meaning at least one location,
worst meaning all locations, and average meaning at least half the locations. Some locations may be unlocalizable
(e.x., a patch may insert a new method, but since the line in which it was inserted was originally whitespace, JaCoCo cannot 
identify that particular line as being executed by any tests). We make no adjustment for this.

For these previous two methods, we use JaCoCo to generate the execution coverage of the failing tests. 
However, for certain bugs in \bears{}, JaCoCo was not able to generate coverage. 
In particular, several bugs in Bears were unable to be executed due to various engineering or reproduction difficulties.
In addition several bugs were able to be executed 
in previous environments but not current ones. 
We had previously gathered execution coverage some of those bugs for different experiment, but stored the results in
a highly abstracted manner that resulted in information loss; nevertheless, we extrapolate from those data where possible, particularly in
the first research question.
Because of the information loss, some of the results for the first RQ may be unsound; 
there may be a slight overestimate on the number of bugs considered ``independent.''
However, this is expected to be a very small number.
Table \ref{tab:runnable_jacoco} shows the number of bugs used in each case, and the paper artifact includes the full list of bugs that were included or excluded.

\begin{table*}
\begin{center}
\begin{tabular}{l | rr rr rr rr }
\toprule
\multirow{2}{*}{Project}  & \multicolumn{2}{c}{Multi-Location}      
& \multicolumn{2}{c}{Executable} 
& \multicolumn{2}{c}{Previously Executable}
& \multicolumn{2}{c}{Unexecutable} \\
& \multicolumn{2}{c}{(2+ Failed Tests)}   
& \multicolumn{2}{c}{October 2021}             
& \multicolumn{2}{c}{Feb 2021}  
& \multicolumn{2}{c}{with Jacoco}
\\
\midrule
Defects4J & 455 & (197) & 455 & (197) & 0 & (0) & 0 & (0) \\
\midrule
\bears (single-module) & 116 & (35) &  67 & (14) & 18 & (18) & 31 & (3)\\
\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:runnable_jacoco} \small The proportion of bugs we were able to reproduce and get results using Jacoco. 
\todo{Will work on formatting and column headers, also note the non-parenthesized numbers are counting out of all bugs and the parens means counting out of bugs with 2+ failing tests.}}
\end{table*}

For the RQ4, in which we investigate whether bugs localizable in the worst case are more likely to be repaired, 
we compare the results from RQ3 with the results from RepairThemAll~\cite{durieux-repair-them-all},
an experiment 
running 11 APR tools on 5 benchmarks, including Defects4J v1 and \bears.
Since the bugs introduced in Defects4J v2 were not included in the original experiment,
we also extended the code provided from the RepairThemAll experiment to obtain results for the new bugs.
We were able to obtain results for most of the bugs aside from bugs in Jsoup 
(due to a documented issue~\footnote{\url{https://github.com/rjust/defects4j/issues/353}}, encompassing 93 bugs) 
and, in most repair techniques, JacksonDatabind (due to engineering challenges and issues with timeouts, encompassing 112 bugs).



\begin{figure}
  \includegraphics[width=\linewidth]{img/coverage_faulty.pdf}
  \caption{\small How the failing tests of different patches behave with respect to the faulty locations. With the vast majority of bugs, the failing tests execute the same faulty locations.}
  \label{fig:coverage-faulty}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{img/coverage-all.pdf}
  \caption{\small How the failing tests of different patches behave with respect to all lines of code in the program. We expect that if the failing tests execute the same faulty locations, they will also execute the same lines of code, faulty or not. Here we see that this largely holds.}
  \label{fig:coverage-all}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{img/debugging_scenarios.pdf}
  \caption{\small Percentage of bugs that SBFL can localize given each debugging scenario. \todo{Need to add avg case, also there might be a bug in calculating worst/best case, I'll get back to this.}}
  \label{fig:debug_scenarios}
\end{figure}

\subsection{Results}
 \label{sec:cov_patterns}
 
 \subsubsection{Multiple failing tests.}
 Figure \ref{fig:coverage-faulty} shows the distribution of bugs in our dataset that have 
 failing tests executing the same fault locations, independent fault locations, or follow neither
 of these patterns.
 We can see that the vast majority of bugs have failing tests executing the same fault locations.

 
 We also investigated whether failing tests execute the same or completely independent lines of code (faulty or not).
 Figure \ref{fig:coverage-all} shows the results, divided by whether the failing tests had executed the same or independent fault locations.
 We can see that many of the bugs whose failing tests execute the same faulty locations also 
 execute the same faulty and non-faulty lines of code.
 However, there is much more variability in bugs where the failing tests execute different fault locations.
 
 \subsubsection{SBFL debugging scenarios.}
 Figure \ref{fig:debug_scenarios} shows the results for the number of bugs localized under each debugging scenario.
 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth,angle=270]{img/fl-outline.jpg}
%     \caption{}
%     \label{fig:outline}
% \end{figure}

\section{Mutation Operators and Fix Code}
\label{sec:mutops}

APR techniques vary in the types of
mutation operators they consider, how they select between them, and how they
select new fix code to instantiate them. 
%  For example, a na{\"i}ve
% approach with only \texttt{insert}, \texttt{replace}, and \texttt{delete}
% operators must choose between them at a location and, in the case of
% \texttt{insert} and \texttt{replace}, choose code to insert/replace at that
% location.  
%
%The few techniques that handle or at least enable multi-edit patches vary in their
%handling of mutation operator selection and instantiation.  
At one extreme, semantics-based repair~\cite{s3,angelix} can represent dependent edits between multiple
locations via as a conjunction of multiple simultaneous constraints; at the other, 
search-based or
evolutionary techniques~\cite{genprog,par} typically represent mutation
operators independently. 
% That is, a modification in one location does not
%inform the selection of a modifications to apply in a second location; instead,
%the heuristic search is trusted to 
However, the search space increases combinatorially~\cite{ae,long-search-spaces}; accordingly, 
heuristic multi-location
techniques~\cite{saha2019harnessing} make assumptions about the 
shape of the search space, e.g., targeting bugs that can be repaired by multiple similar edits.

Note that the types of edits generally included in human patches has been
studied~\cite{zhong2015,soto}.  Here, we are especially
concerned about the 
\emph{relationships} between multiple edits, as such relationships have
key implications for how edits should be designed, selected, and instantiated 
in multi-location repair. 

\subsection{Dependencies}

One relationship between edits is
\emph{dependency}. Selecting and applying edits can
and should be informed by any dependencies between them.  Specifically, we ask:
\rqorinsight{4}{How prevalent and hard to construct are
patches with intra-patch dependencies?}


\subsubsection{Methodology}
We consider a patch to contain \emph{dependent edits} if there exist 
control or data dependencies between added, deleted, or modified statements.
Note that this is different than dependencies within locations, 
as a patch location may contain multiple statements.
%Why line as opposed to location? is it just engineering difficulty?

- finer grained analysis on dependencies

% This expanded dataset contains 659 Defects4J v2 and 151 \bears 
% multi-line patches.
% We explicitly got rid of Table 1 stats on multi-line 
% patches for ESEC/FSE. We did so since we didn't wish to divert attention 
% from multi-location patches during our discussion of the datasets in Sec 2.
% Dependency analysis is the only experiment that analyzes patches at the
% edit granularity of lines, rather than locations.
For practical performance and scalability reasons, 
we perform intraprocedural analysis. We added two heuristics for
interprocedural/field sensitive
information, namely assuming (1) that any invocations of a getter
method  \texttt{Class.getX()} reads \texttt{Class.X}, and (2) any invocation of 
 \texttt{Class.setX()} writes to  \texttt{Class.X}.
%
% We treat all method call statements as dependent on all variables
%passed as arguments. 
% CLG thinks is is just correct treatment of variables passed to method calls
While unsound, these heuristics derive from common Java
practices.
%We use these heuristics to estimate interprocedural 
%dataflow while retaining the scalability of intraprocedural analysis.
%We provide our dependency analyzer in our replication package.

We estimate the difficulty of auto-constructing edit-dependent patches 
by examining whether APR techniques have successfully patched the
bug in question. We use APR success data from 
RepairThemAll~\cite{durieux-repair-them-all}, an experiment 
running 11 APR tools on 5 benchmarks, including Defects4J v1
(a subset of Defects4J v2) and \bears.

\subsubsection{Results}

\begin{table}
{\begin{center}
    \begin{tabular}{lrrrrrr}
        \toprule
        &\multicolumn{3}{c}{Without Dependent Edits} & \multicolumn{3}{c}{With Dependent Edits} \\
        Result& \multicolumn{1}{c}{Defects4J} & \multicolumn{1}{c}{\bears} & \multicolumn{1}{c}{Both} & \multicolumn{1}{c}{Defects4J} & \multicolumn{1}{c}{\bears} & \multicolumn{1}{c}{Both} \\
        \midrule
        Success & 96 & 7 & 103 & 43 & 9 &  52 \\
        Failure & 73 & 49 & 122 & 93 & 86 & 179\\
        \midrule
        Total  & 169 & 56 & 225 & 136 & 95 & 231\\
        \bottomrule
    \end{tabular}
 \end{center}
} \caption{\small APR performance on bugs with and without intra-patch 
      dependencies, from RepairThemAll~\cite{durieux-repair-them-all}.%\protect\footnotemark
      \ \emph{Success} means that at least one of 11 APR tools found a patch; 
      \emph{Failure} means that no tool did.  APR tools found repairs 46\%
      (103/225) of the time for bugs with dependency-free human patches, but only
      23\% (52/231) of the time when a patch contained dependencies.}
  \label{tab:dependency-repair-contingency-table}
\end{table}
%\footnotetext{RepairThemAll provides results for Defects4J v1.}

Table~\ref{tab:dependency-repair-contingency-table}
shows frequencies of multi-line patches with respect to edit dependence 
and whether an APR tool successfully repaired the bug.
Our data substantiates prior research~\cite{zhong2015}:
45\% of multi-line patches we study exhibit intra-patch dependencies. 
Importantly, 
we also find the presence of edit dependencies is correlated with lower rates of
APR tool success. 
Using a $\chi^2$ test, we find a statistically significant relationship ($p < 0.001$)
between APR success and intra-patch dependencies.
This result 
suggests that such dependencies indeed 
add complexity to the repair search that challenges the state-of-the-art.
Dependent edits, however, could
potentially narrow the search space by creating constraints between 
otherwise independent mutations. 


\subsection{Cloned code}
\label{sec:clones}

\subsubsection{Occurrences of cloned code}

A patch composed of multiple, dependent edits constitutes one
extreme. At the other extreme lies patches that apply the same modification in several locations. 
Previous techniques have targeted exactly these types of
bugs~\cite{wang2018,saha2019harnessing}.  We evaluate how often this relationship applies to
multi-location bugs, and thus the usefulness of techniques that assume it:

\rqorinsight{5}{How often do clones occur in multi-location patches?}

%Possible intuition: if we can find some kind of correlation between fault localization results
%and code clones, then if some APR research decided to follow Wang et al's suggestion and
%include "repeat same edits at multiple location" operator, then our results may advise
%the APR to be more likely to apply the repeat edit operator when the fault localization
%result matches specific patterns.

%\subsubsection{Methodology}
Our dataset includes all 470 multi-location bugs with 6 or fewer modified
locations (to reduce the analysis burden). We determine the existence of code 
clones through manual inspection.
%
% We cross validate our list of bugs with clones with results from previous work proposing and 
%evaluating an APR tool named Hercules~\cite{saha2019harnessing}. Hercules is a generate and 
%validate approach that could also exploit similar code locations to generate patches with code 
%clones. It was evaluated with the bugs in Defects4J version 1, which consists of 395 total bugs 
%and 187 multi-location bugs. 
%
%
We are studying code clones in \emph{patches}, such that conventional clone categories (Type I,II,III,IV)~\cite{convcodeclone} do not apply. We categorize clones in \emph{modifications} as follows:
\begin{enumerate}
\item Same: The modifications at two locations are alpha-equivalent (i.e., the two locations are identical or would be
identical, modulo alpha renaming). In the conventional code clone literature,
this corresponds to Type-I (identical) and a Type-II (renamed identifiers) clones~\cite{JiaClones}.
\item Near-same: The edits at two locations are almost alpha-equivalent and
  differ by at most one variable, constant or arithmetic operator via insertion, deletion or replacement.
\item Composite: The patch at one location is exactly copied and contained within the patch at 
the 
second location (the second location  has additional changed lines).
\item Move: One location is an insertion of 
code deleted from the other location. 
\end{enumerate}

%\subsubsection{Results}
\begin{table}
{\begin{center}
\begin{tabular} {lrrrrrr}
\toprule
&&\multicolumn{2}{c}{Dataset} &\multicolumn{3}{c}{Locality}\\
& Total & D4J & \bears & Method & Class & Neither\\
\midrule
Same      &  77 & 67  & 10 & 33 & 31 & 13 \\
Near-same   & 27  & 23  & 4 & 11 & 8  & 8  \\
Composite &  19 & 15  & 4 & 9  & 7  & 3  \\
Move      &  14 & 11  & 3 & 11  & 1  & 2  \\
\midrule
Any       & 133 & 106 & 21 \\
Total     & 470 & 378 & 92\\
\bottomrule
\end{tabular}
\end{center}
}
\caption{\small Code clone incidence, type, and location in our dataset. \emph{Same}, \emph{Near-same},
  \emph{Composite} and \emph{Move} are defined in Section~\ref{sec:clones}.
  \emph{Any} indicates any type of clone; note that some patches can
  contain clones from multiple categories. \emph{Locality} indicates
  the relative locations of clones in a patch (same method, class, or neither). 28.3\% of multi-location bugs
  in our dataset contain some form of cloning. }
\label{tab:clones}
\end{table}

Table~\ref{tab:clones} shows that 133 of 470 multi-location bugs
(28.3\%) included at least one type of cloning,
indicating a significant prevalence of code clones in multi-location human
patches.
%\footnote{Note that ``Any'' may not sum the previous rows because
%bugs can contain multiple pairs of code clones belonging to different
%categories.}
%
Moreover, close to half of code clones are in the same method, while less than
20\% of the cloning occurs across classes. Finally, 77 of 137 (56.2\%) of clones
fall in the ``same'' category, corresponding to alpha-equivalent code cloning.
These results suggest that 
most clones should be applied within the same class or method, and
in most cases, alpha-equivalent code clones will suffice.

% code clones <-> hercules data
% idk how what the conclusion from this information is so I'm just gonna put numbers in
% Hercules successfully repaired 15 multi-location bugs in their dataset. We found 56 
%multi-location bugs with clones. 14 of the bugs that Hercules successfully repaired were among 
%the bugs with code clones we found. The remaining bug that Hercules successfully repaired 
%was 
%a bug whose patch required 6 or more locations, and in going back, the human patch also 
%contained code clones. 
\subsubsection{Relationships between localization and clones}
Intuitively, we expect that \emph{contradicts} bugs, where no patch location is 
covered by all failing tests, are more likely to correspond to patches where
similar fix code is applied at multiple separately tested locations. If 
true, this suggests that fault localization information can help 
predict the utility of code clone-based APR mutation a priori. We ask:

\rqorinsight{6}{Does the existence of code clones in human patches correlate with specific 
localization patterns?}

% CLG sez: nice and clear, but derivable from first principles, and we have a
% length problem.
%By contrast, \emph{holds} bugs, where all failing tests execute 
%the same patch lines, may have
%more interrelated parts that are not merely the same statements copied to
%multiple locations.
%
We focus on the multi-test and multi-location bugs used in the coverage 
experiment (Section~\ref{secFL}), and exclude bugs with more than 6 faulty locations, leaving 177 bugs.

\begin{table}
  {\begin{center}
      \begin{tabular} {lrrrr}
        \toprule
        & Contradicts & Partially Holds & Holds & Total \\
        \midrule
        Clone & 24 & 13 & 12 &  49 \\
        No Clone  & 24 & 46 & 56 & 128 \\
        \midrule
        Total     & 48 & 59 & 68 & 177 \\
        \bottomrule
      \end{tabular}
    \end{center}
  }

  \caption{\small Multi-location and multi-test bugs classified by localization
    category (Section~\ref{secFL}) and patch clones. The category distribution
    is skewed toward \emph{contradicts} when patches contain code clones.}
  \label{tab:cov_clones}
\end{table}

Table~\ref{tab:cov_clones} shows the incidence of patch code clones among the three 
localization categories identified in Section \ref{secFL}. 
Out of 50 \emph{contradicts} bugs, 50\% (24/48) have clones; only 
20\% (12/59) \emph{partially holds} and 16\% (12/68)
\emph{holds} bugs contain clones.
A \emph{contradicts} bug is more likely to contain clones in its
human patch than bugs with other localization  ($p < 0.001$, by a $\chi^2$ test).
This suggests a heuristic for when to try a clone-based repair technique:
%if there appears to be less coverage overlap between multiple failing tests,
if multiple identified patch locations are covered by different failing tests,
it is possible that
applying the same edits at multiple locations~\cite{saha2019harnessing} will be
more likely to succeed. On the flip
side, however, the correlation of clone-containing patches and \emph{contradicts} bugs
suggests that SBFL may not be well-suited for techniques utilizing clones.
%53\% (26/49) of clone-containing patches exhibit \emph{contradicts}
%localization, compared to 17\% (24/142) of non-clones. This suggests that
%clone edits are more likely to contradict SBFL's assumptions.  
This motivates novel fault localization analyses for
clone-oriented techniques.   We
leave a concrete investigation of these possibilities to future work.  

% We also find that 
% Thus, clone edits are more likely to contradict SBFL's core assumptions
% on fault coverage, which suggests that clones are often contextually 
% \emph{different} enough to be not visible to all failing tests.
% Clone-oriented techniques such as Hercules~\cite{saha2019harnessing} may benefit
% from a non-SBFL technique.

\section{Test case-based validation}
\label{sec:tests}

Dynamic program repair typically uses test cases to validate patch plausibility. 
%, and tests
%are often used as a proxy for a full correctness specification. Tests are
%imperfect for this task, and virtually all APR techniques can \emph{overfit} to
%them~\cite{Smith15fse}, but they remain accessible and widely-used in practice.
%patches that satisfy the provided tests, but do not generalize to the
%higher-level specification~\cite{Smith15fse}.  
%
%Indeed, not all edits in a human patch are always necessary to pass all tests. This could be
%due to a human patch including refactoring or other changes that does not actually
%change code behavior~\cite{api-refactoring, tangledchanges}, or it could be a sign that the test
%suite is inadequate.  
Test case quality is a pressing problem in this context in
general~\cite{Smith15fse}, but perhaps especially in search-based approaches,
where tests typically inform the fitness
function. Here, ideally, test cases could usefully identify partial
solutions~\cite{better-fitness}, 
though whether tests are suitable for this purpose is a matter of
debate~\cite{ae,rsrepair}.  Sometimes, tests perform no differently on partial
repairs than they do on the original program~\cite{chris-thesis,
  source-code-checkpoint}; they have also been observed to perform
worse~\cite{gecco09}.  
%
We study this question in a principled fashion to characterize whether or how
tests can support multi-location patch construction.  That is, if we apply only one of
several of the edits in this patch to the buggy program, is the result
identifiable as ``closer'' to the full repair? Alternatively:

\rqorinsight{7}{How well do test case based validation methods identify partial repairs?}

If tests can in general identify partial repairs, this suggests the feasibility
of techniques that construct multi-location repairs by identifying/composing
partial repairs.

% CLG: ...can we get away without mentioning granularity here? Let's find out....
% Moreover, unit test performance can be measured in different granularity levels. 
% We would like to compare the performance of the fitness functions at identifying 
% partial repairs at different granularity levels.


\subsection{Methodology}
\label{sec:partial-repair-methodology}

In this experiment, we effectively apply all partial repairs for
a subset of multi-location bugs, and
evaluate how well the partially-patched programs perform 
as compared to both the original and the fully-patched versions. 

\subsubsection{Partial Repairs and Bug Selection}
We apply each non-empty subset of all location-level edits for a given bug to
its buggy program.  To minimize semantically invalid partial repairs, we \emph{always}
include lines that add new import statements, helper methods/classes, or variable declarations
in a location. 
For example, if a multi-location patch contains a declaration of new variable
\texttt{var} at one edit location,
we include this declaration in all partial repairs, and treat the rest of this location (if any) as one location.
Note that this can reduce the total
number of locations in a patch if an edit location is entirely declarations, and we eliminate
human patches that contain only 
one edit location after this preprocessing step. 

Due to the exponential growth of partial repairs with respect to number of edit locations,
we evaluate multi-location bugs with
between two and six edit locations, excluding Closure, which uses a
non-standard test harness. 
Overall, we evaluated 1894 Defects4J and 660 \bears partial repairs,
of 245 Defects4J and 76 \bears bugs.

\subsubsection{Test Granularity}

The standard use case for JUnit runs
an entire test \emph{class}, which ``passes'' if all test \emph{methods} in
that class also pass.  However, for the purposes of identifying partial repairs,
we can evaluate partial correctness at both the \emph{class-level} and
\emph{method-level} (a program that passes more methods is more
correct).
We also introduce and consider \emph{assertion-level} granularity, looking
roughly at how many assertions \emph{within} a method fail.  We define this as follows:
let $A(M)$ be the set of all \texttt{assert}
statements in a test method $M$. 
When $M$ is run, if an assertion failed, the failure is recorded and the method 
continues to run.  When the method completes, for each
$a\in A(M)$, let $b(a)$ be 1 if $a$ never failed once during execution,
and 0 otherwise. We define the \emph{assertion score} as
$AS(M)=\frac{\Sigma_{a\in A(M)}b(a)}{|A(M)|}$. If $M$ failed to run to completion 
due to timeouts or unrelated exceptions, then
$AS(M)=0$. Thus by definition, $AS(M)=1$ if $M$ passes. If a program passed more 
assertions in $M$, there should be an increase in $AS(M)$.


\subsection{Results}

\looseness-1
\subsubsection{Minimality} Our first immediate takeaway is that not all edits
in a human patch are always necessary to pass all tests~\cite{api-refactoring,
  tangledchanges}:
this is true for 41.2\%  (101 out of 245) of Defects4J and 52.6\% (40 out of 76) of
\bears bugs. 
The relatively high proportion of Defects4J patches that are not 1-minimal is
somewhat surprising, as Defects4J maintainers manually minimize patches to exclude
changes deemed unrelated to the bugs in question.
This suggests that the test suites are weak proxies for total correctness.
\bears patches are not minimized, which likely explains
their higher percentage of non-minimality (52.6\%) (the difference between the two datasets is statistically
significant, $p < 0.001$, via a 
$\chi^2$ test).

\looseness-1
We compute results for RQ7 using the 1-minimal set of human edits
with respect to the unit tests as the ``full'' patch.
We do this for parsimony: including
unminimized edits introduces noise without materially changing
results.  And, from the point of view of an APR tool, only
minimal edits are identifiable or ``visible'' via test case behavior. 
We exclude bugs
minimized to a single edit, leaving
948 Defects4J and 240 \bears partial repairs, from
168 Defects4J and 46 \bears bugs.  

% We therefore present our results for RQ7 in two ways:
% treating the provided human patch as the full repair
% (Unminimized), and considering only the 1-minimal set of edits in the
% human patch with respect to the unit tests (Minimized).

\subsubsection{Partial repair identification} Our overall goal is to identify how many partial repairs pass more test than the
original buggy program  (\emph{positive}), the  same number of tests as the
original buggy code (\emph{neutral}) or fewer tests than the buggy code
(\emph{negative}).  However, 
since each bug can have a different 
number of partial repairs, we
scale the partial repair results to give each bug equal weight.
That is, for a bug with $n$ partial repairs, each edit has weight 
$\frac{1}{n}$.\footnote{We exclude partial
repairs that do not compile from the count, e.g., by throwing errors like ``missing return statement.''}


%\begin{table}
%{\begin{center}
%\begin{tabular}{ll|rr|rr|rr}
%\toprule
%\multicolumn{2}{c}{}&\multicolumn{2}{c}{Defects4J} & \multicolumn{2}{c}{\bears} &\multicolumn{2}{c}{Combined}  \\
%\multicolumn{2}{c}{Minimized?} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes} & \multicolumn{1}{c}{No} & \multicolumn{1}{c}{Yes}  \\
%\midrule
%\multirow{3}{*}{Positive} & Class & 25.95\% & 12.77\% & 23.87\% & 9.22\% & 25.46\% & 12.02\% \\
% & Method & 40.82\% & 34.58\% & 28.41\% & 18.23\% & 37.90\% & 31.14\% \\
% & Assertion & 44.49\% & 39.93\% & 28.68\% & 18.69\% & 40.76\% & 35.46\% \\ 
%\midrule
%\multirow{3}{*}{Neutral} & Class & 59.56\% & 68.30\% & 58.97\% & 73.31\% & 59.42\% & 69.35\% \\
% & Method & 39.62\% & 40.05\% & 53.08\% & 64.30\% & 39.93\% & 40.83\% \\
% & Assertion & 33.73\% & 31.52\% & 49.52\% &  59.60\% & 37.45\% & 37.43\% \\ 
%\midrule
%\multirow{3}{*}{Negative} & Class & 9.30\% & 11.57\% & 8.78\% & 13.30\% & 9.18\% & 12.16\% \\
% & Method & 13.51\% & 17.16\% & 10.13\% & 13.30\% & 12.71\% & 16.35\% \\
% & Assertion & 15.37\% & 19.80\% & 12.75\% &  16.41\% & 14.75\% & 19.09\% \\ 
%\bottomrule
%\end{tabular}
%\end{center}}
%\caption{Weighed percent of partial repairs exhibiting a {\normalfont Positive}, {\normalfont Neutral},
% or {\normalfont Negative} change in test passage, measured at different levels of granularity
% and with both minimized and unminimized patches.\todo{big ole todo: cut
%          the ``unminimized'' results and turn this into a histogram; see my bad
%          sketch in Slack.}}
%\label{yiweitable}
%\vspace{-0.5in}
%\end{table}

\begin{figure*}
        \includegraphics[width=\textwidth]{img/weighted_percent.pdf}
        \caption{\small Weighted percent of partial repairs exhibiting a {\normalfont
            Positive}, {\normalfont Neutral}, or {\normalfont Negative} change
          measured at varying levels of granularity. As granularity increases
          the negative partial repair rate remains low, while the positive
          repair rate rises significantly.}
        \label{fig:fitness}
\end{figure*}


Figure~\ref{fig:fitness} shows results.
Assertion level granularity correctly positively identifies
39.2\% of the partial repairs in Defects4J
and 17.9\% of the partial repairs in \bears.
Only 31.3\% of Defects4J partial repairs and 
57.4\% 
of \bears partial repairs are neutral. Less than 21\% of bugs in both datasets
produce negative test case behavior.
Although the rate of positive and correct identification of partial repairs varies between datasets,
tests are not often adversarial towards partial repairs, contrary to anecdotal
observations in prior work~\cite{gecco09}.

\subsubsection{Granularity} Finer granularity levels are better at identifying partial repairs positively,
but they also increase the chance of erroneously mis-identifying partial repairs.
Improvements between granularity levels vary across datasets --- the improvement 
in positively identified partial repairs between method and assertion level granularities
is negligible in \bears (0.5\%) but greater in Defects4J (5.2\%), while both had around 3\% 
increase in negatively identified partial repairs.
Moreover, evaluating patches at finer granularity levels can add overhead.
For example, assertion level granularity may require instrumenting JUnit with additional
logic to continue execution when assertions fail.
Such overhead lowers the number of
candidate patches that can be validated per unit time.
The choice of a granularity level is a balancing act with implications for
multiple axes of APR performance.


\begin{table}
  {\begin{center}
      \begin{tabular} {llrrrr}
        \toprule
        \multicolumn{2}{c}{Partial Repair} & Contradicts & Partially Holds & Holds & Total \\
        \midrule
        \multirow{2}{*}{Positive} & $\exists$  & 41 & 30 & 12 &  83 \\
                                  & $\nexists$ &  1 &  3 & 19 &  23 \\
        \midrule
        \multirow{2}{*}{Negative} & $\exists$  &  2 &  7 & 16 &  25 \\
                                  & $\nexists$ & 40 & 26 & 15 &  81 \\
        \midrule
        Total                     &            & 42 & 33 & 31 & 106 \\
        \bottomrule
      \end{tabular}
    \end{center}
  }
  \caption{\small Multi-test bugs categorized by both localization pattern
    (Section \ref{secFL}) and existence of positively/negatively identified
    partial repairs. Bug patches with the \emph{holds} coverage pattern are
    statistically significantly more likely to exhibit negative partial
    repairs.  \label{tab:cov_fitness}}
\end{table}

\subsubsection{Relationship to fault localization} 
Finally, we assess the relationship between partial repair behavior and fault
localization/coverage pattern. Table~\ref{tab:cov_fitness} shows 
bugs with 2+ failing tests 
and 2--6 edit locations in the minimized patch, grouped by localization
categories (Section~\ref{secFL})
and the existence of positively/negatively
test-identified partial repairs at any granularity level.
In general, bugs with \emph{holds} localization (where failing tests execute
more similar locations) are less likely to have
positive partial repairs and more likely to have negative partial repairs (statistically significant,
$p < 0.001$ for both, via $\chi^2$ tests).
Thus, a \emph{holds} localization pattern correlates with an
\emph{adversarial} test-based fitness landscape,
and partial repairs may be harder to identify using tests for such bugs. 

These connections 
suggest that fault-revealing tests' patch coverage reflect the tests' ability
to detect components of a patch.
Recall that \emph{holds} localization means all failing tests execute 
the exact same set of patch lines, and \emph{contradicts} localization 
means no patch line is covered by all failing tests.
Intuitively, if a patch can be
assembled from two components \textbf{A} and \textbf{B}, two tests that both
fully cover \textbf{A} and \textbf{B} are less effective at assessing partial
correctness than two tests that only cover \textbf{A} and \textbf{B} separately.
Coverage thus influences the accuracy of tests as a judge of partial correctness.

In practice, we do not know patch coverage \emph{a priori}.  However, if we can infer the
likelihood of a bug belonging to the \emph{holds} category using existing 
fault localization (since \emph{holds} requires all failing tests to cover the 
entire patch), then we can relax judgments of partial correctness to 
preserve more partial repairs that might otherwise be rejected.

\section{Limitations}

\label{sec:limits}
\noindent\textbf{Dataset.}
We use two datasets, Defects4J and \bears, totaling 1018 bugs in 51 projects. 
Although these datasets come from different sources who have created them to be broadly 
applicable for evaluating program repair, there is a risk that our results do not generalize to other 
bugs.

\vspace{1ex}
\noindent\textbf{Coverage.}
We use JaCoCo to collect coverage, but it has some
limitations. It cannot analyze coverage for inserted
cases for \texttt{switch} statements, \texttt{else} statements, method
signatures, and other code constructs that are compiled away during the
transformation to bytecode. Since we focus on how the coverage of different
failing tests compared to each other, we posit that JaCoCo's coverage was a
sufficient approximation.

\vspace{1ex}
\noindent\textbf{Dependency analysis.}
We analyzed dependencies intraprocedurally with some interprocedural 
heuristics, which make assumptions derived from common Java practices.
These assumptions are not always true. We use heuristics to add some
approximated interprocedural data while retaining interprocedural 
analysis's scalability to large, real world software.

\vspace{1ex}
\noindent\textbf{Code clones.}
We examined code clones at the location level.  However, clones also occur at the
granularity of lines~\cite{JiaClones} or
subexpressions~\cite{microclones}. We classified clones into four
categories (Same, Near-same, Composite, Move), but other categorizations 
are possible.
%
We did not distinguish between the number
of edit locations in a particular set of clones. It is possible that a set of
clones contains two or more locations with similar edits, which we do not examine. 

\vspace{1ex}
\noindent\textbf{Partial repair construction.}
We constructed partial repairs by breaking up multi-location patches
into single-location edits.
In reality, most APR techniques mutate code at a finer granularity, so
these edits may not be representative.  Additionally, we only observe if test suites
can identify partial repairs. Perhaps our evaluated partial repairs
are outside of the search space of a
particular APR technique.
%Regardless of this limitation, this experiment provides valuable 
%insight into the challenge of automatically constructing
%multi-location patches.

\section{Related Work}
\label{sec:related}

Recent empirical studies on fault localization find that 
SBFL is more effective than various other techniques, such as 
mutation-based fault localization~\cite{pearson2017evaluating, mut-analysis}.
%% , program 
%% slicing, predicate switching,  information retrieval, and other techniques. 
However, research has shown that multiple fault localization techniques can be
combined to localize different kinds of faults~\cite{zou2019empirical}. Most
empirical studies on fault localization evaluate techniques by determining
whether a technique can localize any faulty line. This usefully evaluates single
line or single-location faults, but not necessarily multi-location faults.

Golagha et. al~\cite{golagha2020can} analyzed the impact of 70 code metrics on
SBFL, including multi-location patches.  They found that the number of locations
changed in the human patch was not correlated to SBFL success.  However, they
considered a fault localization successful if the faulty method was one of the
top-10 most-suspicious predictions, which we argue is inadequate for
identifying multi-location faults for APR.

Mutation testing contains parallels to APR~\cite{weimer2013leveraging} and fault
localization techniques~\cite{metallaxis,muse,mbfl-survey}. Higher-order
mutation testing, where several mutations can be composed, shares similar
difficulties with multi-location APR due to exponential search space
growth~\cite{long-search-spaces}. The current state of the art in higher-order
mutation testing uses a genetic search-based approach~\cite{homs}.

% I know I'm referring to the work by the authors' names,
% but I can't find a good way to refactor their names out without writing awkwardly.
% I would need to refactor all of the usages of "they" to remove their names.
Long and Rinard~\cite{long-search-spaces} studied correct and incorrect
plausible patches in APR search spaces and found incorrect plausible patches to
outnumber correct patches by orders of magnitude~\cite{spr, prophet}. Increasing
the search space with additional mutation operations also increased the number
of correct patches, but also increased the number of incorrect plausible
patches.  While we do not study this particular problem, we similarly seek to
understand the massively larger search space of multi-location repair.

A previous empirical study on human bug fixes~\cite{zhong2015} studied fault
localization difficulty, bug fix complexity, and necessary mutations on over
9000 real-world bugs. This paper also aims to provide useful guidance and
insights for improving APR techniques through empirical studies
of bug fixes. Our study differs in that we focus on source code bugs
patched by multiple edits in multiple locations, drawing insights in fault
localization, fix mutations, and test-based patch evaluations.

Wang et al.~\cite{wang2018} empirically studied multi-entity (i.e., class,
method, or field) changes in patches. They studied why and how often patches
have multi-entity changes, relationships between entities, and recurring
patterns in changes.  They found that 66\%-76\% of multi-entity fixes contain
dependencies, and identified patterns that connect co-changed entities,
suggesting they could be leveraged by APR. In contrast, we focus on bugs
that require multiple edits, including edits to the same entity.

Hercules~\cite{saha2019harnessing} is an APR technique that takes advantage of
fix code similarity to generalize a single repair edit into multiple similar
edits. We discuss fix code similarity and insights on Hercules in
Section~\ref{sec:clones}.

Qi et al.~\cite{patch-correctness} evaluated patches generated by three repair
tools and found that the vast majority of generated patches were incorrect and
equivalent to a single functional deletion. This is an instance of patch
\emph{overfitting}, which can be measured via the use of held-out test
suites~\cite{Smith15fse}. Later work found patch incorrectness to be also
problematic in Defects4J~\cite{d4j-eval} and in semantics-based repair
techniques~\cite{Le2018}. We similarly find that test suites provide an
imperfect specification of program behavior: many bugs can be fixed with only a
portion of a multi-location human patch. However, we also find that test cases
can often identify partial patches.

Prior work in program repair using search-guiding information during
test-suite-based validation include using program
invariants~\cite{better-fitness, dinglyu}, intermediate program
values~\cite{source-code-checkpoint}, and mutation-based fault
localization~\cite{mut-analysis}.
%% Some approaches require additional input, such
%% as suspicious variables~\cite{source-code-checkpoint} or known patches for the
%% bug under repair~\cite{better-fitness}, while others exhibit limited performance
%% improvements~\cite{dinglyu, mut-analysis}.
We show that increasing test granularity can better inform search in the
validation stage by identifying partial repairs.

Schulte et al.~\cite{schulte} found that 37\% of code mutations do not change
test case passage and discussed potential applications of mutational robustness
in repair.  Although our context differs --- they studied random
mutations, we study changes associated with specific bug fixes --- we also find
a non-trivial proportion of neutral edits in multi-location repairs.


\section{Takeaways and Conclusions}
\label{sec:takeaways}

Our findings demonstrate that bugs repaired with multi-location patches have
fundamental differences from bugs repaired with single-location patches. These
differences should be considered for designers of future APR techniques that
generate multi-location bug fixes. These are our key takeaways:

\vspace{1ex}
\noindent\textbf{Assumptions underlying spectrum-based fault
  localization do not always hold in multi-location bugs.}
The assumption that faulty locations
are executed more often by negative test cases does not hold for a substantial
number of the bugs we studied. In our study, 40\% of multi-location bugs
characterized by multiple failing tests partially held this assumption and 27\% of
these bugs contradicted this assumption.
Patches with clones are especially likely 
to fall into this latter category (49\%, compared to 19\% without clones).
In these cases, SBFL is unlikely to be the most 
appropriate choice for a
fault localization technique. 
%
In addition, bugs for which failing tests all execute the same faulty locations (i.e. bugs for 
which the assumption holds) are more likely to have failing tests that execute very similar 
parts of the program overall, indicating that there may be a way to identify these bugs a priori.
This finding suggests both that there is still progress
to be made in fault localization for automated repair, and potentially promising initial directions.

\vspace{1ex}
\noindent\textbf{Patches with dependencies are common, and harder to construct.}
We found that 45\% of multi-location patches contained
intra-patch control and data dependencies. Such bugs are less likely to be
repaired by a broad range of prior repair techniques. 
% Semantic APR can generate dependent code, and can sometimes even 
% account for constraints imposed by dependencies.
% Whether semantic APR techniques are any better at practically generating 
% dependent patches, however, is a data analysis task for the journal extension paper.
This suggests an opportunity to exploit dependencies by
existing or new techniques to improve APR for a large class of presently difficult-to-repair
bugs.

\vspace{1ex}
\noindent\textbf{Code clones are prevalent in multi-location patches.}
We found code clones in 28.3\% of bugs with multi-location
patches. This observation supports recent work that leverages code clones to
generate repairs~\cite{saha2019harnessing}.
We also found that clones are more likely to contradict SBFL's assumptions 
on coverage, which suggests both a heuristic for clone application and the
potential benefits of other fault localization paradigms.

\vspace{1ex}
\noindent\textbf{Tests are
    often quite effective at identifying partial repairs, and test coverage can
    help establish confidence in a test suite's judgment of partial correctness.}
Contrary to prior work~\cite{gecco09}, we found that test suites are infrequently
hostile to partial repairs. That is, partial application of a correct patch
usually does not increase the number of test case failures. This suggests that
one assembling correct patches from partial repairs may be a feasible avenue for
APR.  
Moreover, test cases are a more accurate metric of partial correctness when coverage
overlap of patch components is lower.

We can reduce the possibility of overlapping coverage by decomposing tests into
smaller units, potentially by
refactoring~\cite{b-refactoring} or using a finer level of granularity for
measuring test suite success, such as the assertion level granularity proposed
in Section~\ref{sec:partial-repair-methodology}.
This also implies that class-level granularity validation has trade offs. Some
repair frameworks use class-level granularity for faster validation. This may
come at the cost of less accurate detection of partial patches. However, this
trade off may be worthwhile in Java, given the non-trivial challenges involved in
decomposing JUnit test classes.

\vspace{1ex}
\noindent\textbf{Human patches are often not test-minimal.}
Many bugs do not need all patch locations to pass all tests,
offering further evidence of the incompleteness of test cases as a
proxy for correctness~\cite{patch-correctness} and the
presence of non-corrective changes (e.g., refactoring, enhancements)
in handwritten bug patches~\cite{api-refactoring, tangledchanges}.  This
challenges the practice of using a human patch as the syntactic gold standard,
while suggesting that test suite quality
are still a challenge in modern software.

\vspace{1ex}
\noindent\textbf{Techniques need to be evaluated on diverse benchmarks.}
Our two datasets, Defects4J and \bears, exhibited different characteristics.
These characteristics may explain why APR tools perform unevenly across
different benchmark datasets~\cite{durieux-repair-them-all}. Our findings
provide evidence reinforcing the call for the use of diverse benchmarks when
evaluating tools.

\vspace{1ex}
To date, most automated program repair techniques have been limited to bugs that
can be localized to and repaired at a single program location. However, we find
that 54\% of bugs in the Defects4J dataset and 63\% of bugs in the \bears dataset
were repaired by a human developer with multi-location patches. This motivated
our study on the characteristics and challenges of automatically repairing such bugs. 

Our findings suggest deep implications for program repair targeting
multi-location patches, both in terms of the applicability of existing program
repair techniques and the design future repair techniques.  We hope that our
findings and insights inspire future work to advance the state of the art of APR
and take on difficult multi-location bugs.  Our data/code package is
available at \url{https://github.com/squaresLab/MultiEdit_Experiments}.



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

\bibliographystyle{IEEEtran}
\bibliography{references}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:
\begin{IEEEbiography}[{\includegraphics[angle=90,width=1in,height=1.25in,clip,keepaspectratio]{img/bio-photos/leo}}]{Leo Chen}
    received the BS degree in engineering with a concentration in computing from Olin College.
    They are currently a software engineering PhD student at Carnegie Mellon University.
    Their primary research interests are in automated program repair and human factors.
    
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/bio-photos/zhen}}]{Zhen Yu Ding}
    received the BS degree in computer science from the University of Pittsburgh.
    Zhen Yu Ding is currently a security engineer at Motional, with research and
    industrial interests in software security, program repair, fuzz testing, and
    autonomous vehicle security.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/bio-photos/yiwei}}]{Yiwei Lyu}
    is currently a fourth year undergraduate student at Carnegie Mellon University, majoring in computer science.
    His main research interests include multimodal machine learning and automated program repair.
\end{IEEEbiography}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiography}[{\includegraphics[clip,
        height=1.25in, width=1in, keepaspectratio]{img/bio-photos/jeremy}}]{Jeremy Lacomis}
received the BA degree in computer science from the University of Virginia. He
is currently pursuing a PhD in software engineering at Carnegie Mellon
University. His main research interests include automated program repair and
software reverse engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[clip,
        height=1.25in, width=1in, keepaspectratio]{img/bio-photos/claire}}]{Claire Le~Goues}
received the AB degree in computer science from Harvard University and the MS
and PhD degrees in computer science from the University of Virginia. She is
currently an Associate Professor at Carnegie Mellon University. Her main
research interests span software engineering and programming languages, and
especially in how to construct, maintain, evolve, improve/debug, and assure
high-quality software systems.
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
